# ğŸ“˜ What are LLMs and How Do They Work?

## ğŸ”¹ Foundation Models
- Foundation models are **large-scale neural networks** trained on **vast amounts of data**.
- These models serve as a base for various tasks like NLP, translation, and text generation.
- **Example**: GPT-3 is a foundation model.

## ğŸ”¹ Training Data
- LLMs are trained on a **large corpus of text** including:
  - Books
  - Articles
  - Conversations
  - And more
- Total training data size is in **petabytes**.

---

## ğŸ§  Training Phases of LLMs

### 1ï¸âƒ£ First Stage: Unsupervised Learning
- Learns **patterns and relationships** in the data.
- No labeled data or alignment used.
- The model may output random or irrelevant responses.
- **Example**: "Hey, whatâ€™s up?" â†’ might respond with â€œWhatâ€™s up, with you?â€

### 2ï¸âƒ£ Second Stage: Supervised Learning
- Trained using **clear objectives** (e.g., sentiment classification, translation).
- Adjusts weights to align with **user intent** or specific goals.

### 3ï¸âƒ£ Third Stage: Instruction Fine-tuning
- Uses **supervised instruction tuning** on labeled datasets.
- Further refined using **Reinforcement Learning from Human Feedback (RLHF)**:
  - Users rate the quality or usefulness of model outputs.
  - Model improves based on this feedback.
  - Works similarly to a **reward system**.
- Seen in tools like **ChatGPT**, **Gemini**, etc., where users rate or select best responses.

---
