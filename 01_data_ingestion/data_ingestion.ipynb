{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Data Ingestion : RAG STAGE - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Text Document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.text.TextLoader object at 0x0000027602CE7100>\n",
      "[Document(metadata={'source': '..\\\\01_data_ingestion\\\\speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Loading the text doc.\n",
    "loader = TextLoader(r\"..\\01_data_ingestion\\speech.txt\")\n",
    "print(loader)\n",
    "\n",
    "text_doc = loader.load()\n",
    "print(text_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 13}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'), Document(metadata={'source': '..\\\\01_data_ingestion\\\\attention.pdf', 'page': 14}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]\n",
      "\n",
      "\n",
      "The number of pages in the pdf: 15 \n",
      "The types of the pdf_doc obj: <class 'list'>\n",
      "\n",
      "\n",
      "The first page is: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initiating loader.\n",
    "pdf_loder = PyPDFLoader(r\"..\\01_data_ingestion\\attention.pdf\")\n",
    "\n",
    "# Loading data\n",
    "pdf_doc = pdf_loder.load()\n",
    "print(pdf_doc)\n",
    "print(f\"\\n\\nThe number of pages in the pdf: {len(pdf_doc)} \\nThe types of the pdf_doc obj: {type(pdf_doc)}\\n\\n\")\n",
    "\n",
    "print(f\"The first page is: {pdf_doc[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Web-Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "web_loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), \n",
    "                           bs_kwargs = dict(parse_only=bs4.SoupStrainer(\n",
    "                               class_ = (\"post-header\", \"post-content\"))))\n",
    "\n",
    "webpage_content = web_loader.load()\n",
    "print(webpage_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loding Research Papers (IEEE) from arxiv Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2024-09-06', 'Title': 'Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers', 'Authors': 'Chenglei Si, Diyi Yang, Tatsunori Hashimoto', 'Summary': 'Recent advancements in large language models (LLMs) have sparked optimism\\nabout their potential to accelerate scientific discovery, with a growing number\\nof works proposing research agents that autonomously generate and validate new\\nideas. Despite this, no evaluations have shown that LLM systems can take the\\nvery first step of producing novel, expert-level ideas, let alone perform the\\nentire research process. We address this by establishing an experimental design\\nthat evaluates research idea generation while controlling for confounders and\\nperforms the first head-to-head comparison between expert NLP researchers and\\nan LLM ideation agent. By recruiting over 100 NLP researchers to write novel\\nideas and blind reviews of both LLM and human ideas, we obtain the first\\nstatistically significant conclusion on current LLM capabilities for research\\nideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than\\nhuman expert ideas while being judged slightly weaker on feasibility. Studying\\nour agent baselines closely, we identify open problems in building and\\nevaluating research agents, including failures of LLM self-evaluation and their\\nlack of diversity in generation. Finally, we acknowledge that human judgements\\nof novelty can be difficult, even by experts, and propose an end-to-end study\\ndesign which recruits researchers to execute these ideas into full projects,\\nenabling us to study whether these novelty and feasibility judgements result in\\nmeaningful differences in research outcome.'}, page_content='Can LLMs Generate Novel Research Ideas?\\nA Large-Scale Human Study with 100+ NLP Researchers\\nChenglei Si, Diyi Yang, Tatsunori Hashimoto\\nStanford University\\n{clsi, diyiy, thashim}@stanford.edu\\nAbstract\\nRecent advancements in large language models (LLMs) have sparked optimism about their potential to\\naccelerate scientific discovery, with a growing number of works proposing research agents that autonomously\\ngenerate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very\\nfirst step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by\\nestablishing an experimental design that evaluates research idea generation while controlling for confounders\\nand performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent.\\nBy recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas,\\nwe obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we\\nfind LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged\\nslightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and\\nevaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation.\\nFinally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose\\nan end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to\\nstudy whether these novelty and feasibility judgements result in meaningful differences in research outcome. 1\\n1\\nIntroduction\\nThe rapid improvement of LLMs, especially in capabilities like knowledge and reasoning, has enabled\\nmany new applications in scientific tasks, such as solving challenging mathematical problems (Trinh\\net al., 2024), assisting scientists in writing proofs (Collins et al., 2024), retrieving related works (Ajith\\net al., 2024, Press et al., 2024), generating code to solve analytical or computational tasks (Huang et al.,\\n2024, Tian et al., 2024), and discovering patterns in large text corpora (Lam et al., 2024, Zhong et al.,\\n2023). While these are useful applications that can potentially increase the productivity of researchers,\\nit remains an open question whether LLMs can take on the more creative and challenging parts of\\nthe research process.\\nWe focus on this problem of measuring the research ideation capabilities of LLMs and ask: are current\\nLLMs capable of generating novel ideas that are comparable to expert humans? Although ideation\\nis only one part of the research process, this is a key question to answer, as it is the very first step to the\\nscientific research process and serves as a litmus test for the possibility of autonomous research agents\\nthatcreatetheirownideas. Evaluatingexpert-levelcapabilitiesofLLMsystemsischallenging(Bakhtin\\n1Interested researchers can sign up for this end-to-end study at: https://tinyurl.com/execution-study. We\\nrelease our agent implementation and all human review scores at: https://github.com/NoviScl/AI-Researcher.\\n∗The last two authors advised this project equally.\\n1\\narXiv:2409.04109v1  [cs.CL]  6 Sep 2024\\n7 NLP \\nTopics\\n\\nBias\\nCoding\\nSafety\\nMultilingual\\nFactuality\\nMath\\nUncertainty\\nHuman \\nExperts\\nAI \\nAgent\\nCondition 1 : Human Ideas (N=49)\\nCondition 2 : AI Ideas (N=49)\\nCondition 3 : AI Ideas + Human Rerank (N=49)\\nBlind Review by Experts (N=79)\\nNovelty Score: 4.84\\n Novelty Score: 5.64\\nNovelty Score: 5.81\\nIdea Generation\\nFigure 1: Overview of our study: we recruit 79 expert researchers to perform blind review of 49 ideas\\nfrom each of the three conditions: expert-written ideas, AI-generated ideas, and AI-generated ideas\\nreranked by a human expert. We standardize the format and style of ideas from all conditions before\\nthe blind review. We find AI ideas are judged as significantly more novel than human ideas (p<0.05).\\nHuman\\nAI\\nAI+Rerank\\n3\\n4\\n5\\n6\\n7\\nScore\\n*\\n*\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\n3\\n4\\n5\\n6\\n7\\n*\\n*\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\n3\\n4\\n5\\n6\\n7\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\n3\\n4\\n5\\n6\\n7\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\n3\\n4\\n5\\n6\\n7\\n*\\nOverall\\nFigure 2: Comparison of the three experiment conditions across all review metrics. Red asterisks\\nindicate that the condition is statistically better than the Human baseline with two-tailed Welch’s t-tests\\nand Bonferroni correction. All scores are on a 1 to 10 scale. More detailed results are in Section 5.\\net al., 2022, Collins et al., 2024), and research ideation takes this to an extreme. Qualified expert re-\\nsearchers are difficult to recruit at scale, evaluation criteria can be highly subjective, and it is difficult\\nfor even the best experts to judge the quality of an idea (Beygelzimer et al., 2021, Simsek et al., 2024).\\nWe address these challenges directly, recognizing that for important, high-stakes tasks like research\\nideation, there is no substitute for a large-scale expert evaluation. We design a carefully controlled\\ncomparison of human and LLM ideas that overcomes sample size and baseline problems present\\nin earlier small-scale evaluation studies. Our study recruited a large pool of over 100 highly qualified\\nNLP researchers to produce human baseline ideas and perform blind reviews of human and LLM\\nideas. To reduce the possibility that confounding variables affect our outcome measures, we enforce\\nstrict controls that standardize the styles of human and LLM ideas and match their topic distribution.\\nWe compare our human expert baseline with a simple and effective LLM agent that incorporates\\nretrieval augmentation and adopts recent ideas in inference-time scaling, such as overgenerating and\\nreranking LM outputs. These measures allow us to make statistically rigorous comparisons between\\nhuman experts and state-of-the-art LLMs (Figure 1).\\n2\\nOur evaluation-centric approach complements many recent methods-centric works that attempt to\\ninstantiate research agents (Baek et al., 2024, Li et al., 2024, Lu et al., 2024, Wang et al., 2024, Yang et al.,\\n2024). The majority of these works rely on fast and lower-cost evaluation surrogates – either by decreas-\\ning the number of expert reviewers (Baek et al., 2024, Li et al., 2024, Wang et al., 2024, Yang et al., 2024),\\nconstraining the length and detailedness of the ideas (Wang et al., 2024, Yang et al., 2024), or relying on\\nLLM-as-a-judge (Lu et al., 2024). They do not perform the large-scale human comparison studies that\\nare needed to answer the motivating question of our work. Our work takes the opposite approach,\\nperforming a year-long and high-cost evaluation that provides human expert baselines and a stan-\\ndardized evaluation protocol to serve as a foundation for future follow-up studies and methods work.\\nThrough nearly 300 reviews across all our conditions, we find that AI-generated ideas are judged\\nas more novel than human expert ideas (p<0.05), which holds robustly under multiple hypothesis\\ncorrection and across different statistical tests. We find some signs that these gains are correlated\\nwith excitement and overall score, and may come at the slight expense of feasibility, but our study\\nsize did not have sufficient power to conclusively identify these effects (Figure 2).\\nQualitative analysis of free-text responses in our review corroborates these findings on novelty and\\nfeasibility. Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and\\nopen problems – despite excitement about inference-time scaling of LLMs, we find that they lack idea\\ndiversity when we scale up idea generation, and they cannot currently serve as reliable evaluators.\\n2\\nProblem Setup\\nThe central experiment of our work is a comparison of human- and LLM-generated ideas. While\\nthis goal is simple, there is no existing consensus on how to formulate the task of research ideation\\nand evaluation, and we begin by defining the key aspects of our experiment design.\\nWe think of research idea evaluation as consisting of three separate components: 1). the idea itself,\\ngenerated in response to our instructions, 2). the writeup which communicates the idea, and 3). the\\nevaluation of the writeup by experts. We outline our experiment design in each of these three parts\\nwith particular focus on potential confounders, such as the area of research, the format of a research\\nidea, and the evaluation process.\\nIdeation Scope and Instructions\\nResearch ideas can take many different forms. They can be simple\\ntricks to improve model performance, or they may be large-scale research programs that form the\\nbasis of a Ph.D. thesis. Any experiment on ideation must carefully balance the realisticness and\\ninterestingness of a research idea with the practical realities of eliciting ideas from a large population.\\nIn our case, these tradeoffs are even more pronounced, as we have designed our ideation experiments\\nso that the resulting ideas can be executed by experts in a follow-up set of experiments.\\nThese constraints have led us to study prompting-based NLP research as a testbed for our study.\\nPrompting research has been popular in recent years of NLP and AI research (e.g., Chen et al., 2023,\\nDiao et al., 2024, Madaan et al., 2023, Qin et al., 2024, Schulhoff et al., 2024, Si et al., 2023, Wang et al.,\\n2023, Wei et al., 2022, Yao et al., 2023, Yasunaga et al., 2024, Zhou et al., 2023, inter alia). This class\\nof projects strikes a reasonable trade-off among our constraints. The most impactful prompting\\nprojects like chain-of-thought have had a major influence on LLM performance (Wei et al., 2022), and\\nprompting projects are executable with minimal computing hardware.\\nWe further structure our ideation process to avoid selection-bias-based confounders in ideation. If\\nwe simply ask LLMs and humans to produce ideas on ‘prompting topics’, we may find that LLMs\\nand humans differ in the types of research ideas they produce (for example, LLMs may naturally\\nsuggest more projects on safer topics, which might be judged as less exciting by humans). This would\\n3\\nlead us to simply measure misalignment in research topic preference between LLMs and humans,\\nwhich is not the goal of our study. To address this possibility, we define a set of seven specific research\\ntopics extracted from the Call For Papers page of recent NLP conferences such as COLM. 2 Specifically,\\nour topics include: Bias, Coding, Safety, Multilinguality, Factuality, Math, and Uncertainty (see\\nAppendix A for a complete description of these topics).\\nEach human and LLM participant of the ideation experiment receives the same set of natural language\\ninstructions including the same topic description, idea template, and demonstration example to\\nensure a fair comparison. For human participants, we additionally allow them to select a preferred\\ntopic from the list, and for each selected topic, we generate a corresponding LLM idea. This exactly\\nmatches the idea topic distribution between the LLM and human participants, while ensuring that\\nhuman experts are able to select topics according to their expertise.\\nIdea Writeup\\nAn idea can only be evaluated if it is written up to be communicated, but this writing\\nprocess introduces many additional potential confounders. Human researchers may write in ways\\nthat subtly signal quality research, such as including more examples and implementation details.\\nThe format of the writeup functions as a way to scaffold what contents should be included and the\\nlevel of detailedness. Ideally, we want both human and LLM participants to provide all the necessary\\nimplementation details for their generated ideas.\\nWe take inspiration from guidelines used in grant submissions and introduce a template to specify the\\nstructure and detailedness of idea proposals. Specifically, we construct a template that includes fields\\nfor the title, problem statement, motivation, proposed method, step-by-step experiment plan, test\\ncase examples, and the fallback plan. Both the LLM agent and the human idea writers are instructed\\nto follow this template and our provided demonstration examples to produce a project proposal as\\nthe output (see Appendix B for the full template and Appendix C for the demo example).\\nEven with these templates, there may be subtle writing style cues that affect the outcome measure. For\\nexample, humans may tend to write in a more engaging and informal tone. To reduce this possibility\\nfurther, we developed a style normalization module that uses an LLM to convert all ideas into the\\nsame writing and formatting style without changing the original content. Our small-scale human\\nstudy shows that such a normalization approach leads to a 50% accuracy for expert human judges\\nwho are asked to distinguish AI ideas from human ideas. Finally, the use of an LLM style anonymizer\\nhas the possibility of substantively changing the content of the ideas. To rule this out, the first author\\nof this paper manually verified each human idea proposal to ensure all contents of the original ideas\\nwere preserved. We present the full prompt used in Appendix D.\\nReview and Evaluation\\nReviewing research ideas is notoriously subjective, so we want to design\\na review form that defines all review criteria clearly to standardize and anchor the evaluations as\\nmuch as possible. At the same time, we want our review criteria and measured variables to capture\\nall the desiderata of high-quality research ideas.\\nWe follow best practices from AI conference reviewing (e.g., ICLR and ACL) when designing the\\nreview form, where we define four breakdown metrics including novelty, excitement, feasibility, and\\nexpected effectiveness, apart from the overall score. For each metric, we ask for a numerical score\\non a 1-10 scale along with a free-text rationale. We provide clear definitions and grounding for each\\nnumerical scale to calibrate all reviewers’ standards (see Appendix E for the full review form).\\nOur blind review evaluation will compare ideas from three different conditions:\\n1. Human Ideas: Idea proposals written by our recruited expert researchers.\\n2https://colmweb.org/cfp.html\\n4\\n2. AI Ideas: Idea proposals generated by our LLM agent. We directly take the top-ranked ideas\\nfrom the agent’s output.\\n3. AI Ideas + Human Rerank: Idea proposals generated by our LLM agent. The first author of\\nthis paper manually selected the top-ranked ideas out of all the LLM agent’s generations rather\\nthan relying on the LLM ranker in order to better estimate the upper-bound quality of AI ideas.\\nIn the next two sections, we instantiate how our LLM agent generates ideas and how our expert\\nparticipants generate and review the ideas.\\n3\\nIdea Generation Agent\\nWe build a simple but effective LLM ideation agent to compare with the human expert baseline.\\nRather than focusing on innovating the agent itself, we adhere to a minimalist design principle,\\naiming to understand the current capabilities of LLMs in idea generation. Our research ideation agent\\nhas three essential components: paper retrieval, idea generation, and idea ranking, which we will\\ndescribe in detail below.\\n3.1\\nPaper Retrieval for RAG\\nTo ground idea generation, the agent needs to retrieve papers related to the given research\\ntopic, so that it will be aware of related works when generating new ideas. To do so, we lever-\\nage retrieval-augmented generation (RAG), which has demonstrated effectiveness on many\\nknowledge-intensive tasks (Lewis et al., 2020, Shi et al., 2024). Concretely, given a research topic\\n(e.g., “novel prompting methods that can improve factuality and reduce hallucination of large\\nlanguage models\"), we prompt an LLM to generate a sequence of function calls to the Semantic\\nScholar API. We use claude-3-5-sonnet-20240620 as the backbone model for our agent but\\nthe pipeline should generalize to other LLMs as well. The paper retrieval action space includes:\\n{KeywordQuery(keywords), PaperQuery(paperId), GetReferences(paperId)}. Each\\naction generation is grounded on the previous actions and executed results. We keep the top k=20\\npapers from each executed function call and stop the action generation when a max of N =120 papers\\nhave been retrieved. We then use the LLM to score and rerank all retrieved papers based on three\\ncriteria: 1) the paper should be directly relevant to the specified topic; 2) the paper should be an\\nempirical paper involving computational experiments;3 3) the paper is interesting and can inspire\\nnew projects. The LLM is prompted to score each retrieved paper on a scale of 1 to 10 based on these\\ncriteria and we use the top-ranked papers for the next step of idea generation.\\n3.2\\nIdea Generation\\nOur key insight for idea generation is to generate as many candidate ideas as possible. Our intuition\\nis that only a small fraction of all generated ideas might be high-quality, and we should be willing\\nto expend inference-time compute to generate more candidates so that we can later use a reranker to\\ndiscover the \"diamond in the rough\". This aligns with existing results showing that scaling inference\\ncompute with repeated sampling can boost LLM performance on various coding and reasoning\\ntasks (Brown et al., 2024, Li et al., 2022). Specifically, we prompt the LLM to generate 4000 seed\\nideas on each research topic. The idea generation prompt includes the demonstration examples and\\nthe retrieved papers. We craft k =6 demonstration examples by manually summarizing exemplar\\n3Note that we exclude position papers, survey papers, and analysis papers throughout this study since their evaluation\\ntends to be very subjective.\\n5\\npapers (Dhuliawala et al., 2023, Madaan et al., 2023, Weller et al., 2023, Weston and Sukhbaatar, 2023,\\nYasunaga et al., 2024, Zheng et al., 2024) into our desired idea format. For retrieval augmentation,\\nwe randomly select k=10 papers from the top-ranked retrieved papers and concatenate their titles\\nand abstracts to prepend to the idea generation prompt. We also append the titles of all previously\\ngenerated ideas to the prompt to explicitly ask the LLM to avoid repetitions.\\nTo remove duplicated ideas from this large pool of candidate ideas, we first perform a round of dedu-\\nplication by encoding all seed ideas with all-MiniLM-L6-v2 from Sentence-Transformers (Reimers\\nand Gurevych, 2020) and then computing pairwise cosine similarities. We set a similarity threshold\\nof 0.8 for the idea deduplication based on manual inspection. 4 This leaves about 5% non-duplicated\\nideas out of all the generated seed ideas. We expand more on this duplication issue later in Section 7.1.\\n3.3\\nIdea Ranking\\nThe next step is for our ideation agent to rank all the remaining ideas so that we can find the best\\nones among them. To build such an automatic idea ranker, we use public review data as a proxy.\\nSpecifically, we scraped 1200 ICLR 2024 submissions related to LLMs (with keyword filtering) along\\nwith their review scores and acceptance decisions. We explored multiple ways of predicting the\\nscores and decisions of these submissions and found that LLMs are poorly calibrated when asked\\ndirectly to predict the final scores or decisions, but can achieve non-trivial accuracy when asked to\\njudge which paper is better in pairwise comparisons.\\nN\\nTop-10\\nBottom-10\\nGap\\n1\\n6.28\\n5.72\\n0.56\\n2\\n6.14\\n5.24\\n0.90\\n3\\n5.83\\n4.86\\n0.97\\n4\\n5.94\\n4.99\\n0.95\\n5\\n6.42\\n4.69\\n1.73\\n6\\n6.11\\n4.81\\n1.30\\nTable 1: Average ICLR review scores of\\ntop- and bottom-10 papers ranked by our\\nLLM ranker, with different rounds (N) of\\npairwise comparisons.\\nWe converted the ICLR submissions into our stan-\\ndard project proposal format and randomly paired\\nup accepted and rejected papers and asked LLMs\\nto predict which one is accepted.\\nOn this task,\\nClaude-3.5-Sonnet achieves an accuracy of 71.4%\\nwith zero-shot prompting. For comparison, GPT-4o\\nachieves 61.1% and Claude-3-Opus achieves 63.5%,\\nand we do not observe significant gains from addi-\\ntional prompting techniques like few-shot or chain-\\nof-thought prompting.\\nWe therefore choose the\\nClaude-3.5-Sonnet zero-shot ranker.\\nIn order to obtain reliable scores for all project proposals\\nbased on pairwise comparisons, we adopt a Swiss system\\ntournament where all project proposals are paired with\\nthose whose accumulated scores are similar, and if the proposals are judged to be better, they gain an\\nadditional point. We repeat this for N rounds so the total score of each project proposal will be within\\nthe [0, N] range. As a sanity check, we use the Claude-3.5-Sonnet ranker to rank the 1.2K ICLR\\nLLM-related submissions and compare the average review scores of the top 10 ranked papers and the\\nbottom 10 ranked papers in Table 1. We see a clear separation between the top and bottom ranked\\npapers, indicating the effectiveness of the LLM ranker. We choose N =5 for all our experiments since\\nit gives the best ranking result on this validation set. The top-ranked project proposals from the agent\\nwill be directly used for the AI Ideas condition of the human study.\\nSince our AI ranker is still far from perfect, we also introduce another experiment condition where\\nthe first author of this paper manually reranked the generated project proposals instead of relying\\non the LLM ranker, and we call this the AI Ideas + Human Rerank condition. As we show in\\n4We provide randomly sampled idea pairs and their similarities in Appendix H. We also provide additional implementa-\\ntion details about the ideation agent in Appendix F.\\n6\\nTable 12, 17 out of the 49 ideas in the AI Ideas + Human Rerank condition overlap with the AI\\nIdeas condition, while the other 32 are different, indicating the discrepancy between the LLM ranker\\nand the human expert reranking.\\n4\\nExpert Idea Writing and Reviewing\\nIn this section, we shift focus to the human branch of idea generation comparison. We present the\\ndetails of our human study, including information about the recruited experts, the human idea\\ngeneration task, and the subsequent review process.\\n4.1\\nExpert Recruitment\\nWe recruit our expert participants (including for idea writing and reviewing) by sending sign-up\\nforms to several channels, including: 1) the OpenNLP Slack channel with 1426 NLP researchers from\\n71 institutions (with consent from the channel manager); 2) Twitter (X); 3) Slack channels of various\\nNLP groups by direct communication with the group members; and 4) official chat app of the NAACL\\n2024 conference. We also conducted in-person recruitment by giving out name cards and wearing\\nT-shirts 5 with sign-up links at the NAACL 2024 conference as well as various other local NLP social\\nevents. Our study has been approved by the Stanford IRB (ID 74246).\\nWe performed screening on all the US participants 6 based on their provided Google Scholar profiles.\\nWe set a minimum requirement of having published at least one paper at a major AI venue. 7 We\\nreached out to all participants who satisfied this requirement with the consent form and followed up\\nwith the annotation documents for those who consented to participate.\\nIn the end, we recruited N =49 experts for writing ideas, and N =79 experts for reviewing ideas. Note\\nthat 24 out of the 79 reviewers also participated in the idea writing, and we made sure no reviewer\\nwould review their own idea. This results in N = 104 total participants across the two tasks. Each\\nidea writer is asked to write one idea within 10 days and we compensate $300 for each, with a $1000\\nbonus for the top 5 ideas as scored by the expert reviewers. Each idea reviewer is assigned 2 to 7 ideas\\nto review and we collected N = 298 unique reviews in total. They are given one week to finish the\\nreviews and we compensated $25 for each review written by the idea reviewers.\\n4.2\\nExpert Qualifications\\nPhD\\n73%\\nMaster\\n18%\\nOther\\n8%\\nPhD\\n79%\\nMaster\\n6%\\nOther\\n5%\\nPostdoc\\n8%\\nFigure 3: Positions of our idea writer\\n(left) and reviewer (right) participants.\\nOur pool of participants is highly qualified and diverse.\\nThe 49 idea writers come from 26 different institutions (Ta-\\nble 15) and the majority of them are current PhD students\\n(Figure 3 left). The 79 reviewers come from 32 institutions\\n(Table 16) and are mostly PhD students and Postdocs (Fig-\\nure 3 right). We use their Google Scholar profiles to extract\\nseveral proxy metrics, including the number of papers,\\ncitations, h-index, and i10-index at the time of their submis-\\nsion. Table 2 shows that our idea writers have an average\\nof 12 papers and 477 citations, while every reviewer has\\npublished at least two papers and has an average citation\\nof 635 and h-index of 7. Moreover, based on their survey\\n5https://x.com/ChengleiSi/status/1804273510656749649\\n6We have to recruit participants located in the US due to logistical reasons.\\n7E.g., *ACL, NeurIPS, ICLR, ICML, AAAI.\\n7\\nIdea Writing Participants (N=49)\\nIdea Reviewing Participants (N=79)\\nMetric\\nMean\\nMedian\\nMin\\nMax\\nSD\\nMean\\nMedian\\nMin\\nMax\\nSD\\npapers\\n12\\n10\\n2\\n52\\n9\\n15\\n13\\n2\\n52\\n10\\ncitations\\n477\\n125\\n2\\n4553\\n861\\n635\\n327\\n0\\n7276\\n989\\nh-index\\n5\\n4\\n1\\n21\\n4\\n7\\n7\\n0\\n21\\n4\\ni10-index\\n5\\n4\\n0\\n32\\n6\\n7\\n5\\n0\\n32\\n6\\nTable 2: Research profile metrics of the idea writing and reviewing participants. Data are extracted\\nfrom Google Scholar at the time of idea or review submission.\\nMetric\\nMean\\nMedian\\nMin\\nMax\\nSD\\nHuman Ideas\\nFamiliarity (1-5)\\n3.7\\n4.0\\n1.0\\n5.0\\n1.0\\nDifficulty (1-5)\\n3.0\\n3.0\\n1.0\\n5.0\\n0.7\\nTime (Hours)\\n5.5\\n5.0\\n2.0\\n15.0\\n2.7\\nLength (Words)\\n901.7\\n876.0\\n444.0\\n1704.0\\n253.5\\nAI Ideas\\nLength (Words)\\n1186.3\\n1158.0\\n706.0\\n1745.0\\n233.7\\nAI + Human Rerank Ideas\\nLength (Words)\\n1174.0\\n1166.0\\n706.0\\n1708.0\\n211.0\\nTable 3: Statistics of the 49 ideas from each condition.\\nresponses, 72 out of the 79 reviewers have previously reviewed for major AI conferences or journals.\\nThese statistics indicate that our participants are highly qualified and have substantial research\\nexperience. 8\\n4.3\\nIdea Writing\\nTopic\\nCount\\nBias\\n4\\nCoding\\n9\\nSafety\\n5\\nMultilingual\\n10\\nFactuality\\n11\\nMath\\n4\\nUncertainty\\n6\\nTotal\\n49\\nTable 4: Idea topic distribution.\\nWe report statistics of our idea writers’ ideas to measure their\\nquality. As shown in Table 3, idea writers indicate a moderately\\nhigh familiarity with their selected topic (3.7 on a 1 to 5 scale),\\nand indicate the task as moderately difficult (3 on a 1 to 5 scale).\\nThey spent an average of 5.5 hours on the task and their ideas\\nare 902 words long on average. These indicate that participants\\nare putting substantial effort into this task. 9 We also show the\\ndistribution of their selected topics in Table 4.\\n4.4\\nIdea Reviewing\\nReview Assignment We let all reviewer participants select their\\ntop two preferred topics as well as their preferred reviewing\\nload (from 2 to 7). We then randomly assign them to ideas within\\ntheir selected topics and all ideas are anonymized. In the assignment, we balance the number of ideas\\nfrom each condition for each reviewer and ensure that each reviewer gets at least one human idea\\nand one AI idea. Every idea is reviewed by 2 to 4 different reviewers. We also avoid assigning ideas\\nwritten by authors from the same institution to avoid any potential contamination. Table 5 shows that\\neach reviewer wrote an average of 3.8 reviews from 2 or 3 conditions, across 1 to 3 topics.\\n8Detailed breakdown of participant positions is in Appendix K.\\n9See Appendix J for more details on the quality control of human ideas.\\n8\\nMetric\\nMean\\nMedian\\nMin\\nMax\\nSD\\nOurs\\nFamiliarity (1-5)\\n3.7\\n3.0\\n1.0\\n5.0\\n0.9\\nConfidence (1-5)\\n3.7\\n4.0\\n1.0\\n5.0\\n0.7\\nTime (Minutes)\\n31.7\\n30.0\\n5.0\\n120.0\\n16.8\\nLength (Word)\\n231.9\\n208.0\\n41.0\\n771.0\\n112.1\\nICLR 2024\\nConfidence (1-5)\\n3.7\\n4.0\\n1.0\\n5.0\\n0.8\\nLength (Word)\\n421.5\\n360.0\\n14.0\\n2426.0\\n236.4\\nLength (Word; Strengths & Weaknesses)\\n247.4\\n207.0\\n2.0\\n2010.0\\n176.4\\nTable 6: Statistics of our collected reviews, with ICLR 2024 reviews as a baseline (for the 1.2K submis-\\nsions that mentioned the keyword “language models\").\\nMetric\\nMean\\nMin\\nMax\\nSD\\n# Reviews\\n3.8\\n2.0\\n7.0\\n1.3\\n# Conditions\\n2.5\\n2.0\\n3.0\\n0.5\\n# Topics\\n1.5\\n1.0\\n3.0\\n0.6\\nTable 5: Statistics of the review assignment.\\nReview Quality Check Apart from ensuring reviewer\\nqualifications, we also compute statistics to measure\\nthe quality of the reviews in Table 6. On average, the\\nreviewers indicated a familiarity of 3.7 (out of 5) in\\ntheir selected topic and a confidence of 3.7 (out of 5) in\\ntheir reviews. This is comparable with the 1.2K ICLR\\n2024 submissions related to language models, where\\nthe reviewers also have an average confidence of 3.7 out of 5. Moreover, reviewers spent an average of\\n32 minutes on each review, with each review being about 232 words long.\\nSince our review forms are different from the ICLR review forms, we compare them with the ICLR\\nreviews where we remove the summary and question sections and only count the lengths of the\\nstrengths and weaknesses sections. This way, the ICLR reviews have an average length of 247, similar\\nto our collected reviews. As an additional measure of review quality, out of the 298 unique reviews\\nthat we have collected, 80 of them provided links to existing papers in their rationales to justify why\\nthe proposed method is not novel. These results further validate the high quality of our review data.\\n5\\nMain Result: AI Ideas Are Rated More Novel Than Expert Ideas\\nIn this section, we present our main finding on whether LLMs can generate better research ideas than\\nexperts. Consistently across three different statistical tests accounting for the possible confounders,\\nwe find that AI ideas have higher novelty scores than human ideas while being comparable on all\\nother metrics.\\n5.1\\nTest 1: Treating Each Review as an Independent Datapoint\\nIn Test 1, we treat each review as an independent datapoint and aggregate all reviews from the same\\ncondition. We treat the Human Ideas as the baseline condition and compare it with AI Ideas and AI\\nIdeas + Human Rerank using two-tailed Welch’s t-tests with Bonferroni correction. We show the\\nbarplot in Figure 2 and the detailed numerical results in Table 7. Both AI Ideas (µ=5.64±σ=1.76)\\nand AI Ideas + Human Rerank (µ = 5.81±σ = 1.66) are significantly better than Human Ideas\\n(µ = 4.84 ± σ = 1.79) on the novelty score (p < 0.01). In this particular test, the AI ideas in both\\nconditions are also significantly better than human ideas on the excitement score (p<0.05), and the\\nAI Ideas + Human Rerank condition is also significantly better than Human Ideas in terms of\\n9\\nCondition\\nSize\\nMean\\nMedian\\nSD\\nSE\\nMin\\nMax\\np-value\\nNovelty Score\\nHuman Ideas\\n119\\n4.84\\n5\\n1.79\\n0.16\\n1\\n8\\n–\\nAI Ideas\\n109\\n5.64\\n6\\n1.76\\n0.17\\n1\\n10\\n0.00**\\nAI Ideas + Human Rerank\\n109\\n5.81\\n6\\n1.66\\n0.16\\n2\\n10\\n0.00***\\nExcitement Score\\nHuman Ideas\\n119\\n4.55\\n5\\n1.89\\n0.17\\n1\\n8\\n–\\nAI Ideas\\n109\\n5.19\\n6\\n1.73\\n0.17\\n1\\n9\\n0.04*\\nAI Ideas + Human Rerank\\n109\\n5.46\\n6\\n1.82\\n0.17\\n1\\n9\\n0.00**\\nFeasibility Score\\nHuman Ideas\\n119\\n6.61\\n7\\n1.99\\n0.18\\n1\\n10\\n–\\nAI Ideas\\n109\\n6.34\\n6\\n1.88\\n0.18\\n2\\n10\\n1.00\\nAI Ideas + Human Rerank\\n109\\n6.44\\n6\\n1.63\\n0.16\\n1\\n10\\n1.00\\nExpected Effectiveness Score\\nHuman Ideas\\n119\\n5.13\\n5\\n1.76\\n0.16\\n1\\n8\\n–\\nAI Ideas\\n109\\n5.47\\n6\\n1.58\\n0.15\\n1\\n10\\n0.67\\nAI Ideas + Human Rerank\\n109\\n5.55\\n6\\n1.52\\n0.15\\n1\\n9\\n0.29\\nOverall Score\\nHuman Ideas\\n119\\n4.68\\n5\\n1.90\\n0.17\\n1\\n9\\n–\\nAI Ideas\\n109\\n4.85\\n5\\n1.70\\n0.16\\n1\\n9\\n1.00\\nAI Ideas + Human Rerank\\n109\\n5.34\\n6\\n1.79\\n0.17\\n1\\n9\\n0.04*\\nTable 7: Scores across all conditions by treating each review as an independent datapoint (Test\\n1). Size is the number of reviews for each condition and the p-values are computed with two-\\ntailed Welch’s t-tests with Bonferroni correction. We bold results that are statistically significant\\n(∗p < 0.05;∗∗p < 0.01;∗∗∗p < 0.001). AI ideas are judged as significantly better than human ideas in\\nterms of novelty and excitement while being comparable on all other metrics.\\nthe overall score (p<0.05). We do not observe significant differences between AI-generated ideas and\\nhuman-written ideas on the other metrics.\\n5.2\\nTest 2: Treating Each Idea as an Independent Datapoint\\nSince we collect multiple reviews for each idea, one could argue that we should not treat each review\\nas an independent datapoint. To account for this potential confounder, we perform Test 2 where\\nwe average the scores of each idea and treat each idea as one datapoint. This way, the sample\\nsize for every condition will be N = 49, namely the number of ideas. We treat the Human Ideas\\nas the baseline condition and compare it with AI Ideas and AI Ideas + Human Rerank using\\ntwo-tailed Welch’s t-tests with Bonferroni correction. As shown in Table 8, we still see significant\\nresults (p < 0.05) where both AI Ideas (µ = 5.62 ± σ = 1.39) and AI Ideas + Human Rerank\\n(µ=5.78±σ=1.07) have higher novelty scores than Human Ideas (µ=4.86±σ=1.26).\\n5.3\\nTest 3: Treating Each Reviewer as an Independent Datapoint\\nAnother possible confounder is that different reviewers might have different biases, for example,\\nsome reviewers may be more lenient than others. To account for such reviewer biases, we perform Test\\n10\\nCondition\\nSize\\nMean\\nMedian\\nSD\\nSE\\nMin\\nMax\\np-value\\nNovelty Score\\nHuman Ideas\\n49\\n4.86\\n5.00\\n1.26\\n0.18\\n1.50\\n7.00\\n–\\nAI Ideas\\n49\\n5.62\\n5.50\\n1.39\\n0.20\\n1.50\\n8.33\\n0.03*\\nAI Ideas + Human Rerank\\n49\\n5.78\\n6.00\\n1.07\\n0.15\\n3.00\\n8.33\\n0.00**\\nExcitement Score\\nHuman Ideas\\n49\\n4.56\\n4.33\\n1.16\\n0.17\\n2.00\\n7.00\\n–\\nAI Ideas\\n49\\n5.18\\n5.50\\n1.33\\n0.19\\n2.50\\n7.33\\n0.08\\nAI Ideas + Human Rerank\\n49\\n5.45\\n5.50\\n1.36\\n0.19\\n1.00\\n7.33\\n0.00**\\nFeasibility Score\\nHuman Ideas\\n49\\n6.53\\n7.00\\n1.50\\n0.21\\n3.00\\n9.00\\n–\\nAI Ideas\\n49\\n6.30\\n6.00\\n1.27\\n0.18\\n2.50\\n8.50\\n1.00\\nAI Ideas + Human Rerank\\n49\\n6.41\\n6.50\\n1.06\\n0.15\\n4.00\\n9.00\\n1.00\\nExpected Effectiveness Score\\nHuman Ideas\\n49\\n5.10\\n5.33\\n1.14\\n0.16\\n3.00\\n7.00\\n–\\nAI Ideas\\n49\\n5.48\\n5.50\\n1.23\\n0.18\\n2.00\\n7.50\\n0.58\\nAI Ideas + Human Rerank\\n49\\n5.57\\n5.50\\n0.99\\n0.14\\n3.00\\n7.50\\n0.17\\nOverall Score\\nHuman Ideas\\n49\\n4.69\\n4.67\\n1.16\\n0.17\\n2.00\\n6.67\\n–\\nAI Ideas\\n49\\n4.83\\n5.00\\n1.34\\n0.19\\n1.50\\n7.50\\n1.00\\nAI Ideas + Human Rerank\\n49\\n5.32\\n5.50\\n1.24\\n0.18\\n2.00\\n7.50\\n0.06\\nTable 8: Scores across all conditions by averaging the scores for each idea and treating each idea as one\\ndata point (Test 2). Size is the number of ideas for each condition, and the p-values are computed with\\ntwo-tailed Welch’s t-tests with Bonferroni correction. We bold results that are statistically significant\\n(∗p<0.05;∗∗p<0.01). AI ideas are judged as significantly better than human ideas in terms of novelty\\nwhile being comparable on all other metrics.\\n3 where we treat each reviewer as one datapoint and compute their average score on each condition.\\nThen for each reviewer, we get their mean score difference between the AI Ideas condition and\\nthe Human Ideas condition, as well as the difference between the AI Ideas + Human Rerank\\ncondition and the Human Ideas condition. This way, we only analyze the differences among the\\ndifferent conditions. That is, if the differences are significantly higher than zero under the one-sample\\nt-test, that indicates reviewers are giving higher scores to one condition compared to the other. The\\nresults are shown in Table 9, and we see significant results (p<0.05) that AI ideas in both the AI Ideas\\nand AI Ideas + Human Rerank conditions are rated more novel than Human Ideas. Therefore,\\nwe conclude that AI ideas generated by our ideation agent are judged as more novel than human\\nexpert generated ideas, consistently across all three different statistical tests. 10\\n6\\nIn-Depth Analysis of the Human Study\\nWhile the above main results highlight the promise of LLMs in generating novel research ideas, there\\nare some additional nuances. In this section, we move beyond the statistical comparisons and dive\\n10We also include results of fitting linear mixed-effects models in Appendix N, which reinforces our conclusions. Addi-\\ntionally, we plot the breakdown of all metrics by topic in Appendix O.\\n11\\nN\\nMean Diff\\np-value\\nNovelty Score\\nAI Ideas vs Human Ideas\\n70\\n0.94\\n0.00**\\nAI Ideas + Human Rerank vs Human Ideas\\n65\\n0.86\\n0.00**\\nExcitement Score\\nAI Ideas vs Human Ideas\\n70\\n0.73\\n0.01*\\nAI Ideas + Human Rerank vs Human Ideas\\n65\\n0.87\\n0.00**\\nFeasibility Score\\nAI Ideas vs Human Ideas\\n70\\n-0.29\\n0.36\\nAI Ideas + Human Rerank vs Human Ideas\\n65\\n-0.08\\n0.74\\nEffectiveness Score\\nAI Ideas vs Human Ideas\\n70\\n0.42\\n0.16\\nAI Ideas + Human Rerank vs Human Ideas\\n65\\n0.39\\n0.16\\nOverall Score\\nAI Ideas vs Human Ideas\\n70\\n0.24\\n0.36\\nAI Ideas + Human Rerank vs Human Ideas\\n65\\n0.66\\n0.01*\\nTable 9: Mean score differences between AI ideas and human ideas by treating each reviewer as a data\\npoint (Test 3). All p-values are computed with one-sample t-tests with Bonferroni correction. We bold\\nresults that are statistically significant (∗p<0.05;∗∗p<0.01).\\ninto other aspects of our collected data. Specifically, we focus on the quality of human ideas, reviewer\\npreferences, and the extent of reviewer agreement.\\n6.1\\nHuman Experts May Not Be Giving Their Best Ideas\\nWe first investigate whether human experts are submitting their best ideas to us. We did a post-\\nstudy survey to understand how idea-writing participants came up with their ideas. Out of the 49\\nparticipants, 37 of them came up with the idea on the spot, while the other 12 already had the idea\\nbefore the study. Furthermore, we asked the survey question: “How does this idea compare to your past\\nresearch ideas (ideas that you actually worked on)? Please answer with a percentile. E.g., this idea is one of my\\ntop 10% ideas.” Our participants indicated that on average their submitted ideas are about the top 43%\\nof all their past ideas. This implies that our collected ideas are likely the median-level ideas from these\\nexpert researchers, which is reasonable given that most of them came up with the idea within the\\n10-day time constraint of the task.\\n6.2\\nReviewers Tend to Focus More on Novelty and Excitement\\nTo gain a deeper understanding of the dynamics between the different metrics in the review process,\\nwe explore whether reviewers focus on specific aspects when evaluating the ideas. We compute the\\npairwise correlation between different metrics in Table 10. The overall score mostly correlates with the\\nnovelty score (r=0.725) and excitement score (r=0.854), while having almost no correlation (r<0.1)\\nwith the feasibility score. This implies that reviewers might be paying more attention to the novelty\\nand excitement aspects of the ideas when they are reviewing.\\n12\\nOverall\\nNovelty\\nExcitement\\nFeasibility\\nEffectiveness\\nOverall\\n–\\n0.725\\n0.854\\n0.097\\n0.642\\nNovelty\\n0.725\\n–\\n0.719\\n-0.073\\n0.357\\nExcitement\\n0.854\\n0.719\\n–\\n-0.031\\n0.565\\nFeasibility\\n0.097\\n-0.073\\n-0.031\\n–\\n0.251\\nEffectiveness\\n0.642\\n0.357\\n0.565\\n0.251\\n–\\nTable 10: Pairwise correlation between different metrics (symmetric matrix).\\n6.3\\nReviewing Ideas is Inherently Subjective\\nFinally, we acknowledge that reviewing is inherently subjective, and reviewing based on ideas\\nrather than executed papers might be even more subjective. We investigate this using inter-reviewer\\nagreement. Specifically, we randomly split reviewers of each paper into half, use one half to rank the\\ntop and bottom 25% of all ideas, and then measure agreement with the held-out set of reviewers. 11 As\\nshown in the first block of Table 11, reviewers have a relatively low agreement (56.1%) despite the\\nfact that we have provided detailed explanations for each metric in our review form. As a baseline\\ncomparison, the NeurIPS 2021 reviewer consistency experiment found 66.0% accuracy using this\\nreviewer agreement metric in the balanced setting (Beygelzimer et al., 2021, Lu et al., 2024). We also\\ncomputed the reviewer agreement using the same metric on the 1.2K ICLR 2024 submissions related\\nto language models, which has a balanced accuracy of 71.9%. While our reviewer agreement is higher\\nthan random (50%), it is generally lower than conference reviewing, most likely due to the higher\\nsubjectivity involved when evaluating ideas without seeing the actual experiment results.\\n7\\nLimitations of LLMs\\nWith our findings from the human study in mind, we now turn to LLM performance to provide\\ninsights that could inform future methods for improving idea generation systems. Our ideation agent\\nis motivated by two potential strengths of LLMs: their ability to scale by generating a vast number\\nof ideas - far more than any human could - and the possibility of filtering these ideas to extract the\\nbest ones from the large pool. In theory, this approach could lead to high-quality ideas by leveraging\\ninference scaling. However, we present empirical evidence that this naive assumption about scaling\\nidea generation has significant limitations.\\n7.1\\nLLMs Lack Diversity in Idea Generation\\nWe adopted an over-generate and rank paradigm in idea generation. This raises the question: is there\\nan upper limit to how many new ideas LLMs can generate? To answer this question, we take a closer\\nlook at 4000 generated seed ideas for each topic.\\nWe encode all raw ideas with all-MiniLM-L6-v2 from Sentence-Transformers. For each idea, we\\ncompute its cosine similarity with all previously generated ideas on the same topic. We consider\\nan idea as a duplicate if it has a similarity of above 0.8 with any of the previously generated ideas.\\nIn Figure 4, we show that as the agent keeps generating new batches of ideas, the percentage of\\nnon-duplicates in newly generated batches keeps decreasing, and the accumulated non-duplicate\\nideas eventually plateau. In fact, out of the 4000 generated seed ideas, there are only 200 non-duplicate\\n11This metric follows the balanced accuracy metric as used in Lu et al. (2024) and avoids the limitations of other agreement\\nmetrics like Krippendorff’s alpha, which require overlapping reviews and would result in a sparse matrix due to the\\nnon-overlapping nature of our reviewer assignments. We do the random splitting 20 times and report the average to reduce\\nvariances.\\n13\\n0\\n500\\n1000 1500 2000 2500 3000 3500 4000\\nTotal Number of Ideas Generated\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nNon-Duplicate Percentage (%)\\nEvolution of Non-Duplicates (%) Across Generations\\n% Non-Duplicates\\n0\\n500\\n1000 1500 2000 2500 3000 3500 4000\\nTotal Number of Ideas Generated\\n0\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200\\nAccumulated Non-Duplicate Ideas\\nAccumulation of Non-Duplicate Ideas Across Generations\\nAccumulated Non-Duplicates\\nFigure 4: Measuring duplication of AI-generated ideas: the left figure plots the percentage of non-\\nduplicate ideas in each new bucket of generated ideas; the right figure plots the accumulated non-\\nduplicate ideas as the agent keeps generating new ideas. All data points are averaged across all topics.\\nunique ideas. This sets a bottleneck on our inference-time scaling since increasing the number of\\ngenerated ideas simply leads to repeating duplicate ideas.\\n7.2\\nLLMs Cannot Evaluate Ideas Reliably\\nConsistency\\nRandom\\n50.0\\nNeurIPS’21\\n66.0\\nICLR’24\\n71.9\\nOurs\\n56.1\\nGPT-4o Direct\\n50.0\\nGPT-4o Pairwise\\n45.0\\nClaude-3.5 Direct\\n51.7\\nClaude-3.5 Pairwise\\n53.3\\n“AI Scientist” Reviewer\\n43.3\\nTable 11:\\nReview score consis-\\ntency among human reviewers\\n(first block) and between humans\\nand AI (second block).\\nMost prior works have adopted LLM-as-a-judge for evaluating\\nresearch ideas (Lu et al., 2024) motivated by the observation that\\nLLMs can have a higher agreement with human evaluators than\\nthe inter-human agreement. However, we offer some empirical\\nevidence that LLMs cannot evaluate ideas reliably yet.\\nConcretely, we use the average review score of each idea to rank\\nthe top and bottom 25% of all our collected human and AI ideas,\\nand use this to benchmark various LLM evaluators. Specifically,\\nwe obtainthe LLMpredicted scores ofall ideas andset themedian\\nscore as the threshold to measure their accuracy on our balanced\\nidea ranking data.\\nIn the second block of Table 11, we compare several different LLM\\nevaluators: 1) directly giving the review criteria and prompting\\nfor a final score (Baek et al., 2024, Li et al., 2024, Yang et al., 2024);\\n2) our pairwise ranker as described in Section 3.3; and 3) the\\n“AI Scientist” reviewer agent (Lu et al., 2024). All of these LLM\\nevaluators have a lower agreement than our expert reviewers’ scores. Even the best LLM evaluator\\n— our own Claude-3.5 pairwise ranker — only achieves an accuracy of 53.3%, lower than our inter-\\nreviewer consistency of 56.1%.\\nEven if AI-human agreement eventually matches or exceeds human-human agreement, simply\\nmeeting this baseline does not imply that AI-as-a-reviewer is meaningful, since we may be trading\\nvariance for bias, where AI reviewers are more consistent but rely on spurious correlations (Durmus\\net al., 2022). Our findings in Table 11 are consistent with these brittleness concerns, as we find a\\nsignificant drop in AI-human agreement scores under our study compared to the original studies.\\nFinally, even though Claude-3.5 pairwise agreements may seem close to human agreement, many\\nother pieces of evidence throughout the paper leads us to be cautious about the use of LLM-as-a-judge\\n14\\nin such a complex and subjective task. These include our findings on the significant discrepancy\\nbetween the agent’s top-ranked ideas and the human expert’s top-ranked ideas (Appendix I) and how\\nthe AI Ideas + Human Rerank condition tends to score higher than the AI Ideas condition on\\nall metrics in Section 5. These limitations of LLM auto-evaluation not only constrain the effectiveness\\nof our over-generate-and-rank paradigm for idea generation but also raise concerns about trusting\\nconclusions that are based primarily on LLM evaluators.\\n8\\nQualitative Analysis and Examples\\nIn this section, we offer some qualitative analysis of human- and AI-generated ideas based on our\\ncollected reviews and present four pairs of randomly sampled human and AI ideas as case studies.\\n8.1\\nAnalysis of Free-Text Reviews\\nFollowing recent practices of using LLMs to extract patterns from text corpora (Zhong et al., 2022,\\n2023), we use Claude-3.5 to extract and cluster the main points from all reviews. We then manually\\nverified and labeled each cluster.\\nMany reviews reinforce our quantitative finding that AI ideas tend to be more novel. For example,\\nreviewers noted: “The idea of [...] is quite novel in an in-context learning setting.”, “The idea of\\nexploring [...] using an LLM-based iterative approach is novel.”, “The idea of [...] when constructing\\nprompts to improve cross-lingual transfer is one that I have not heard of before.”, “I like the idea to\\n[...], and think it will be helpful for other researchers in the community.”, “Combining [...] is a unique\\nway of attempting to preserve the gist of the information while likely losing specific identifiers.”, and\\n“Safeguarding using [...] is clearly novel. Similar ideas have not been seen in the related work.”.\\nNext, we summarize some common failure modes of AI ideas:\\n1. Being too vague on implementation details. For example, one reviewer noted: “I’m not\\nsuper clear on the details of this lattice and how the model will be prompted, so I’m not super\\nsure how well the model will complete these subtasks and how well-suited this particular\\nstructure is to completing the overall task.” and another reviewer noted: “\"For analyzing the\\neffectiveness of the method, the proposal only provides a very ad-hoc + hand-wavey suggestion\\nto compare responses across predefined questions.” In another case, the AI idea is criticized for\\nnot considering practical implementation details: “I think in each of the steps, there is something\\nhard to execute. For example, in step Constellation Formation, how do we do the weighted\\nsum?” Similarly, other reviews noted: “It’s unclear how CLIP is connected to the language model\\nand how training a CLIP model would enable the LM to understand images.”, and “There’s\\nno mentioning on how to prompt the model to generate defensive strategies and refine the\\nmodel’s responses using these strategies.” Such vagueness often makes it difficult for reviewers\\nto make confident judgments: “Because this idea is too general and vague, I can’t really answer\\nthe previous question. An idea needs a certain level of details to be determined if it fits for a\\nconference/journal but this one misses them.”\\n2. Misuse of datasets. For example: “I’m not sure about the datasets picked. StereoSet is not a QA\\ndataset; it simply contains statements. Also, I don’t understand why Dialogue NLI responses\\nrequire empathy.”, “I’m concerned the datasets proposed are the right test cases for security of the\\ncode (since they are really just ML/programming problems, not system-level programming).”,\\nand “the choice of datasets might not be the best to show the effect of incorporating multiple\\nperspectives, especially TruthfulQA and ScienceQA, which seems to have a single correct\\ninterpretation and answer.” In another example, the benchmark datasets chosen are considered\\n15\\ntoo easy by the reviewer: “none of the chosen datasets (MATH, GSM8K, and MMLU) uses\\ncomplicated math concepts”.\\n3. Missing or inappropriate baselines. For example: “The proposed method needs to be compared\\nto simply asking the model to think of one (or several) facts about the question before answering\\nusingmoreturns. Thiscouldbeanadditionalbaselinetoverifythescoringprocessismeaningful.”\\nand “Although the proposal includes some baselines that should be compared to, it does not\\nmention some methods which seem to do quite well with LLMs.” Sometimes, “the chosen\\nbaselines may not be suitable”, for example, because they are not directly comparable with the\\nproposed method.\\n4. Making unrealistic assumptions. For example: “The assumption that model can (mostly)\\naccurately flag its own hallucinations is quite tricky.”, “there is a presupposed assumption that\\nhallucinations in LLMs are ungrounded and independent of the data they are trained on, which\\nis generally not considered true”, “The big issue for the effectiveness of the proposed method is\\nthat, it asserts very strong assumptions on downstream tasks, such as there must exist only two\\nextremes.”, “Some assumptions (e.g., [...]) are unlikely to be true in practice, especially when\\nlow-resource languages and less represented cultures are included in the study.”, and “A major\\nassumption in this approach is that the model is able to [...]. However, [...]”.\\n5. Being too resource-demanding. Despite the fact that we explicitly prompted the agent to\\nconsider feasibility when generating ideas, some of the generated ideas are still too resource-\\ndemanding. For example, one reviewer noted: “The biggest issue to feasibility I see is that the\\nproject calls for fine-tuning BLOOM (See step 5). BLOOM has 176B parameters so it’s going to\\ntake quite a lot of GPUs to fine-tune. From a systems perspective, I see this as causing delays.”\\nIn some other cases, manual data annotation is being criticized for feasibility: “The bottleneck\\nseems to be the dataset collection process if there are no existing datasets that fit the requirements\\nof the paper.”, and “the manual evaluation by native speakers or cultural experts could be\\ntime-consuming and resource-intensive”.\\n6. Not well-motivated. For example: “Not well-motivated and there is not a clear intuition that\\nthis work can work to increase the factuality.”, “And in general the method is not well-motivated\\nand needs reasons why retrieving from model itself is meaningful by use cases or specific tasks.”,\\nand “The idea simply doesn’t make sense to me. Given current LLMs’ ability, I’m pretty sure\\nthey can simply recite code like inserting data to a binary search tree.”\\n7. Not adequately following existing best practices. For example: “The proposal does not seem\\nto include awareness of what has been previously tried, or more strategic ways to evaluate\\nsuccess/failures...”\\nWe contrast these with some of the unique strengths and weaknesses of human ideas:\\n1. Human ideas are generally more grounded in existing research and practical considerations,\\nbut may be less innovative. For example, these ideas might be applying existing techniques\\nto new problems: “Multilinguality as a debiasing method has already been considered in the\\nliterature, although not necessarily in the prompt engineering framework.” Sometimes people\\napply incremental changes to existing techniques: “The overall idea shares quite a similar idea\\nwith program-of-thought (PoT). The only difference is that there is an additional step where an\\nLLM is prompted to decide whether to use code or not.” Some ideas try to combine existing\\ntechniques: “Query decomposition and RAG separately are well studied, if there is no existing\\nwork that combines both (which I’m not aware of), then it’s reasonably novel.” As some reviewers\\n16\\nnoted, human ideas tend to build on known intuitions and results: “There are already existing\\nworks on using available lexicons to improve the translation capabilities of LLMs in general.”\\n2. Human ideas tend to be more focused on common problems or datasets in the field. For\\nexample: “The problem of models not handling negation properly is a very common problem,\\nespecially among propriety LMs such as claude-3-5-sonnet.”, “The data exist. This project mainly\\nentails plugging in these datasets to a prompt template and finetuning for a bit. There is little left\\nunspecified, and it should be quite simple to execute on.”, “I haven’t found any work using this\\nidea to solve this specific problem, but [...] is definitely not new.”, and “While existing works\\nhave explored the problem of calibration in long-form answers (e.g. [...]), the specific method for\\ncalibration is different.”\\n3. Human ideas sometimes prioritize feasibility and effectiveness rather than novelty and\\nexcitement. For example, reviewers noted: “I don’t think this will be a groundbreaking finding,\\nbut it will probably work.” and “while the idea is promising and could lead to significant\\nimprovements, it may not be groundbreaking enough to be considered transformative or worthy\\nof a best paper award”.\\n8.2\\nRandomly Sampled Human and AI Ideas with Reviews\\nWe randomly sample four pairs of ideas from different topics to ground our numerical results with\\nactual examples. In each pair, there is one AI idea and one human idea. To save space, we include the\\nfull project proposal of each idea along with the full set of reviews in the Appendix, but we list their\\ntitles, topics, and average scores here for quick reference (we reveal whether each idea is AI-generated\\nor human-written in Appendix X):\\n1. Modular Calibration for Long-form Answers: Appendix P\\nTopic: Uncertainty; Average Overall Score: 5.5\\n2. Semantic Resonance Uncertainty Quantification: Calibrating LLM Confidence through Multi-\\nPath Reasoning: Appendix Q\\nTopic: Uncertainty; Average Overall Score: 6\\n3. Translation with LLMs through Prompting with Long-Form Context: Appendix R\\nTopic: Multilingual; Average Overall Score: 4\\n4. Linguistic Pivot Constellation: Enhancing Cross-Lingual Transfer for Low-Resource Languages\\nand Dialects: Appendix S\\nTopic: Multilingual; Average Overall Score: 6.7\\n5. LLM Directed Retrieval Querying for Improving Factuality: Appendix T\\nTopic: Factuality; Average Overall Score: 4.7\\n6. Semantic Divergence Minimization: Reducing Hallucinations in Large Language Models\\nthrough Iterative Concept Grounding: Appendix U\\nTopic: Factuality; Average Overall Score: 3.3\\n7. Autoprompting: Generate Diverse Few-shot Examples for Any Application: Appendix V\\nTopic: Coding; Average Overall Score: 5\\n8. Temporal Dependency Unfolding: Improving Code Generation for Complex Stateful Systems:\\nAppendix W\\nTopic: Coding; Average Overall Score: 6.7\\n17\\n9\\nRelated Work\\nResearch idea generation and execution. Several prior works explored methods to improve idea\\ngeneration, such as iterative novelty boosting (Wang et al., 2024), multi-agent collaboration (Baek et al.,\\n2024), and multi-module retrieval and revision (Yang et al., 2024). While some of them share similar\\ncomponents as our ideation agent, these works focus on improving the idea generation methods over\\nvanilla prompting baselines, without comparisons to any human expert baselines. Beyond ideation,\\nanother line of work uses LLMs for executing experiments by generating code given the research\\nproblems (Huang et al., 2024, Tian et al., 2024), or combining idea generation with code generation\\nto directly implement AI-generated ideas (Li et al., 2024, Lu et al., 2024). These works either use\\nautomatic evaluation on a pre-defined set of problems and benchmarks, setting a constrained problem\\nspace; or rely on proxy metrics like LLM evaluators, which are often unreliable.\\nLLM for other research-related tasks. LLMs have also been used for several other research-related\\ntasks, such as generating code to perform data-driven discovery (Gu et al., 2024, Guo et al., 2024, Hu\\net al., 2024, Ifargan et al., 2024, Majumder et al., 2024), automatic review generation (D’Arcy et al.,\\n2024, Liang et al., 2024), related work curation (Ajith et al., 2024, Kang and Xiong, 2024, Lehr et al.,\\n2024, Press et al., 2024), experiment outcome prediction (Hewitt et al., 2024, Lehr et al., 2024, Manning\\net al., 2024, Zhang et al., 2024), and future work recommendation (Zhang et al., 2024). Unlike these\\nworks, we tackle the more creative and open-ended task of research ideation.\\nComputational creativity. Our work also connects to the line of work on examining AI’s novelty\\nand diversity in creative tasks. Chakrabarty et al. (2024) found that AI writings are less creative\\nthan professional writers, while we show LLM-generated ideas can be more novel than experts\\non the task of research ideation. Another line of work found that LLM generations lack collective\\ndiversity (Anderson et al., 2024, Zhou et al., 2024), which matches our findings on idea generation.\\nLastly, several other works conducted human evaluation to study the impact of AI exposure or human-\\nAI collaboration on novelty and diversity (Ashkinaze et al., 2024, Liu et al., 2024, Padmakumar and He,\\n2024) with mixed conclusions. While we also conduct a human evaluation of idea novelty, we focus\\non the human-AI comparison on the challenging task of research ideation with expert participants.\\n10\\nDiscussion\\nTo summarize, we compared research ideas generated by our AI agent with ideas written by expert\\nresearchers, and observed the robust finding that expert reviewers rate AI ideas as statistically more\\nnovel than expert ideas. In this section, we discuss some high-level questions readers might have and\\nsuggest some ways to address them.\\nQuestion 1: Do these collected expert ideas represent their best ideas? One might argue that these\\nideas submitted by our idea-writing participants might not represent their best ideas as we discussed\\nin Section 6.1, since most of them came up with the idea on the spot within a short period. In order to\\naddress this concern, we have designed an experiment where we will compare AI ideas with papers\\naccepted at top-tier AI conferences. To avoid any possible contamination, we target the upcoming\\nEMNLP 2024 conference, which will release the accepted papers in October 2024. We have generated\\nAI ideas with our agent on 23 topics from the EMNLP Call For Papers page in July 2024 and cached\\nthem. We pre-registered our analysis plan which also includes the link to the cached ideas. 12 Apart\\nfrom comparing the quality of these ideas, we will also compute the overlap between AI-generated\\nideas and accepted papers on the same topics.\\n12https://osf.io/z6qa4\\n18\\nQuestion 2: Are evaluations based solely on ideas subjective? In this current study, we focused\\nsolely on evaluating the ideas themselves. Ideas that sound novel and exciting might not necessarily\\nturn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas.\\nWe view the current study as a preliminary evaluation of AI-generated ideas. In the next phase, we\\nwill recruit researchers to execute some AI and human-generated ideas into full projects. This will\\nenable reviewers to assess the complete experimental outcomes, providing a more reliable basis for\\nevaluation. Furthermore, it will allow us to analyze whether our initial idea evaluations align with\\nthe assessments of the actual project outcomes.\\nQuestion 3: Why do you focus only on prompting-based research in NLP? The scope of our study\\nis limited to prompting research ideas within NLP. We chose this design to facilitate the next phase\\nof our execution experiment, where we prefer research ideas that are less resource-demanding and\\ncan be executed relatively quickly. We believe that the evaluation protocols we established should be\\napplicable to other research domains as well, although the conclusions could be different depending\\non the research fields. Future work should consider extending such human study to other research\\ndomains and it would be interesting to compare how the conclusions differ.\\nQuestion 4: Can you automate idea execution as well? It is tempting to envision an end-to-end\\nautomated research pipeline where AI agents can implement AI-generated ideas to directly evaluate\\ntheir effectiveness. Apart from speeding up scientific discovery, one could also imagine using such\\nexecution agents to automatically verify experiment results in existing papers or new submissions.\\nWe have also explored building an LLM agent to generate code to implement the generated ideas.\\nSpecifically, we provide a template codebase that consists of: (1) loading datasets from Huggingface\\nor generating synthetic test examples; (2) implementing baseline methods; (3) implementing the\\nproposed method; (3) loading or implementing the evaluation metrics; (4) running experiments on\\nthe testset with the baselines and the proposed method, so that the output of the agent will be a report\\nof the baseline performance as well as the proposed method’s performance. While this agent can\\ngenerate code that compiles and executes, we find that the automated experiments can be misleading\\nbecause the agent often skips or modifies steps in the baselines or proposed methods. In some cases,\\nthe metric functions are also not correctly defined. This highlights the core challenge: just comparing\\nthe final experiment results is not enough; we have to verify the faithfulness of the implementations\\nas well. Performing such implementation verification is not a trivial task, and we leave it to future\\nwork. We provide detailed description of our idea execution agent in Appendix Y.\\n11\\nEthical Considerations\\nPublication Policy. The growing use of AI to generate research ideas raises serious concerns about the\\npotential abuse of these technologies by students or researchers who may flood academic conferences\\nwith low-quality or poorly thought-out submissions. The availability of LLM-generated content\\ncould lead to a decline in the overall quality of academic discourse, as some individuals might take a\\nlazy approach, relying on AI to both generate ideas and review submissions. This would undermine\\nthe credibility and integrity of the review process. The risks are real. Without proper oversight, we\\ncould see a deluge of submissions that lack depth or intellectual merit. To prevent this, it is essential to\\nhold researchers accountable for the outputs generated through AI tools. Rigorous standards must be\\napplied equally to both AI-assisted and human-generated research to ensure that the use of LLMs\\ndoes not result in misleading, superficial, or unethical academic contributions.\\nIntellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity\\naround the concept of intellectual credit. Traditional frameworks for attributing credit in research,\\nbased on human authorship and contribution, become less clear when AI plays a significant role\\n19\\nin idea generation. Questions arise around how to distribute credit between the developers of the\\nLLM, the researchers who designed the frameworks for its use, and the researchers who integrate\\nAI-generated ideas into their work. Furthermore, it becomes increasingly difficult to trace the origins\\nof AI-generated contributions, especially when they draw from vast datasets composed of numerous\\nsources. This complexity calls for a broader rethinking of how intellectual credit is assigned in\\nAI-driven research. While a complete overhaul of legal and academic norms is beyond the scope\\nof this project, we advocate for the adoption of transparent documentation practices. Researchers\\nshould clearly disclose the role AI played in the idea generation process, specifying which models,\\ndata sources, and frameworks were used, and outlining the level of human involvement. This could\\nensure that the credit distribution in AI-supported research is as transparent and fair as possible.\\nPotential for Misuse. AI-generated research ideas, especially those that introduce novel concepts,\\nhave the potential to be misused in ways that could lead to harmful or destabilizing outcomes. For\\ninstance, ideation agents could be leveraged to generate adversarial attack strategies or other unethical\\napplications. This concern aligns with broader arguments from those focused on existential risk\\n(X-risk), who argue that AI-driven innovation could be a primary route to destabilizing the status\\nquo, posing risks at a societal or even global level. Our stance is that such discussions on safety\\nshould be evidence-based to the extent that it is possible, and careful evaluation work is an important\\ncomponent of keeping these discussions grounded in actual, measured capabilities of these systems.\\nWe advocate for continued safety research specifically targeting these types of concerns—such as the\\ndevelopment of Reinforcement Learning from Human Feedback (RLHF) systems or anti-jailbreak\\nmechanisms for research ideation agents. Additionally, we believe it would be meaningful to create\\nsafety benchmarks that assess the ethical and safe application of AI-generated ideas.\\nIdea Homogenization. Our analysis showed that current LLMs lack diversity in idea generation.\\nThis raises important concerns that wide adoption of LLMs can result in idea homogenization, where\\nthe generated ideas only reflect a narrow set of perspectives or have systematic biases. Over time,\\nthis could lead to a reduction in the richness and diversity of research outputs globally. Future work\\nshould develop ways to either improve LLMs themselves or refine our idea generation methods to\\npromote idea diversity. It’s also important to note that our evaluation primarily assesses the quality of\\nthe typical ideas being generated, and may not fully capture the long tail of unique or novel ideas that\\nwould be truly transformative.\\nImpact on Human Researchers. The integration of AI into research idea generation introduces a\\ncomplex sociotechnical challenge, as research is fundamentally a community-driven, collaborative\\neffort. By introducing AI, particularly LLMs, into this social system, we risk unforeseen consequences.\\nOverreliance on AI could lead to a decline in original human thought, while the increasing use of\\nLLMs for ideation might reduce opportunities for human collaboration, which is essential for refining\\nand expanding ideas. To mitigate these risks, future works should explore new forms of human-AI\\ncollaboration, and our results on human reranking of AI ideas show that even naive human-AI\\ncollaboration approaches can be effective. Beyond reranking, humans can play a critical role in the\\nideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for\\nfurther development, and bringing their unique expertise into the process. Understanding how to\\nintegrate LLMs into this collaborative process without disrupting the social fabric of research will\\nbe an important ongoing problem, requiring careful consideration of the broader sociotechnical\\nimplications.\\n20\\nPositionality Statement\\nWe disclose the authors’ anticipated outcomes of the human study before the experiment was con-\\nducted to be transparent about experimenter biases. Among the three authors, Tatsu and Diyi were\\nexpecting a null result from the study while Chenglei expected AI to be better than humans.\\nAcknowledgement\\nWe thank all participants who wrote and reviewed ideas for us. Many of them also provided insightful\\nfeedback on various aspects of this study. This project would not have been possible without their\\nsupport. To ensure the integrity and fairness of phase II of our study, we leave our participants\\nanonymous but will update this manuscript with a detailed acknowledgment of all participants in the\\nproject’s final report.\\nWe thank Rose Wang, Dora Zhao, Irena Gao, Isabel Gallegos, Ken Liu, Aryaman Arora, Harshit Joshi,\\nShi Feng, Tianyu Gao, Xinran Zhao, Yangjun Ruan, Xi Ye, Mert Yuksekgonul, and members of Tatsu\\nLab and SALT Lab for their helpful feedback on the early version of this draft.\\nWe thank our undergraduate intern Isha Goswami and faculty administrator Eric Alejandro Pineda\\nfor assisting with review data collection and financial logistics.\\nThis work was supported by gifts from Open Philanthropy, Tianqiao and Chrissy Chen Institute,\\nMeta, IBM, and Amazon, and grants from ONR, NSF IIS-2247357, and CNS-2308994.\\nReferences\\nAnirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. LitSearch:\\nA Retrieval Benchmark for Scientific Literature Search. ArXiv, abs/2407.18940, 2024.\\nBarrett R Anderson, Jash Hemant Shah, and Max Kreminski. Homogenization Effects of Large\\nLanguage Models on Human Creative Ideation. In Proceedings of the 16th Conference on Creativity &\\nCognition, 2024.\\nJoshua Ashkinaze, Julia Mendelsohn, Qiwei Li, Ceren Budak, and Eric Gilbert. How AI Ideas Affect the\\nCreativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment.\\nArXiv, abs/2401.13481, 2024.\\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. ResearchAgent: Iter-\\native Research Idea Generation over Scientific Literature with Large Language Models. ArXiv,\\nabs/2404.07738, 2024.\\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew\\nGoff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae\\nKwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sandra Mitts, Adithya Renduchintala,\\nStephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, and\\nMarkus Zijlstra. Human-level play in the game of diplomacy by combining language models with\\nstrategic reasoning. Science, 378:1067 – 1074, 2022.\\nAlina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan.\\nThe\\nneurips\\n2021\\nconsistency\\nexperiment.\\nhttps://blog.neurips.cc/2021/12/08/\\nthe-neurips-2021-consistency-experiment, 2021.\\nNeural Information Processing\\nSystems blog post.\\n21\\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R’e, and Azalia\\nMirhoseini. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. ArXiv,\\nabs/2407.21787, 2024.\\nTuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, and Chien-Sheng Wu.\\nArt or Artifice? Large Language Models and the False Promise of Creativity. In CHI, 2024.\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of Thoughts Prompting:\\nDisentangling Computation from Reasoning for Numerical Reasoning Tasks. TMLR, 2023.\\nKatherine M. Collins, Albert Qiaochu Jiang, Simon Frieder, Li Siang Wong, Miri Zilka, Umang Bhatt,\\nThomas Lukasiewicz, Yuhuai Wu, Joshua B. Tenenbaum, William Hart, Timothy Gowers, Wenda\\nLi, Adrian Weller, and Mateja Jamnik. Evaluating language models for mathematics through\\ninteractions. Proceedings of the National Academy of Sciences of the United States of America, 121, 2024.\\nMike D’Arcy, Tom Hope, Larry Birnbaum, and Doug Downey. MARG: Multi-Agent Review Genera-\\ntion for Scientific Papers. ArXiv, abs/2401.04259, 2024.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. Chain-of-Verification Reduces Hallucination in Large Language Models. ArXiv,\\nabs/2309.11495, 2023.\\nShizhe Diao, Pengcheng Wang, Yong Lin, Xiang Liu, and Tong Zhang. Active Prompting with\\nChain-of-Thought for Large Language Models. In ACL, 2024.\\nEsin Durmus, Faisal Ladhak, and Tatsunori B. Hashimoto. Spurious Correlations in Reference-Free\\nEvaluation of Text Generation. In Annual Meeting of the Association for Computational Linguistics, 2022.\\nURL https://api.semanticscholar.org/CorpusID:248300077.\\nKen Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran\\nPan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike A. Merrill, Jeffrey\\nHeer, and Tim Althoff. BLADE: Benchmarking Language Model Agents for Data-Driven Science.\\nArXiv, abs/2408.09667, 2024.\\nSiyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. DS-Agent: Automated\\nData Science by Empowering Large Language Models with Case-Based Reasoning. In ICML, 2024.\\nLuke Hewitt, Ashwini Ashokkumar, Isaias Ghezae, and Robb Willer. Predicting Results of Social\\nScience Experiments Using Large Language Models. Preprint, 2024. URL https://docsend.\\ncom/view/ity6yf2dansesucf.\\nXueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu,\\nMing Zhu, Yao Cheng, Jianbo Yuan, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. InfiAgent-\\nDABench: Evaluating Agents on Data Analysis Tasks. In ICML, 2024.\\nQian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating Language\\nAgents on Machine Learning Experimentation. In ICML, 2024.\\nTal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, and Roy Kishony. Autonomous LLM-driven\\nresearch from data to human-verifiable research papers. ArXiv, abs/2404.17605, 2024.\\nHao Kang and Chenyan Xiong. ResearchArena: Benchmarking LLMs’ Ability to Collect and Organize\\nInformation as Research Agents. ArXiv, abs/2406.10291, 2024.\\n22\\nMichelle S. Lam, Janice Teoh, James Landay, Jeffrey Heer, and Michael S. Bernstein. Concept Induction:\\nAnalyzing Unstructured Text with High-Level Concepts Using LLooM. In CHI, 2024.\\nSteven A. Lehr, Aylin Caliskan, Suneragiri Liyanage, and Mahzarin R. Banaji. ChatGPT as Research\\nScientist: Probing GPT’s Capabilities as a Research Librarian, Research Ethicist, Data Generator and\\nData Predictor. Proceedings of the National Academy of Sciences of the United States of America, 121 35,\\n2024.\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In NeurIPS, 2020.\\nRuochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. MLR-Copilot: Autonomous Machine\\nLearning Research based on Large Language Models Agents. ArXiv, abs/2408.14033, 2024.\\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom,\\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\\nde, Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven\\nGowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson,\\nPushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\\ncode generation with AlphaCode. Science, 378:1092 – 1097, 2022.\\nWeixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vo-\\ndrahalli, Siyu He, Daniel Scott Smith, Yian Yin, Daniel A. McFarland, and James Zou. Can Large\\nLanguage Models Provide Useful Feedback on Research Papers? A Large-Scale Empirical Analysis.\\nNEJM AI, 1(8), 2024.\\nYiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, and Yun Huang.\\nHow AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an\\nLLM-based Agent. In CHI, 2024.\\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist:\\nTowards Fully Automated Open-Ended Scientific Discovery . ArXiv, abs/2408.06292, 2024.\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder,\\nShashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative Refinement with\\nSelf-Feedback. In NeurIPS, 2023.\\nBodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi, Abhijeetsingh\\nMeena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. Discovery-\\nBench: Towards Data-Driven Discovery with Large Language Models. ArXiv, abs/2407.01725,\\n2024.\\nBenjamin S. Manning, Kehang Zhu, and John J. Horton. Automated Social Science: Language Models\\nas Scientist and Subjects. SSRN Electronic Journal, 2024.\\nVishakh Padmakumar and He He. Does Writing with Language Models Reduce Content Diversity?\\nIn ICLR, 2024.\\nOri Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, and Matthias Bethge.\\nCiteME: Can Language Models Accurately Cite Scientific Claims? ArXiv, abs/2407.12861, 2024.\\n23\\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu,\\nDonald Metzler, Xuanhui Wang, and Michael Bendersky. Large Language Models are Effective Text\\nRankers with Pairwise Ranking Prompting. In ACL, 2024.\\nNils Reimers and Iryna Gurevych. Making Monolingual Sentence Embeddings Multilingual using\\nKnowledge Distillation. In EMNLP, 2020.\\nSander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yin-\\nheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, Pranav Sandeep Dulepet, Saurav Vidyad-\\nhara, Dayeon Ki, Sweta Agrawal, Chau Pham, Gerson C. Kroiz, Feileen Li, Hudson Tao, Ashay\\nSrivastava, Hevander Da Costa, Saloni Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe Sarli,\\nIgor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal Anadkat, Alexander Miserlis\\nHoyle, and Philip Resnik. The Prompt Report: A Systematic Survey of Prompting Techniques.\\nArXiv, abs/2406.06608, 2024.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer,\\nand Wen tau Yih. REPLUG: Retrieval-Augmented Black-Box Language Models. In NAACL, 2024.\\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan L. Boyd-Graber, and\\nLijuan Wang. Prompting GPT-3 To Be Reliable. In ICLR, 2023.\\nMüge Simsek, Mathijs de Vaan, and Arnout van de Rijt. Do grant proposal texts matter for funding\\ndecisions? a field experiment. Scientometrics, 129:2521–2532, 2024.\\nMinyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland\\nHaas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh,\\nChenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Min Zhu, Kilian Lieret, Yanxin\\nLu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, E. A. Huerta, and Hao Peng.\\nSciCode: A Research Coding Benchmark Curated by Scientists. ArXiv, abs/2407.13168, 2024.\\nTrieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad geometry without\\nhuman demonstrations. Nature, 625:476 – 482, 2024.\\nQingyun Wang, Doug Downey, Heng Ji, and Tom Hope. SciMON: Scientific Inspiration Machines\\nOptimized for Novelty. In ACL, 2024.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-\\nConsistency Improves Chain of Thought Reasoning in Language Models. In ICLR, 2023.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le,\\nand Denny Zhou. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In\\nNeurIPS, 2022.\\nOrion Weller, Marc Marone, Nathaniel Weir, Dawn J Lawrie, Daniel Khashabi, and Benjamin Van\\nDurme. “According to . . . ”: Prompting Language Models Improves Quoting from Pre-Training\\nData. In EACL, 2023.\\nJason Weston and Sainbayar Sukhbaatar. System 2 Attention (is something you might need too).\\nArXiv, abs/2311.11829, 2023.\\nZonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and E. Cambria. Large Language\\nModels for Automated Open-domain Scientific Hypotheses Discovery. ACL Findings, 2024.\\n24\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\\nNarasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In\\nNeurIPS, 2023.\\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed Huai\\nhsin Chi, and Denny Zhou. Large Language Models as Analogical Reasoners. In ICLR, 2024.\\nXingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen,\\nDongsub Shim, Honglak Lee, and Qiaozhu Mei. MASSW: A New Dataset and Benchmark Tasks for\\nAI-Assisted Scientific Workflows. ArXiv, abs/2406.06357, 2024.\\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed Huai hsin Chi, Quoc V.\\nLe, and Denny Zhou. Take a Step Back: Evoking Reasoning via Abstraction in Large Language\\nModels. In ICLR, 2024.\\nRuiqi Zhong, Charles Burton Snell, Dan Klein, and Jacob Steinhardt. Describing Differences between\\nText Distributions with Natural Language. In ICML, 2022.\\nRuiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal Driven\\nDiscovery of Distributional Differences via Language Descriptions. In NeurIPS, 2023.\\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-Most Prompting Enables Complex\\nReasoning in Large Language Models. In ICLR, 2023.\\nYilun Zhou, Caiming Xiong, Silvio Savarese, and Chien-Sheng Wu. Shared Imagination: LLMs\\nHallucinate Alike. ArXiv, abs/2407.16604, 2024.\\n25\\nA\\nList of Research Topics\\nWe selected the following list of research topics for our research ideation task:\\n1. Bias: novel prompting methods to reduce social biases and stereotypes of large language models\\n2. Coding: novel prompting methods for large language models to improve code generation\\n3. Safety: novel prompting methods to improve large language models’ robustness against adver-\\nsarial attacks or improve their security or privacy\\n4. Multilingual: novel prompting methods to improve large language models’ performance on\\nmultilingual tasks or low-resource languages and vernacular languages\\n5. Factuality: novel prompting methods that can improve factuality and reduce hallucination of\\nlarge language models\\n6. Math: novel prompting methods for large language models to improve mathematical problem\\nsolving\\n7. Uncertainty: novel prompting methods that can better quantify uncertainty or calibrate the\\nconfidence of large language models\\nWe use these topics descriptions to elicit ideas from both human participants and our LLM agent.\\n26\\nB\\nProject Proposal Template\\nWe give the following project proposal template to both the AI agent and human idea writers.\\n1. Title: A concise statement of the main research question to be used as the paper title.\\n2. Problem Statement: Clearly define the problem your research intends to address. Explain clearly\\nwhy this problem is interesting and important.\\n3. Motivation: Explain why existing methods are not good enough to solve the problem, and explain\\nthe inspiration behind the new proposed method. You should also motivate why the proposed\\nmethod would work better than existing baselines on the problem.\\n4. Proposed Method: Explain how the proposed method works, describe all the essential steps.\\n5. Step-by-Step Experiment Plan: Break down every single step of the experiments, make sure every\\nstep is executable. Cover all essential details such as the datasets, models, and metrics to be used. If\\nthe project involves prompting, give some example prompts for each step.\\n6. Test Case Examples: Give at least two concrete examples. The first example should show how the\\nbaseline method fails on the test case. If there are multiple baselines, give examples for all of them.\\nThe second example should show how the proposed method succeeds on the test case. For each test\\ncase, include the input (test example and the full prompt) and the expected output. You should also\\nprovide an explanation for why the outputs from the proposed prompt are better. If the proposed\\nmethod has multiple steps, break them down into intermediate steps.\\n7. Fallback Plan: Propose some alternative plans for what should the students do if the proposed\\nmethod doesn’t manage to satisfy the success criteria. For example, you can suggest additional\\nanalysis to help debug why the proposed method didn’t work, which could inform alternative new\\nmethods, or just turn the project into an analysis paper instead by offering some interesting ablation\\nand insights.\\n27\\nC\\nProject Proposal Demo Example\\nWe present a manually written demonstration example used for project proposal generation. The\\nexample is summarized from an existing paper (Dhuliawala et al., 2023). This same example is given\\nto both the AI agent as well as the idea-writing experts.\\n1. Title:\\nChain-of-Verification Reduces Hallucination in Large Language Models\\n2. Problem Statement:\\nGeneration of plausible yet incorrect factual information, termed hallucination, is an unsolved issue\\nin large language models.\\n3. Motivation:\\nA majority of the methods for reducing hallucination can be divided into roughly three categories:\\ntraining-time correction, generation-time correction, and via augmentation (tool-use). We want to\\ntake a simpler approach that fully leverages the power of LLM itself. Our key motivation is that large\\nlanguage models, when suitably prompted, can both generate and execute a plan of how to verify\\nthemselves in order to check their own work, and finally incorporate this analysis into an improved\\nresponse.\\n4. Proposed Method:\\nOur overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps:\\n(1) Generate Baseline Response: Given a query, generate the response using the LLM.\\n(2) Plan Verifications: Given both query and baseline response, generate a list of verification\\nquestions that could help to self-analyze if there are any mistakes in the original response.\\n(3) Execute Verifications: Answer each verification question in turn, and hence check the answer\\nagainst the original response to check for inconsistencies or mistakes.\\n(4) Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a\\nrevised response incorporating the verification results.\\nEach of these steps is performed by prompting the same LLM in different ways to obtain the desired\\nresponse.\\n5. Step-by-Step Experiment Plan:\\n1: Gather Datasets: We choose datasets that evaluate factual correctness, including the Multi-\\nSpanQA dataset on closed-book QA and the FactScore dataset on generating biographies.\\n2: Construct Prompts: For the baseline, we use direct prompting where, given a query, we generate\\nleft-to-right as usual using the LLM, with no special tricks. Given that such baseline generations\\nare typically prone to hallucination, CoVe attempts to identify these hallucinations and correct\\nthem in the following steps:\\n(1) Plan Verifications: Conditioned on the original query and the baseline response, the\\nmodel is prompted to generate a series of verification questions that test the factual claims\\nin the original baseline response.\\n(2) Execute Verifications: Given the planned verification questions, the next step is to answer\\nthem in order to assess if any hallucinations exist. The planning prompt conditions on the\\nbaseline response in the first step. The verification questions generated from planning are\\nanswered in the second step, where crucially the context given to the LLM prompt only\\ncontains the questions and not the original baseline response, hence preventing the LLM\\nfrom repeating those answers directly.\\n28\\n(3) Generate Final Verified Response: Finally, the improved response that takes verifica-\\ntion into account is generated. This is executed by a final few-shot prompt where the\\ncontext takes into account all of the previous reasoning steps, the baseline response, and\\nverification question-answer pairs, so that the corrections can take place.\\n3: Select Models: We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as\\nthe open-source LLaMA-2-70B-chat.\\n4: Get Results: Get answer predictions from the models on these datasets with both the baselines\\nand the proposed method.\\n5: Analyze Results: Compare whether the new method improves the performance of LLMs in\\nthese tasks as compared to the baselines.\\n6. Test Case Examples:\\n• Baseline Prompt Input (Direct Prompting): Name some politicians who were born in NY, New\\nYork.\\n• Baseline Prompt Expected Output (Direct Prompting): Here are some politicians who were\\nborn in NY, New York:\\n1. Hillary Clinton - former secretary of state and former Democratic presidential nominee.\\n2. Donald Trump - former president of the United States.\\n3. Michael Bloomberg - former Mayor of New York City and former Democratic presidential\\ncandidate.\\n• ProposedPromptInput(Chain-of-VerificationPrompting; Step1: PlanVerifications): Generate\\na set of verification questions to verify each answer.\\n• Proposed Prompt Expected Output (Chain-of-Verification Prompting; Step 1: Plan Verifica-\\ntions):\\n– Where was Hillary Clinton born?\\n– Where was Donald Trump born?\\n– Where was Michael Bloomberg born?\\n• Proposed Prompt Input (Chain-of-Verification Prompting; Step 2: Execute Verifications):\\nAnswer each of the verification questions.\\n• Proposed Prompt Expected Output (Chain-of-Verification Prompting; Step 2: Execute Verifica-\\ntions):\\n1. Hillary Clinton was born in Chicago, Illinois, United States on October 26, 1947.\\n2. Donald Trump was born on June 14, 1946, in Queens, New York City, New York, United\\nStates.\\n3. Michael Bloomberg was born on February 14, 1942, in Boston, Massachusetts, United States.\\n• Proposed Prompt Input (Chain-of-Verification Prompting; Step 3: Final Response): (Prepend\\nthe original question and the baseline response and the verification QA pairs) Given the verifica-\\ntion answers, generate a revised response incorporating the verification results.\\n• Proposed Prompt Expected Output (Chain-of-Verification Prompting; Step 3: Final Response):\\nHere are some politicians who were born in NY, New York:\\n1. Donald Trump - former president of the United States.\\n2. Alexandria Ocasio-Cortez - Democratic member of the U.S. House of Representatives.\\n• Explanation: Given a user query, a large language model with direct prompting generates a\\nbaseline response that may contain inaccuracies, e.g., factual hallucinations. To improve this,\\nChain-of-Verification first generates a plan of a set of verification questions to ask, and then\\n29\\nexecutes that plan by answering them and hence checking for agreement. We find that individual\\nverification questions are typically answered with higher accuracy than the original accuracy of\\nthe facts in the original longform generation. Finally, the revised response takes into account the\\nverifications.\\n7. Fallback Plan:\\nIf the proposed method does not help as compared to the baseline, analyze each step of the CoVe\\nprocess to see if the verification questions are relevant, if the answers to the verification questions\\nare correct, and whether the generated final verified response is indeed improved over the baseline\\nresponse by considering the verification QA pairs. This can help us debug the proposed method or\\nturn this into interesting analysis on the model’s ability to verify and correct its own responses.\\n30\\nD\\nStyle Standardization Prompt\\nStyle Standardization Prompt\\nYou are a writing assistant specialized in editing academic writing. I will give you a student’s research idea and an\\nidea template. Your task is to edit the student’s idea to follow the template’s format.\\nStudent idea: (Insert the student’s idea here)\\nTemplate: (Insert the template idea here)\\nMake sure that you only edit the wording and formatting, including things like punctuation, capitalization,\\nlinebreaks, and bullet points. Also make sure to edit any informal wording and phrasing to use vocabulary that\\nsounds like the template’s writing style. No other changes are allowed beyond these.\\nThe main sections should be indexed clearly without indentation at the beginning. The title section does not need\\nindexing; other sections, including problem statement, motivation, proposed method, step-by-step experiment\\nplan, test case examples, and fallback plan, should be indexed 1 to 6. Each section can then have sub-bullets for\\nsub-sections if applicable. Leave an empty line after each section.\\nYou should use tab as indentation and make sure to use appropriate nested indentation for sub-bullets. All bullets\\nshould have a clear hierarchy so people can easily differentiate the sub-bullets. Only leave empty lines between\\nsections and remove any extra line breaks. If many bullet points are clustered together in a paragraph, separate\\nthem clearly with indentation and appropriate bullet point markers. Change to a new line for each new bullet point.\\nFor the fallback plan, do not list a bunch of bullet points. Instead, condense them into one coherent paragraph.\\nFor line breaks, avoid Raw String Literals or Double Backslashes when using \"\\\\n\", and change them to spaces or\\ntabs.\\nFor in-line citations, if the citation mentioned the author’s last name (like \"(Si et al., 2023)\" or \"(An et al., 2024)\"), you\\nshould keep them there; but if the citation is just a number (like \"[1]\" or \"[3,4,5]\"), you should just remove it and do\\nsome necessary rephrasing to make the sentence still sound coherent without the references.\\nApart from minor rephrasing and changing formatting, do not change any content of the idea. You must preserve\\nthe exact meaning of the original idea, do not change, remove, or add any other details. Do not drop any sections\\n(including test case examples). Do not rename any models, datasets, or methods. Do not drop clarification or\\nexamples in brackets and do not drop any data source mentions (e.g., Chatbot Arena or Wildchat)! Note that\\nwhen indexing test case examples, each test case example could have multiple steps of inputs and outputs and you\\nshouldn’t give separate indices to them. Each test case example should be a whole set of input-output pairs for the\\nbaseline(s) and proposed method.\\nFor the proposed method section, avoid any big changes. If the section comes in as a coherent paragraph, you don’t\\nhave to break it down into bullet points. If the section is already in bullet points, you should keep it that way. If the\\nsection is a mix of both, you should keep the bullet points and the coherent paragraph as they are.\\nKeep all the clarification and examples mentioned in all the sections and do not remove any of them (including those\\nin brackets).\\nFor model selection, if any version of Claude is mentioned, change it to the latest version of Claude (Claude-3.5); if\\nany version of LLaMA is mentioned, change it to the latest version LLaMA-3. Do not make any other model changes.\\nNow directly generate the edited student idea to match the format of the template.\\n31\\nE\\nIdea Review Form\\nWe use the following review form to elicit reviews from all expert reviewers. Reviewers have one\\nweek of time to finish each review.\\n1. Name\\n2. Institution\\n3. Email\\n4. Consent\\n5. Honor Code: I confirm that I will not use ChatGPT, Claude, Gemini, or any other AI tools when\\nwriting my reviews.\\n6. Familiarity: Before reviewing the idea, please indicate how familiar you are with the given topic on\\na scale of 1 - 5 (this is just for us to understand potential confounders).\\n1. You have never read about this topic before\\n2. You have read at least one paper on this topic\\n3. You have read multiple papers on this topic but have not published any paper on it\\n4. You have co-authored at least one paper on this topic\\n5. You have co-authored multiple papers on this topic or have published at least one first-author\\npaper on this topic\\n7. Experience: Have you reviewed for major NLP or AI conferences before (e.g., *ACL, COLING,\\nNeurIPS, ICLR, ICML, AAAI)?\\n8. Full Research Idea Proposal\\n9. Novelty Score: Whether the idea is creative and different from existing works on the topic, and\\nbrings fresh insights. You are encouraged to search for related works online. You should consider all\\npapers that appeared online prior to July 2024 as existing work when judging the novelty.\\n1. Not novel at all - there are many existing ideas that are the same\\n2.\\n3. Mostly not novel - you can find very similar ideas\\n4.\\n5. Somewhat novel - there are differences from existing ideas but not enough to turn into a new\\npaper\\n6. Reasonably novel - there are some notable differences from existing ideas and probably enough\\nto turn into a new paper\\n7.\\n8. Clearly novel - major differences from all existing ideas\\n9.\\n10. Very novel - very different from all existing ideas in a very interesting and clever way\\n10. Novelty Rationale: Short justification for your score. If you give a low score, you should specify\\nsimilar related works. (Your rationale should be at least 2-3 sentences.)\\n32\\n11. Feasibility Score: How feasible it is to implement and execute this idea as a research project?\\nSpecifically, how feasible the idea is for a typical CS PhD student to execute within 1-2 months of time.\\nYou can assume that we have abundant OpenAI / Anthropic API access, but limited GPU compute.\\n1. Impossible: the idea doesn’t make sense or the proposed experiments are flawed and cannot be\\nimplemented\\n2.\\n3. Very challenging: there are flaws in the proposed method or experiments, or the experiments\\nrequire compute/human resources beyond any academic lab\\n4.\\n5. Moderately feasible: It can probably be executed within the given time frame but would require\\ncareful planning, efficient use of APIs or some advanced computational strategies to overcome\\nthe limited GPU resources, and would require some modifications to the original proposal to\\nmake it work\\n6. Feasible: Can be executed within the given constraints with some reasonable planning\\n7.\\n8. Highly Feasible: Straightforward to implement the idea and run all the experiments\\n9.\\n10. Easy: The whole proposed project can be quickly executed within a few days without requiring\\nadvanced technical skills\\n12. Feasibility Rationale: Short justification for your score. If you give a low score, you should specify\\nwhat parts are difficult to execute and why. (Your rationale should be at least 2-3 sentences.)\\n13. Expected Effectiveness Score: How likely the proposed idea is going to work well (e.g., better\\nthan existing baselines).\\n1. Extremely Unlikely: The idea has major flaws and definitely won’t work well\\n2.\\n3. Low Effectiveness: The idea might work in some special scenarios but you don’t expect it to\\nwork in general\\n4.\\n5. Somewhat ineffective: There might be some chance that the proposed idea can work better than\\nexisting baselines but the improvement will be marginal or inconsistent\\n6. Somewhat effective: There is a decent chance that the proposed idea can beat existing baselines\\nby moderate margins on a few benchmarks\\n7.\\n8. Probably Effective: The idea should offer some significant improvement over current methods\\non the relevant benchmarks\\n9.\\n10. Definitely Effective: You are very confident that the proposed idea will outperform existing\\nmethods by significant margins on many benchmarks\\n33\\n14. Expected Effectiveness Rationale: Short justification for your score. (Your rationale should be at\\nleast 2-3 sentences.)\\n15. Excitement Score: How exciting and impactful this idea would be if executed as a full project.\\nWould the idea change the field and be very influential.\\n1. Poor: You cannot identify the contributions of this idea, or it’s not interesting at all and you\\nwould fight to have it rejected at any major AI conference\\n2.\\n3. Mediocre: this idea makes marginal contributions and is very incremental\\n4.\\n5. Leaning negative: it has interesting bits but overall not exciting enough\\n6. Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental\\n7.\\n8. Exciting: would deepen the community’s understanding or make major progress in this research\\ndirection\\n9.\\n10. Transformative: would change the research field profoundly and worth a best paper award at\\nmajor AI conferences\\n16. Excitement Rationale: Short justification for your score. (Your rationale should be at least 2-3\\nsentences.)\\n17. Overall Score: Overall score: Apart from the above, you should also give an overall score for\\nthe idea on a scale of 1 - 10 as defined below (Major AI conferences in the descriptions below refer to\\ntop-tier NLP/AI conferences such as *ACL, COLM, NeurIPS, ICLR, and ICML.):\\n1. Critically flawed, trivial, or wrong, would be a waste of students’ time to work on it\\n2. Strong rejection for major AI conferences\\n3. Clear rejection for major AI conferences\\n4. Ok but not good enough, rejection for major AI conferences\\n5. Decent idea but has some weaknesses or not exciting enough, marginally below the acceptance\\nthreshold of major AI conferences\\n6. Marginally above the acceptance threshold of major AI conferences\\n7. Good idea, would be accepted by major AI conferences\\n8. Top 50% of all published ideas on this topic at major AI conferences, clear accept\\n9. Top 15% of all published ideas on this topic at major AI conferences, strong accept\\n10. Top 5% of all published ideas on this topic at major AI conferences, will be a seminal paper\\n18. Overall Rationale: You should also provide a rationale for your overall score. (Your rationale\\nshould be at least 2-3 sentences.)\\n34\\n19. Confidence: Additionally, we ask for your confidence in your review on a scale of 1 to 5 defined as\\nfollowing:\\n1. Your evaluation is an educated guess\\n2. You are willing to defend the evaluation, but it is quite likely that you did not understand central\\nparts of the paper\\n3. You are fairly confident that the evaluation is correct\\n4. You are confident but not absolutely certain that the evaluation is correct\\n5. You are absolutely certain that the evaluation is correct and very familiar with the relevant\\nliterature\\n20. Time: How many minutes did you spend on this task?\\n35\\nF\\nIdea Generation Agent: Additional Implementation Details\\nSeed Idea Generation\\nDue to the max output length limit of the LLM API, we first generate a large\\nnumber of shorter seed ideas. We keep the seed ideas short so that we can explore more different\\nideas given the same output token budget. We provide a demonstration example of the seed idea in\\nAppendix G. Then, we perform duplication and expand each remaining seed idea into a full project\\nproposal following our standard template in Appendix B.\\nRetrieval Augmentation\\nWe apply retrieval augmentation to the idea generation prompt in order to\\nincrease diversity in the idea generation. To maximize diversity, we apply retrieval augmentation\\nhalf of the time when generating seed ideas, and we randomly select k =10 papers from the top 20\\nretrieved papers when applying retrieval augmentation.\\nIdea Filtering\\nAfter expanding seed ideas into full project proposals, we did some basic filtering to\\nremove any project proposals that failed the novelty and feasibility checks:\\n1. Novelty: We use the literature review module to retrieve the top 10 most relevant papers to the\\ngenerated idea and ask the LLM to compare each of them to the generated idea. The idea will be\\nfiltered as long as any one of the retrieved papers is judged as equivalent.\\n2. Feasibility: The idea will be filtered if it requires extensive manual labor or hardware resources\\nbeyond the capacity of a typical academic lab. The idea will also be filtered if it involves any\\ninconsistency in the experimental setups or assumptions. For example, if the idea assumes\\nonly black-box API access of the LLMs, then it shouldn’t involve experiments that need internal\\nweight access.\\nThis filtered out about 1% of the generated project proposals.\\n36\\nG\\nDemonstration Example: Seed Idea Generation\\nWe present a demonstration example used for seed idea generation. The example is summarized from\\nan existing paper (Dhuliawala et al., 2023).\\nTitle:\\nChain-of-Verification Prompting\\nProblem:\\nGeneration of plausible yet incorrect factual information, termed hallucination, is an unsolved issue\\nin large language models.\\nExisting Methods:\\nA majority of the methods for reducing hallucination can be divided into roughly three categories:\\ntraining-time correction; generation-time correction; and via augmentation (tool-use).\\nMotivation:\\nA key observation is that large language models, when suitably prompted, can both generate and\\nexecute a plan of how to verify themselves in order to check their own work, and finally incorporate\\nthis analysis into an improved response.\\nProposed Method:\\nOur overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps:\\n(1) Generate Baseline Response: Given a query, generate the response using the LLM.\\n(2) Plan Verifications: Given both query and baseline response, generate a list of verification\\nquestions that could help to self-analyze if there are any mistakes in the original response.\\n(3) Execute Verifications: Answer each verification question in turn, and hence check the answer\\nagainst the original response to check for inconsistencies or mistakes.\\n(4) Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a\\nrevised response incorporating the verification results.\\nEach of these steps is performed by prompting the same LLM in different ways to obtain the desired\\nresponse.\\nExperiment Plan:\\nCompare with zero-shot prompting, Chain-of-Thought, and few-shot prompting on the MultiSpanQA\\ndataset on closed-book QA and FactScore dataset on generating biographies.\\n37\\nH\\nGenerated Seed Ideas and Their Nearest Neighbors\\nWe present several randomly sampled generated seed ideas (see Appendix F for the definition of seed\\nideas) on the topic of “novel prompting methods that can better quantify uncertainty or calibrate\\nthe confidence of large language models”. For each idea, we show the most similar idea (nearest\\nneighbor) based on the embedding similarity, along with the similarity score. In practice, we set a\\nthreshold threshold of 0.8 for determining whether two ideas are duplicates.\\nIdea 1:\\nTitle: Adaptive Precision Boundary Probing\\nProblem: LLMs often provide uncertainty estimates that are either too coarse-grained or inappropri-\\nately precise, failing to adapt to the inherent ambiguity or precision requirements of different queries.\\nExisting Methods: Existing uncertainty quantification methods typically use fixed precision scales\\nor calibration techniques that don’t adapt to the specific context and precision requirements of each\\nquery.\\nMotivation: Human experts adjust the precision of their uncertainty estimates based on the nature of\\nthe question and the available evidence. We can incorporate this adaptive approach to improve LLM\\nuncertainty quantification.\\nProposed Method: We introduce Adaptive Precision Boundary Probing (APBP), a dynamic prompt-\\ning technique that iteratively refines the precision of uncertainty estimates. Given a query, APBP\\nstarts with a coarse-grained confidence interval. It then prompts the model to assess whether this\\ninterval is appropriately precise given the query’s context and the model’s knowledge. If the model\\ndetermines that greater precision is warranted, APBP iteratively narrows the interval, prompting\\nthe model at each step to justify the increased precision. Conversely, if the model recognizes high\\nambiguity or limited knowledge, APBP widens the interval. Throughout this process, the model is\\nasked to explicitly reason about the factors influencing the appropriate level of precision, such as the\\nspecificity of the query, the reliability of relevant knowledge, and potential sources of ambiguity. The\\nfinal output is an uncertainty estimate with a precision level tailored to the specific query and the\\nmodel’s knowledge state.\\nExperiment Plan: We will evaluate APBP on a diverse set of tasks with varying inherent precision\\nrequirements, including numerical estimation, date prediction, and open-ended text generation. We’ll\\ncompare APBP against fixed-precision uncertainty estimation methods, measuring both calibration\\naccuracy and the appropriateness of precision levels as judged by human experts.\\nNearest Neighbor of Idea 1:\\nTitle: Contextual Confidence Oscillation\\nProblem: Current methods for quantifying uncertainty in large language models often fail to capture\\nthe dynamic nature of confidence across different contexts within a single query.\\nExisting Methods: Most existing approaches use static confidence scores or calibration techniques\\nthat don’t account for intra-query contextual shifts.\\nMotivation: Human confidence often fluctuates as we process different parts of a complex question\\nor task. By mimicking this oscillation, we can potentially capture a more nuanced and accurate\\nrepresentation of model uncertainty.\\nProposed Method: We propose Contextual Confidence Oscillation (CCO), a novel prompting tech-\\nnique that encourages the model to continuously re-evaluate and express its confidence as it processes\\na query. The prompt is structured as a series of checkpoints, where the model must pause its reasoning,\\nreflect on its current confidence level, and explain any changes since the last checkpoint. This creates a\\nconfidence trajectory that can be analyzed for patterns, sudden drops, or gradual increases. Addi-\\n38\\ntionally, we introduce ’confidence disruptors’ - intentionally ambiguous or challenging sub-queries\\ninserted at various points to test the model’s ability to recognize and express increased uncertainty\\nwhen appropriate.\\nExperiment Plan: We will evaluate CCO against standard uncertainty quantification methods on\\na range of tasks, including multi-step reasoning problems, ambiguous queries, and long-form text\\nanalysis. We’ll measure not just overall accuracy of uncertainty estimates, but also the correlation\\nbetween confidence oscillations and human-annotated difficulty levels of different parts of each\\nquery. We’ll also analyze how well the model’s expressed confidence trajectory aligns with its actual\\nperformance across different segments of complex tasks.\\nSimilarity: 0.70\\nIdea 2:\\nTitle: Quantum Superposition Confidence Prompting\\nProblem: Current LLMs struggle to accurately quantify uncertainty across multiple possible answers,\\noften defaulting to overconfidence in a single response.\\nExisting Methods: Existing approaches typically involve single-path reasoning or limited branching,\\nfailing to capture the full spectrum of uncertainty.\\nMotivation: Inspired by quantum mechanics, where particles can exist in multiple states simultane-\\nously, we propose a method that allows LLMs to consider multiple answer possibilities concurrently.\\nProposed Method: We introduce Quantum Superposition Confidence Prompting (QSCP), where the\\nLLM is instructed to generate multiple potential answers simultaneously, assigning confidence scores\\nto each. The prompt encourages the model to ’exist in multiple states,’ exploring contradictory an-\\nswers and their implications concurrently. For example: ’Imagine you are in a quantum superposition\\nof multiple expert personas. Each persona will provide an answer to the following question, along\\nwith a confidence score (0-100%). Ensure the personas explore contradictory viewpoints. Question:\\n[INSERT QUESTION]’. The LLM then generates responses from multiple personas, each with its own\\nconfidence score. The final uncertainty is derived from the distribution of these scores, providing a\\nmore nuanced understanding of the model’s confidence across possible answers.\\nExperiment Plan: Compare QSCP against standard prompting, chain-of-thought, and other uncer-\\ntainty quantification methods on diverse question-answering datasets. Evaluate using metrics such\\nas calibration error, Brier score, and a novel ’quantum uncertainty score’ that measures the spread and\\ncoherence of the generated answer superposition.\\nNearest Neighbor of Idea 2:\\nTitle: Quantum Superposition Prompting\\nProblem: Traditional methods for uncertainty quantification in large language models often fail to\\ncapture the full range of possible interpretations and outcomes, especially for queries with inherent\\nambiguity or multiple valid perspectives.\\nExisting Methods: Current approaches typically focus on generating a single response with an\\nassociated confidence score, or at best, a small set of discrete alternatives.\\nMotivation: Drawing inspiration from the principle of superposition in quantum mechanics, we pro-\\npose a method to represent and reason about multiple possible outcomes simultaneously, providing a\\nricher and more nuanced uncertainty quantification.\\nProposed Method: We present Quantum Superposition Prompting (QSP), a novel framework for\\nexploring and quantifying uncertainty in language model outputs. QSP begins by prompting the\\nmodel to generate a ’superposition’ of possible interpretations or approaches to the given query. Each\\nelement in this superposition is assigned a complex amplitude, representing both its probability\\n39\\nand its relationship to other elements. The model is then guided through a series of ’measurement’\\nprompts, designed to collapse this superposition along different bases of interpretation. These mea-\\nsurements yield probability distributions over outcomes, capturing different facets of uncertainty.\\nQSP employs techniques inspired by quantum computing, such as interference and entanglement, to\\nmodel how different interpretations interact and influence each other. The final uncertainty quantifi-\\ncation is derived from the full set of measurements, providing a multi-dimensional representation of\\nthe model’s uncertainty that captures ambiguity, conflicting evidence, and the interdependence of\\ndifferent interpretations.\\nExperiment Plan: We will evaluate QSP on tasks that inherently involve multiple valid perspectives\\nor ambiguous interpretations, such as ethical dilemmas, creative writing prompts, and open-ended\\nanalytical questions. Metrics will include the diversity and coherence of generated superpositions, the\\nability to capture human-judged ambiguities, and improvements in uncertainty calibration compared\\nto classical methods.\\nSimilarity: 0.77\\nIdea 3:\\nTitle: Fractal Uncertainty Decomposition\\nProblem: LLMs often provide overly simplistic uncertainty estimates that fail to capture the hierarchi-\\ncal and nested nature of uncertainty in complex knowledge domains.\\nExisting Methods: Current uncertainty quantification methods typically produce flat, single-\\ndimensional confidence scores that don’t reflect the multi-layered structure of knowledge and uncer-\\ntainty.\\nMotivation: By recursively decomposing a query into sub-components and assessing uncertainty\\nat multiple levels of granularity, we can construct a more comprehensive and structurally informed\\nuncertainty estimate.\\nProposed Method: We introduce Fractal Uncertainty Decomposition (FUD), a prompting technique\\nthat recursively breaks down a query into a hierarchical structure of sub-queries, assessing uncertainty\\nat each level. Given an initial query, FUD prompts the model to identify key sub-components or\\naspects of the question. For each sub-component, the model provides an answer and a confidence\\nestimate. If the confidence for a sub-component is below a certain threshold, FUD recursively applies\\nthe same decomposition process to that sub-component. This continues until either a maximum\\ndepth is reached or all sub-components have high confidence. The resulting structure is a tree of\\nnested confidence estimates. FUD then aggregates these estimates bottom-up, using a combination\\nof statistical methods and prompted meta-analysis by the model. The final output is both an overall\\nuncertainty estimate and a detailed map of the uncertainty structure, showing how confidence varies\\nacross different aspects and levels of the query.\\nExperiment Plan: We will evaluate FUD on complex, multi-faceted tasks such as scientific explanation,\\nhistorical analysis, and technical troubleshooting. We will compare its performance to flat confidence\\nestimation methods and other hierarchical approaches. Evaluation metrics will include traditional\\ncalibration measures, as well as new metrics designed to assess the quality and informativeness of the\\nuncertainty decomposition. We will also conduct case studies to demonstrate how FUD can provide\\nmore actionable and interpretable uncertainty information in real-world scenarios.\\nNearest Neighbor of Idea 3:\\nTitle: Semantic Fractal Decomposition\\nProblem: Current uncertainty quantification methods for large language models often fail to capture\\nthe hierarchical and self-similar nature of conceptual understanding, leading to inconsistent confi-\\n40\\ndence estimates across different levels of abstraction.\\nExisting Methods: Existing approaches typically focus on flat, single-level uncertainty estimates or\\nsimple hierarchical decompositions that don’t fully capture the complex, nested nature of semantic\\nunderstanding.\\nMotivation: Drawing inspiration from fractal geometry, where patterns repeat at different scales, we\\npropose a method that recursively decomposes concepts and queries into self-similar sub-components,\\nallowing for a more nuanced and scale-invariant approach to uncertainty quantification.\\nProposed Method: We present Semantic Fractal Decomposition (SFD), a prompting technique that\\nguides the model to recursively break down a given query or concept into smaller, self-similar com-\\nponents. At each level of decomposition, the model is asked to provide a confidence estimate. The\\nprocess continues until a predefined depth is reached or the model indicates it can no longer meaning-\\nfully decompose the concept. The final uncertainty estimate is then constructed by aggregating these\\nmulti-level confidence scores using a novel fractal dimension-inspired algorithm. This approach\\nallows for capturing uncertainty that may be present at different semantic scales and provides a more\\nrobust and consistent measure of the model’s confidence across varying levels of abstraction.\\nExperiment Plan: We will evaluate SFD on a diverse set of tasks ranging from simple factual queries\\nto complex, multi-faceted questions in domains like philosophy, science, and law. We will compare\\nits performance against traditional flat confidence estimation techniques and simpler hierarchical\\nmethods. Key metrics will include the consistency of uncertainty estimates across related queries at\\ndifferent levels of abstraction, the correlation between fractal-aggregated confidence scores and actual\\nmodel performance, and the interpretability of the decomposition process.\\nSimilarity: 0.81\\n41\\nI\\nOverlap Between AI Ranking and Expert Reranking\\nWe show the overlap between the AI Ideas condition and the AI Ideas + Human Rerank condi-\\ntions in Table 12. We note that 17 out of the 49 ideas in the AI Ideas + Human Rerank condition\\nare also ranked as top ideas in the AI Ideas condition by the AI ranker, while the other 32 are not.\\nTopic\\nOverlap\\nNew\\nBias\\n2\\n2\\nCoding\\n4\\n5\\nSafety\\n2\\n3\\nMultilingual\\n5\\n5\\nFactuality\\n2\\n9\\nMath\\n2\\n2\\nUncertainty\\n1\\n5\\nTotal\\n18\\n31\\nTable 12: Overlap of ideas between AI + Human Rerank and AI conditions, broken down by topic.\\nJ\\nQuality Control of Human Expert Ideas\\nEach expert is instructed to choose one of the seven specified topics and write one idea on it within 10\\ndays, following the given template in the annotation document. We included an honor code statement\\nto ask the participants to not use any AI tools in their idea writing. We collected N =50 ideas originally\\nand manually checked all of them for quality control. We filtered out one of them as being essentially a\\nparaphrase of an existing paper’s abstract. We compensated the participant nevertheless but excluded\\nthem from the review task.\\n42\\nK\\nBreakdown of Participant Positions\\nWe show the detailed position breakdown of our 49 idea-writing participants in Table 13 and the\\npositions of our 79 reviewer participants in Table 14.\\nPosition\\nCount\\nPostdoc\\n1\\nPhD\\n36\\nMaster\\n9\\nUndergraduate\\n1\\nResearch Scientist\\n1\\nMachine Learning Engineer\\n1\\nTable 13: Positions of the 49 idea writing participants.\\nPosition\\nCount\\nPostdoc\\n7\\nPhD\\n63\\nMaster\\n5\\nResearch Scientist\\n3\\nMachine Learning Engineer\\n1\\nTable 14: Positions of the 79 idea reviewing participants.\\n43\\nL\\nInstitutions of the Idea Writing Participants\\nInstitution\\nCount\\nStanford University\\n11\\nUniversity of Southern California\\n6\\nUniversity of Maryland\\n3\\nUniversity of Illinois Urbana-Champaign\\n3\\nJohns Hopkins University\\n3\\nColumbia University\\n2\\nCarnegie Mellon University\\n2\\nUniversity of Pennsylvania\\n1\\nPrinceton University\\n1\\nPenn State University\\n1\\nPortland State University\\n1\\nStony Brook University\\n1\\nUniversity of Chicago\\n1\\nUniversity of Washington\\n1\\nUC Berkeley\\n1\\nUCSD\\n1\\nMassachusetts Institute of Technology\\n1\\nGeorge Washington University\\n1\\nYale University\\n1\\nUniversity of Toronto\\n1\\nGeorgia Institute of Technology\\n1\\nNational University of Singapore\\n1\\nPeking University\\n1\\nTsinghua University\\n1\\nLinkedIn\\n1\\nNorm AI\\n1\\nTable 15: Institutions of the 49 idea writing participants.\\n44\\nM\\nInstitutions of the Idea Reviewing Participants\\nInstitution\\nCount\\nStanford University\\n25\\nUC Berkeley\\n4\\nUT Austin\\n4\\nUniversity of Maryland\\n4\\nPrinceton University\\n3\\nUniversity of Washington\\n3\\nUniversity of Southern California\\n3\\nCarnegie Mellon University\\n3\\nUniversity of Chicago\\n2\\nJohns Hopkins University\\n2\\nUCLA\\n2\\nGeorgia Institute of Technology\\n2\\nUniversity of Illinois Urbana-Champaign\\n2\\nTsinghua University\\n2\\nStony Brook University\\n1\\nOhio State University\\n1\\nNational University of Singapore\\n1\\nUniversity of Michigan\\n1\\nDartmouth College\\n1\\nMassachusetts Institute of Technology\\n1\\nUniversity of Pennsylvania\\n1\\nUniversity of Toronto\\n1\\nPortland State University\\n1\\nPenn State University\\n1\\nNew York University\\n1\\nColumbia University\\n1\\nUC Santa Barbara\\n1\\nBrown University\\n1\\nAmazon\\n1\\nLinkedIn\\n1\\nNorm AI\\n1\\nAMD\\n1\\nTable 16: Institutions of the 79 reviewer participants.\\n45\\nN\\nMixed-Effects Models\\nOne way to combine all the statistical tests above is to fit a linear mixed-effects model where we treat\\nthe condition as the fixed effect and other factors including reviewer and idea as random effects, while\\nalso accounting for the differences among different topics. This way, we can rely on the regression to\\naccount for all the possible confounders as the random effects. Specifically, for each metric, we fit the\\nfollowing linear mixed-effects model:\\nmodel = smf.mixedlm(\"Score ~ Condition\", df,\\ngroups=df[\"Topic\"],\\nre_formula=\"~Condition\",\\nvc_formula={\"ReviewerID\": \"0 + C(ReviewerID)\",\\n\"IdeaID\": \"0 + C(IdeaID)\"})\\nThis mixed-effects model analyzes the relationship between Score and Condition, while accounting\\nfor the hierarchical structure of the data. Fixed effects estimate the average effect of Condition on\\nScore. Random intercepts for Topic allow for varying baseline scores across topics, and random slopes\\nfor Condition within each topic allow the effect of Condition to vary by topic. Additionally, variance\\ncomponents for ReviewerID and IdeaID account for variability in scores specific to individual reviewers\\nand ideas, respectively.\\nThe results are shown in Table 17. The intercepts in the mixed-effects models represent the estimated\\nmean score of the baseline condition, which in this context is the Human Ideas. The coefficients for\\nCondition[AI Ideas] and Condition[AI Ideas + Human Rerank] in the mixed-effects models\\nrepresent the difference in the mean score for each metric between the AI ideas and the baseline\\n(human ideas). For example, a positive coefficient of 0.761 for the novelty score means that AI Ideas,\\non average, score 0.761 points higher than Human Ideas on the novelty score metric; conversely, a\\nnegative coefficient of -0.330 for the feasibility score means that AI Ideas, score 0.330 points lower\\nthan Human Ideas on feasibility on average. The topic (group) variance in the mixed-effects model\\nrepresents the variability in the outcome metric that can be attributed to differences between the\\ntopics, which is relatively small in general. Similarly, the idea variance and reviewer variance in\\nthe mixed-effects model represent the variability in the outcome metric that can be attributed to\\ndifferences between individual ideas and between reviewers, respectively. The reviewer variances\\nare high in general, suggesting that there is substantial variability in how different reviewers rate the\\nsame ideas. This implies that reviewer differences play a significant role in the observed scores, with\\nsome reviewers consistently giving higher or lower ratings.\\nOverall, the results from the mixed-effects models confirm our main conclusion that AI ideas are rated\\nas significantly more novel than human ideas.\\n46\\nCoef.\\nSE\\np\\nNovelty Score\\nIntercept\\n4.826\\n0.217\\n0.000***\\nCondition[AI Ideas]\\n0.756\\n0.331\\n0.023*\\nCondition[AI Ideas + Human Rerank]\\n0.902\\n0.305\\n0.003**\\nIdea Var\\n0.412\\n0.178\\nReviewer Var\\n0.803\\n0.202\\nExcitement Score\\nIntercept\\n4.493\\n0.212\\n0.000***\\nCondition[AI Ideas]\\n0.626\\n0.303\\n0.039*\\nCondition[AI Ideas + Human Rerank]\\n0.879\\n0.298\\n0.003**\\nIdea Var\\n0.495\\n0.227\\nReviewer Var\\n0.782\\n0.167\\nFeasibility Score\\nIntercept\\n6.595\\n0.224\\n0.000***\\nCondition[AI Ideas]\\n-0.300\\n0.294\\n0.307\\nCondition[AI Ideas + Human Rerank]\\n-0.183\\n0.314\\n0.561\\nIdea Var\\n0.476\\n0.188\\nReviewer Var\\n1.035\\n0.261\\nExpected Effectiveness Score\\nIntercept\\n5.156\\n0.211\\n0.000***\\nCondition[AI Ideas]\\n0.310\\n0.140\\n0.027*\\nCondition[AI Ideas + Human Rerank]\\n0.383\\n0.242\\n0.114\\nIdea Var\\n0.200\\n0.151\\nReviewer Var\\n0.469\\n0.141\\nOverall Score\\nIntercept\\n4.660\\n0.242\\n0.000***\\nCondition[AI Ideas]\\n0.137\\n0.294\\n0.640\\nCondition[AI Ideas + Human Rerank]\\n0.610\\n0.320\\n0.056\\nIdea Var\\n0.262\\n0.154\\nReviewer Var\\n1.071\\n0.225\\nTable 17: Results of linear mixed-effects models. We bold results that are statistically significant\\n(∗p < 0.05;∗∗p < 0.01;∗∗∗p < 0.001). Our main conclusion on AI ideas being more novel than human\\nideas still holds here.\\n47\\nO\\nScore Breakdown by Topic\\nWe show the breakdown of all scores across all conditions by topic. Note that due to the smaller\\nsample sizes for the per-topic breakdown, most results are not statistically significant and only offer\\nan intuitive understanding of the trends.\\nHuman\\nAI\\nAI+Rerank\\n0\\n2\\n4\\n6\\n8\\nMultilingual\\n*\\n*\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\n*\\n*\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\nOverall\\nHuman\\nAI\\nAI+Rerank\\n0\\n2\\n4\\n6\\n8\\nFactuality\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\nOverall\\nHuman\\nAI\\nAI+Rerank\\n0\\n2\\n4\\n6\\n8\\nBias\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\nOverall\\nHuman\\nAI\\nAI+Rerank\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nUncertainty\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\nOverall\\nHuman\\nAI\\nAI+Rerank\\n0\\n2\\n4\\n6\\n8\\nSafety\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\nOverall\\nHuman\\nAI\\nAI+Rerank\\n0\\n2\\n4\\n6\\n8\\nMath\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\nOverall\\nHuman\\nAI\\nAI+Rerank\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nCoding\\nNovelty\\nHuman\\nAI\\nAI+Rerank\\nExcitement\\nHuman\\nAI\\nAI+Rerank\\nFeasibility\\nHuman\\nAI\\nAI+Rerank\\nEffectiveness\\nHuman\\nAI\\nAI+Rerank\\nOverall\\nFigure 5: Breakdown of all scores by topic.\\n48\\nP\\nExample Idea: Modular Calibration for Long-form Answers\\nModular Calibration for Long-form Answers (Part 1)\\n1. Problem Statement: Calibrating the confidence of Large Language Models (LLMs) when generating long-form\\nanswers, such as essays and code, remains an open challenge in the field of natural language processing.\\n2.\\nMotivation: While numerous methods have been developed to calibrate the performance of LLMs on\\nmultiple-choice questions or open-domain questions with short answers, extending these approaches to tasks\\nrequiring lengthy responses presents significant difficulties. For instance, in code generation tasks (e.g., the\\nHumanEval dataset), traditional confidence extraction methods like perplexity may prove inadequate due to\\nthe substantial variation in answer length across questions. Verbalized confidence can be affected by instruction\\ntuning artifacts or unclear scope, while the reliability of metrics such as Expected Calibration Error (ECE) and\\nMacro-averaged Calibration Error (MacroCE) may be compromised by differences in task settings. Our aim is\\nto propose a novel pipeline for confidence extraction and calibration of LLMs for long-form answers, drawing\\ninspiration from methods used for short or fixed-set answers. This approach will enable us to monitor the model’s\\nlong-form answer generation process and apply targeted external augmentation when necessary, thereby enhancing\\nboth performance and efficiency.\\n3. Proposed Method: We introduce Modular Calibration, a process comprising four core steps:\\n1. Extend: Prompt the model to elaborate on the original question in relation to the answer, identifying which\\ncomponents of the question are addressed in the long-form response.\\n2. Decompose: Instruct the LLM to break down the extended question and long-form answer into multiple\\nmodules.\\n3. Extract Confidence: Utilize verbalized confidence or perplexity to determine the confidence level for each\\nmodule.\\n4. Merge: Based on the relationships between the modular questions/answers and the overall questions/an-\\nswers, prompt the model to combine the modular confidence scores into an overall score representing the\\nconfidence in the long-form answer.\\nEach of these steps is executed by prompting the same LLM in different ways to elicit the desired response.\\n4. Step-by-Step Experiment Plan:\\n1. Gather Datasets: Select datasets featuring long answers with correctness annotations. Potential candidates\\ninclude GSM8K, Code Gen, and Essay Writing.\\n2. Construct Prompts:\\n(a) Establish a baseline using direct prompting, where a query is presented without special techniques.\\n(b) Analyze outputs to refine prompts for the Extend and Decompose steps.\\n(c) For the Confidence step, employ vanilla perplexity or verbalized confidence extraction. If performance\\nis unsatisfactory, explore advanced methods built upon these techniques, such as those presented in\\nrecent research (e.g., FaR paper).\\n3. Select Models: Evaluate GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as the open-\\nsource LLaMA-3-70B-chat.\\n4. Get Results: Obtain confidence predictions from the models on the selected datasets using both baseline\\nmethods and the proposed Modular Calibration approach.\\n5. Analyze Results: Compare the calibration performance of LLMs using the new method against the baselines\\n(e.g., the perplexity of the entire long-form answer). Conduct qualitative and quantitative analyses on each\\ncomponent of the Modular Calibration process.\\n49\\nModular Calibration for Long-form Answers (Part 2)\\n5. Test Case Examples:\\n• Test Case 1: Verbalized Confidence Prompting\\n– Input: <Q> <A> Confidence (0-1)\\n– Output: [Model generates a confidence score between 0 and 1]\\n• Test Case 2: Modular Calibration Step 1 (Extend)\\n– Input: Given the answer, can you extend the question and elaborate on what points are covered in the\\nanswer?\\n– Output: The answer covers these points of the question: (1) how fast A runs; (2) how fast B runs; (3) if A\\nis faster than B.\\n• Test Case 3: Modular Calibration Step 2 (Decompose)\\n– Input: Please decompose the above extended question and answers into modules.\\n– Output:\\n* How fast A runs: [relevant excerpt from the original answer]\\n* How fast B runs: [relevant excerpt from the original answer]\\n[Additional modules as needed]\\n• Test Case 4: Modular Calibration Step 3 (Extract)\\n– Input: How fast A runs: [relevant excerpt from the original answer] Confidence (0-1)\\n– Output: 1. 0.9; 2. 0.6 [Additional confidence scores for other modules]\\n• Test Case 5: Modular Calibration Step 4 (Merge)\\n– Input: For each of these points related to question X, the confidence is: 0.9, 0.6, ... What is the overall\\nconfidence for the whole problem?\\n– Output: [Model generates an overall confidence score]\\n6. Fallback Plan: If the proposed Modular Calibration method does not demonstrate improvement over the baseline,\\nwe will execute each sub-question and module individually to assess whether calibration is enhanced for each\\ncomponent. This approach will facilitate debugging of the proposed method and potentially yield interesting\\ninsights into the relationships between performance/calibration of decomposed modules and overall problems.\\nAlternatively, we may analyze the model’s ability to effectively decompose questions and answers into appropriate\\nmodules. These analyses will inform potential refinements to the method or provide valuable insights into the\\nlimitations and capabilities of LLMs in handling complex, long-form responses.\\n50\\nReviewer 1\\nNovelty: 6 (reasonably novel - there are some notable differences from existing ideas and probably enough to turn\\ninto a new paper)\\nRationale: Focus on the long-form setting is novel at the moment. The idea of obtaining modular confidence\\nestimates for different claims in a long-form output, and synthesizing them into a single uncertainty estimate is not\\nthat complicated, but it does seem to be underexplored.\\nFeasibility: 8 (Highly Feasible: Straightforward to implement the idea and run all the experiments.)\\nRationale: The only part of the project that seems challenging is obtaining correctness annotations for one of the\\ndatasets (e.g., Essay Writing). GSM8K and code datasets like HumanEval seem like very natural long-form output\\nsettings to try out the idea. Other than this, iterating on the prompts for decomposition / verbalized UQ for each of\\nthe modules will be important, but the author mentions this.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: It’s possible that first obtaining verbalized uncertainty estimates for each module, and then synthesizing\\ninto a single score, will outperform the standard baselines of self-consistency over the entire long-form output (using\\nmajority vote as the confidence score). However, I don’t expect this to be dramatically better. If the paper instead set\\nout with the goal of actually producing the UQ estimates for each claim, then almost no prior work does this, and the\\nbaselines would be less strong.\\nExcitement: 5 (Leaning negative: it has interesting bits but overall not exciting enough)\\nRationale: This seems like the most straightforward possible way to obtain uncertainty estimates for a long-form\\ngeneration with an LLM. This means the project could produce some useful engineering artifacts, but it doesn’t\\nreally push the idea to its logical conclusion. Therefore I don’t consider it \"exciting enough\". There is some mention\\nof \"using the uncertainty estimates to possibly condition on more information\" but this is not fleshed out – it could\\nbe more interesting. For example, studying how the fine-grained uncertainty estimates could be used to selectively\\nretrieve factual information from Wikipedia etc. on a knowledge-intensive task.\\nOverall Score: 5 (Decent idea but has some weaknesses or not exciting enough, marginally below the acceptance\\nthreshold of major AI conferences)\\nRationale: I like the focus on long-form generations. However, this proposal is a very straightforward baseline\\nand extension of existing work to the long-form generation setting (just produce the long generation, decompose it,\\napply verbalized uncertainty on each claim, and finally aggregate them). I could see the paper being well-cited, but I\\ndon’t see an interesting/novel angle here.\\nConfidence: 5 (You are absolutely certain that the evaluation is correct and very familiar with the relevant literature)\\n51\\nReviewer 2\\nNovelty: 6 (reasonably novel - there are some notable differences from existing ideas and probably enough to turn\\ninto a new paper)\\nRationale:\\nWhile existing works have explored the problem of calibration in long-form answers (e.g.\\nhttps://arxiv.org/abs/2402.06544), the specific method for calibration is different. Also seems related to FactScore\\n(https://arxiv.org/abs/2305.14251) where the task was different (getting a factuality score) but the idea of breaking\\nlong form generations into smaller units, evaluating each separately and then combing does seem related.\\nFeasibility: 8 (Highly Feasible: Straightforward to implement the idea and run all the experiments.)\\nRationale: The idea seems simple enough to implement with API access, considering all the steps involved in the\\nmethod can be done via prompting with API. The proposal does mention using LLaMA3-70B as an additional\\nmodel, which would require GPUs I guess.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: Since it has been shown that LLMs are quite well calibrated when asked to verbalize the confidence\\nfor short answers, I’m guessing the calibration scores would be pretty good for individual modules. Also LLMs\\nmight be decent at combining confidence scores (especially with detailed instructions and some examples in the\\nprompt), so overall the method might work well. But it’s unclear if it would do better than the methods proposed in -\\nhttps://arxiv.org/abs/2402.06544.\\nExcitement: 6 (Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental)\\nRationale: If the method does work well in getting calibration for long-form answers, I think that would be pretty\\nexciting. One thing which is missing from the proposal (and why the score was not higher) was that it does not touch\\nupon the issue that for long-form answers we won’t have a binary correct/incorrect decision but answers can be\\npartially correct.\\nOverall Score: 6 (Marginally above the acceptance threshold of major AI conferences)\\nRationale: The overall idea makes sense to me, but the score is not higher right now because: (a) it’s unclear what\\nexactly is meant by ’modules’ especially for essay writing which the proposal mentions as one of the tasks ; (b) the\\nissue for partial correctness which was mentioned above.\\nConfidence: 3 (You are fairly confident that the evaluation is correct)\\n52\\nQ\\nExample Idea: Semantic Resonance Uncertainty Quantification\\nSemantic Resonance Uncertainty Quantification (SRUQ) (Part 1)\\n1. Problem Statement: Current uncertainty quantification methods for Large Language Models (LLMs) often\\nrely on simple statistical measures or model-specific attributes, which may not capture the nuanced semantic\\nuncertainties in complex reasoning tasks. This limitation can lead to overconfident or poorly calibrated model\\noutputs, potentially resulting in unreliable decision-making in critical applications.\\n2.\\nMotivation: Existing approaches typically use softmax probabilities, entropy measures, or ensemble\\ndisagreement to quantify uncertainty. However, these methods often fail to capture the semantic nuances and\\nreasoning complexities in tasks that require deep understanding and multi-step reasoning. Human experts, on\\nthe other hand, gauge their uncertainty by considering how well their reasoning ’resonates’ with their broader\\nknowledge and experience. By mimicking this process in LLMs, we can potentially develop a more robust and\\nsemantically grounded approach to uncertainty quantification.\\n3. Proposed Method: We propose Semantic Resonance Uncertainty Quantification (SRUQ), which prompts the\\nLLM to generate multiple independent reasoning paths for a given problem, then quantifies uncertainty based on\\nthe semantic coherence and mutual reinforcement among these paths. The process involves five key steps:\\n1. Generating diverse solution attempts using different prompting strategies.\\n2. Cross-evaluating each solution attempt against the others, assessing logical consistency and mutual support.\\n3. Constructing a ’resonance graph’ where nodes are solution attempts and edges represent semantic reinforce-\\nment.\\n4. Computing a resonance score based on graph properties like connectivity and centrality.\\n5. Mapping the resonance score to a calibrated uncertainty estimate.\\n53\\nSemantic Resonance Uncertainty Quantification (SRUQ) (Part 2)\\n4. Step-by-Step Experiment Plan:\\n1. Dataset Preparation\\n• Utilize three datasets covering different reasoning tasks:\\n(a) GSM8K for mathematical problem-solving\\n(b) EntailmentBank for logical deduction\\n(c) HotpotQA for multi-hop question answering\\n• Split each dataset into train, validation, and test sets if not already done.\\n2. Baseline Implementation\\n• Implement three baseline uncertainty quantification methods:\\n(a) Softmax probabilities\\n(b) Monte Carlo Dropout\\n(c) Ensemble disagreement (using different few-shot prompts)\\n• Generate predictions and uncertainty estimates on the validation and test sets for each baseline.\\n3. SRUQ Implementation\\n(a) Generate 5 diverse solution attempts using different few-shot prompts and temperature settings.\\n(b) For each pair of solutions, prompt the LLM to evaluate their consistency and mutual support.\\n(c) Construct the resonance graph using the pairwise evaluations.\\n(d) Compute the resonance score using graph centrality measures (e.g., PageRank).\\n(e) Map the resonance score to a calibrated uncertainty estimate using isotonic regression on the validation\\nset.\\n4. Evaluation\\n• Compare SRUQ against the baselines using the following metrics:\\n(a) Expected Calibration Error (ECE)\\n(b) Brier score\\n(c) Area Under the Precision-Recall Curve (AUPRC) for uncertainty ranking\\n• Evaluate the correlation between uncertainty estimates and actual errors.\\n5. Analysis\\n• Visualize the resonance graphs for high and low uncertainty examples.\\n• Analyze the relationship between graph properties and prediction accuracy.\\n• Investigate cases where SRUQ significantly outperforms or underperforms compared to baselines.\\n6. Ablation Studies\\n• Vary the number of solution attempts.\\n• Compare different graph centrality measures.\\n• Evaluate the impact of the cross-evaluation step.\\n7. Generalization Test\\n• Test the generalization of SRUQ on out-of-distribution samples by applying the method trained on one\\ndataset to examples from the other datasets.\\n54\\nSemantic Resonance Uncertainty Quantification (SRUQ) (Part 3)\\n5. Test Case Examples:\\n• Baseline Example:\\n– Input: Q: If a train travels at 60 miles per hour, how far will it travel in 2.5 hours?\\n– Softmax Output: The train will travel 150 miles in 2.5 hours. (Confidence: 0.92)\\n– Explanation: The softmax probability is high, but it does not capture the reasoning process or potential\\nuncertainties in the calculation.\\n• SRUQ Example:\\n– Input: Q: If a train travels at 60 miles per hour, how far will it travel in 2.5 hours?\\n– Solution Attempts:\\n1. Distance = Speed × Time\\n* Distance = 60 miles/hour × 2.5 hours\\n* Distance = 150 miles\\n2. In 1 hour, the train travels 60 miles\\n* In 2 hours, it’s 120 miles\\n* In 0.5 hours, it’s 30 miles\\n* Total: 120 + 30 = 150 miles\\n3. Let’s break it down:\\n* 1 hour: 60 miles\\n* 1 hour: 60 miles\\n* 0.5 hour: 30 miles\\n* Sum: 60 + 60 + 30 = 150 miles\\n– Cross-Evaluation: All three solutions are consistent and mutually supportive. They use different\\napproaches but arrive at the same result.\\n– Resonance Graph: Fully connected graph with high edge weights\\n– Resonance Score: 0.95\\n– Calibrated Uncertainty: 0.05\\n– Final Output: The train will travel 150 miles in 2.5 hours. (Uncertainty: 0.05)\\n– Explanation: SRUQ generates multiple solution paths, evaluates their consistency, and quantifies\\nuncertainty based on their semantic resonance. The high resonance score indicates low uncertainty,\\nwhich is then calibrated to provide a final uncertainty estimate.\\n6. Fallback Plan: If SRUQ does not significantly outperform baselines, we can pivot to an analysis paper exploring\\nwhy semantic resonance might not capture uncertainty effectively. We could investigate the quality and diversity of\\ngenerated solution attempts, potentially improving the prompting strategies. Additionally, we could examine the\\neffectiveness of the cross-evaluation step, possibly incorporating external knowledge or more structured reasoning.\\nFurthermore, we could explore the relationship between graph properties and actual uncertainty, which might\\nreveal insights about how LLMs represent confidence internally. We could also consider combining SRUQ with\\ntraditional uncertainty quantification methods, creating a hybrid approach that leverages both statistical and\\nsemantic information.\\n55\\nReviewer 1\\nNovelty: 6 (reasonably novel - there are some notable differences from existing ideas and probably enough to turn\\ninto a new paper)\\nRationale: I haven’t seen (and couldn’t find) any prior work which exactly has the same idea as in this proposal.\\nThe proposed idea is definitely related to using consistency among multiple solutions to estimate uncertainty (e.g.\\nhttps://arxiv.org/abs/2405.18711 does this across solutions decoded from different layers) but I have not seen the\\nidea of constructing resonance graph and using graph properties to estimate uncertainty.\\nFeasibility: 8 (Highly Feasible: Straightforward to implement the idea and run all the experiments.)\\nRationale: The proposed method, SRUQ, should be pretty easy to implement given that LLM API access is\\nabundant. SRUQ involves multiple steps all of which can be done through prompting via API — getting multiple\\nsolutions, prompting LLMs to get a consistency score between each pair of solutions etc. The parts which cannot\\nbe implemented through API are the baselines e.g. Monte Carlo dropout, and would require GPUs. To do a fair\\ncomparison to the baselines, I imagine SRUQ will also have to be done on open models which could also require\\nGPUs.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: Although the proposal includes some baselines that should be compared to, it does not mention\\nsome methods which seem to do quite well with LLMs (especially getting better with scale) – e.g. methods like\\nP(True) (https://arxiv.org/abs/2207.05221) or verbalized confidence (https://arxiv.org/abs/2305.14975). It’s not\\nclear/obvious to me that the proposed method should do better than these baselines.\\nExcitement: 6 (Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental)\\nRationale: While the method is novel and feasible, I’m not too excited by it since some of the other existing methods\\nout there mentioned above (like https://arxiv.org/abs/2207.05221, https://arxiv.org/abs/2305.14975) are much\\nsimpler and work quite well. Compared to that SRUQ is more complex, and hence maybe has less chance of being\\nvery impactful (unless it works really better).\\nOverall Score: 6 (Marginally above the acceptance threshold of major AI conferences)\\nRationale: The above accept score is assuming the idea does work better than the baselines on some category of\\ntasks. Overall, given that the idea is novel, the proposal includes comparison to other baselines as well analysis &\\nablations, I think that could be enough to get accepted into an AI conference.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n56\\nReviewer 2\\nNovelty: 6 (reasonably novel - there are some notable differences from existing ideas and probably enough to turn\\ninto a new paper)\\nRationale: The proposed approach shares some similar ideas with self-consistency (which suggests the consistency\\nof sampled LLMs outputs is relatively well calibrated). But the approach is more generalized and fine-grained than\\nexisting work if the approach uses more advanced ‘mutual support evaluation‘ beyond simply comparing the final\\nanswers.\\nFeasibility: 5 (Moderately feasible: It can probably be executed within the given time frame but would require\\ncareful planning, efficient use of APIs or some advanced computational strategies to overcome the limited GPU\\nresources, and would require some modifications to the original proposal to make it work.)\\nRationale: There lacks some important details in terms of the cross-evaluation part. How is the mutual support\\nevaluated (by prompting or some other methods?). This part is crucial for implementing the whole pipeline of this\\napproach.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: I think it has some chances to beat the proposed baselines. If the cross-evaluation part is properly\\nexecuted. Again, the success of this proposal is highly dependent on that part.\\nExcitement: 6 (Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental)\\nRationale: If this idea actually works, at least it tells something new about how to use multiple samples to provide\\nbetter confidence estimation than simple consistency. But the idea itself is still somewhat incremental given the\\nexistence of current consistency-based calibrators.\\nOverall Score: 6 (Marginally above the acceptance threshold of major AI conferences)\\nRationale: Overall there are some incremental contributions, but not too exciting. The algorithm itself can be neat. I\\nthink it can be worth a borderline acceptance.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n57\\nReviewer 3\\nNovelty: 6 (reasonably novel - there are some notable differences from existing ideas and probably enough to turn\\ninto a new paper)\\nRationale: I think the idea is reasonable and indeed identifies some limitations of current works on uncertainty\\nestimation. However, the consistency between reasoning paths is somehow similar to self-consistency reasoning\\nfrom Google and SelfCheckGPT.\\nFeasibility: 7\\nRationale: I think it could be easy to implement and quickly be tried by PhD students or even undergrads. Also, in\\nthe test case example, the setting is straightforward and well-defined.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: Based on my experience, the consistency-based methods, although not fully theoretically grounded, can\\nwork pretty well in current uncertainty estimation questions. I believe working this on the reasoning path level\\ncould also work to some extent.\\nExcitement: 6 (Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental)\\nRationale: Overall, this idea identified a good research question, although the method might not be very exciting to\\nme.\\nOverall Score: 6 (Marginally above the acceptance threshold of major AI conferences)\\nRationale: The novelty and the actual application of this method in the area is limited, but could be an inspiring idea.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n58\\nR\\nExample Idea: Translation with LLMs through Prompting with\\nLong-Form Context\\nTranslation with LLMs through Prompting with Long-Form Context (Part 1)\\n1. Problem Statement: Stable generation of text in low-resource languages is an unsolved issue in large language\\nmodels.\\n2.\\nMotivation: While LLMs can often produce surprisingly good translations despite not being explicitly\\ntrained for this task, this does not hold for lower-resource languages. LLMs are both more likely to gener-\\nate off-target text (text in another language than intended) when prompted to translate to a lower-resource\\nlanguage, and show increased instability in translation quality across prompt templates in lower-resource languages.\\n3. Proposed Method: Our proposed method investigates the use of long-form templates to improve generated\\ntranslation quality and reduce off-target translations in lower-resource languages. We propose to provide additional\\nprompt context by translating multi-sentence input, with additional views of the target language with the langid\\ntemplate provided as context. We do so in multiple stages:\\n1. Querying the language model to first generate a paragraph containing the source sentence to be translated.\\n2. Prepending monolingual text in the target language, with langid: tags, above the translation prompt.\\n3. Presenting both these additional sources of content, prompting the LLM for a translation.\\n4. Step-by-Step Experiment Plan:\\n1. Choose datasets: Evaluate on the FLORES-200 datasets, which allow for wide language coverage on the\\nWikipedia domain, as well as the WMT-21 test sets for news and law/medical domain.\\n2. Choose languages: Opt for English-centric translation with:\\n• 5 high-resource languages with different scripts (French, German, Russian, Chinese, Japanese)\\n• 5 mid-resource languages (Farsi, Vietnamese, Arabic, Korean, Hebrew)\\n• 5 low-resource languages with considerably lower likelihood of incidental bilingualism (Gujarati, Thai,\\nTajik, Sindhi, Pashto)\\n3. Choose models: Include the API-based GPT-3.5 (Text-Davinci-003) and GPT-4 model from OpenAI and\\nGemini from Google, as well as the open-weight LLaMA-3, Gemma, and Aya models which enable additional\\nanalysis.\\n4. Gather translation results: Systematically compare standard MT prompt templates to our proposed method\\nacross different models and language pairs. Additionally ablate the steps of the new method (removing langid\\ntemplates; replacing langid templates with endonymic langid tags; provide only the generated paragraph;\\nonly the monolingual content).\\n5. Perform analysis: Evaluate whether the new method improves the performance of LLMs in these tasks as\\ncompared to the baselines using multiple standard automatic metrics for MT (chrF, COMET, BLEU) and token-\\nlevel LID to measure off-target translations. Assess which component(s) are necessary for this improvement\\nand whether or not there are changes across language pair direction and language resource levels or scripts.\\n59\\nTranslation with LLMs through Prompting with Long-Form Context (Part 2)\\n5. Test Case Example:\\n• Baseline Prompt Template:\\n[English] This is an example.\\n[Spanish]\\n• Baseline Prompt Expected Output:\\n[Spanish] Este es un ejemplo.\\n• Proposed Prompt Input (step 1):\\nWrite a paragraph containing the following sentence:\\n[English] This is an example.\\n• Expected model output (step 1):\\nThis is an example link. Italics and Bold: use asterisks and underscores\\nto indicate spans of emphasis. Use one asterisk (or underscore) for\\nitalics and two asterisks (or two underscores) for bold. For example...\\n• Proposed Prompt Input (step 2):\\n[Spanish] Computadoras simples son lo suficientemente pequeñas\\npara residir en los dispositivos móviles.\\n[Spanish] Las computadoras portátiles, tales come tabletas,\\nnetbooks, notebooks, ultrabooks, pueden ser alimentadas por\\npequeñas baterías.\\n[Spanish] Las computadoras personales en sus diversas formas son\\niconos de la llamada era de la información y son lo que la mayoría\\nde la gente considera como «computadora».\\nTranslate this paragraph from English to Spanish:\\n[English] This is an example link. Italics and Bold: use asterisks\\nand underscores to indicate spans of emphasis. Use one asterisk\\n(or underscore) for italics and two asterisks (or two underscores)\\nfor bold. For example...\\n[Spanish]\\n• Proposed Prompt Expected Output:\\n[Spanish] Este es un enlace de ejemplo. Cursiva y negrita: utilice\\nasteriscos y guiones bajos para indicar intervalos de énfasis.\\nUtilice un asterisco (o guión bajo) para cursiva y dos asteriscos\\n(o dos guiones bajos) para negrita. Por ejemplo...\\n60\\nTranslation with LLMs through Prompting with Long-Form Context (Part 3)\\n5. Test Case Example (Continued):\\n• Proposed Prompt Input (step 3):\\nWhich of the following sentences are translations of the following\\nEnglish sentence? Multiple sentences can be chosen.\\n[English] This is an example.\\n1. Este es un enlace de ejemplo.\\n2. Cursiva y negrita: utilice asteriscos y guiones bajos para indicar\\nintervalos de énfasis.\\n3. Utilice un asterisco (o guión bajo) para cursiva y dos asteriscos\\n(o dos guiones bajos) para negrita.\\n4. Por ejemplo...\\n• Proposed Prompt Expected Output:\\nThe sentence \"This is an example.\" can be translated to Spanish as:\\n1. Este es un ejemplo.\\n2. Por ejemplo...\\nThese two options correctly translate the meaning of\\n\"This is an example.\" into Spanish.\\n6. Fallback Plan: If the proposed method does not help as compared to the baseline, analyzing the results of\\nstep 3 would likely provide further insights into how the template should be modified. In addition to potentially\\nidentifying off-target errors, it may be that the model is unable to identify correct translations even if they have\\nbeen generated, and results are likely to vary across languages based on their training data. Using the generated\\nparagraph as provided context and still querying the model to translate at only the sentence level could be compared.\\nRestricting monolingual text to be retrieved text within the domain of the source sentence could be explored. Adding\\nfew-shot examples in the prompt and comparing other MT prompt templates may also help debug the proposed\\nmethod. Including an additional query where the model is first asked to label each generated token by langid and\\nthen asked to re-translate the source including those tokens which are correctly labelled in target may reinforce\\nlangid and guide generation in the target language. Performing layer-wise analyses of likelihood of generating the\\nnext token in-language and in-script for open-weight models may also help debug where and why off-target issues\\npersist.\\n61\\nReviewer 1\\nNovelty: 5 (somewhat novel - there are differences from existing ideas but not enough to turn into a new paper)\\nRationale: While I’m not aware of papers that have used this exact prompting strategy, I don’t think that this\\nproposal will be enough to justify a publication. I think that there should be a variety of strategies suggested + an\\nanalysis of multiple prompting strategies rather than suggesting one strategy. I think that a thorough analysis of the\\neffects of additional context / langids could potentially turn this into a paper.\\nFeasibility: 9\\nRationale: Such a project that only uses LLM APIs could be executed very quickly without much expertise in\\ncoding/architecture. The only time-consuming part might be iterating and adjusting the prompts in the ablation\\nstudies.\\nExpected Effectiveness: 7\\nRationale: I think that this proposal could work well to guide LLMs to translate in the desired target language, since\\nthis is a known problem with current prompt-based MT strategies (as the writers have suggested).\\nExcitement: 5 (Leaning negative: it has interesting bits but overall not exciting enough)\\nRationale: I’m not sure how well this method will transfer to future models, and this could be a limiting factor in the\\nlongevity of this research. (But this is a limitation of all prompting research...)\\nOverall Score: 5 (Decent idea but has some weaknesses or not exciting enough, marginally below the acceptance\\nthreshold of major AI conferences)\\nRationale: I think that the work should focus on the ablation studies and comparison of multiple prompting\\nstrategies / analysis, rather than focusing on one new strategy.\\nConfidence: 3 (You are fairly confident that the evaluation is correct)\\n62\\nReviewer 2\\nNovelty: 1 (not novel at all - there are many existing ideas that are the same)\\nRationale: There are multiple existing works on prompting LLMs on low-resource translation, usually using\\nfew-shot demo. https://proceedings.mlr.press/v202/garcia23a/garcia23a.pdf https://arxiv.org/pdf/2305.14857\\nAlso work explaining why few-shot prompt would work: https://arxiv.org/pdf/2305.10266\\nFeasibility: 5 (Moderately feasible: It can probably be executed within the given time frame but would require\\ncareful planning, efficient use of APIs or some advanced computational strategies to overcome the limited GPU\\nresources, and would require some modifications to the original proposal to make it work.)\\nRationale: The prompting experiment is mostly feasible given one can afford the API calls. The model, prompts,\\nand evaluation metrics are concrete, although unclear if the proposed experiment is useful for proving the research\\nidea, e.g., a few high-resource languages are listed for a research idea that focuses on low-resource languages.\\nExpected Effectiveness: 3 (Low Effectiveness: The idea might work in some special scenarios but you don’t expect it\\nto work in general.)\\nRationale: The proposed experiment can help find a set of relatively high-performing prompts, but it is unclear\\namong the prompts proposed if any of them will bring any improvement.\\nExcitement: 3 (Mediocre: this idea makes marginal contributions and is very incremental)\\nRationale: The ability to do prompting/few-shot translation is fundamentally tied to the training data, see\\nhttps://arxiv.org/pdf/2305.10266, so trying to solve this problem from the prompting space is inherently limited.\\nOverall Score: 3 (Clear rejection for major AI conferences)\\nRationale: There is similar work on prompting LLMs to generate translation in low-resource languages, hence the\\nidea is not very novel. Moreover, in terms of the goal to generate high-quality low-resource translation, the gains\\nlikely are not going to come from prompting.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n63\\nS\\nExample Idea: Linguistic Pivot Constellation: Enhancing\\nCross-Lingual Transfer for Low-Resource Languages and Dialects\\nLinguistic Pivot Constellation (LPC): Enhancing Cross-Lingual Transfer for Low-Resource\\nLanguages and Dialects (Part 1)\\n1. Problem Statement: Large language models struggle with cross-lingual transfer, especially for low-resource\\nlanguages and dialects. This limitation hinders the models’ ability to perform well on multilingual tasks involving\\nthese languages, potentially exacerbating digital language divides.\\n2. Motivation: Current approaches often rely on parallel data or multilingual pretraining, which are limited for\\nmany language pairs. Inspired by how polyglots leverage similarities between known languages to learn new ones,\\nwe propose creating a network of conceptual bridges across languages. This method could potentially overcome\\nthe limitations of existing approaches by leveraging the model’s broad knowledge to create connections between\\nknown and unknown linguistic territories.\\n3. Proposed Method: We introduce Linguistic Pivot Constellation (LPC), a novel prompting technique that\\nconstructs a dynamic network of linguistic pivot points. For a given task, LPC first identifies conceptually similar\\nlanguages or dialects to the target language. It then generates a constellation of prompts in these pivot languages,\\neach capturing a different aspect of the task. The model is guided to ’triangulate’ the correct response by considering\\nthese multiple perspectives. For example, to translate a rare dialect, LPC might use prompts in related languages,\\nregional lingua francas, and even etymologically connected languages.\\n4. Step-by-Step Experiment Plan:\\n1. Data Collection\\n• Gather datasets for translation and question-answering tasks across a diverse set of low-resource\\nlanguages and dialects.\\n• Utilize the FLORES-101 dataset for machine translation and the TyDi QA dataset for question answering.\\n2. Baseline Implementation\\n• Implement standard few-shot prompting and existing cross-lingual transfer methods (e.g., zero-shot\\ncross-lingual transfer) as baselines.\\n3. LPC Implementation\\n(a) Create a language similarity matrix based on language families and geographical proximity.\\n(b) Implement a function to select the most relevant pivot languages for a given target language.\\n(c) Design prompts for each pivot language that capture different aspects of the task.\\n4. Prompt Construction\\n(a) Select 3-5 pivot languages based on the similarity matrix.\\n(b) Generate task-specific prompts in each pivot language.\\n(c) Combine these prompts into a ’constellation’ prompt that includes the original task in the target language.\\n5. Model Selection\\n• Use GPT-4 as the primary model for experiments.\\n• Test with GPT-3.5-turbo for comparison.\\n6. Experiment Execution\\n(a) Run the baseline methods.\\n(b) Run the LPC method with varying numbers of pivot languages (1, 3, and 5).\\n(c) Record the model outputs and performance metrics.\\n64\\nLinguistic Pivot Constellation (LPC): Enhancing Cross-Lingual Transfer for Low-Resource\\nLanguages and Dialects (Part 3)\\n4. Step-by-Step Experiment Plan (Continued):\\n7. Evaluation\\n• Evaluate the results using task-specific metrics:\\n– BLEU score for translation tasks\\n– F1 score for question answering tasks\\n8. Analysis\\n• Analyze the effectiveness of different pivot language combinations and the method’s scalability to\\nextremely low-resource scenarios.\\n• Compare LPC performance against baselines across different language families and resource levels.\\n5. Test Case Examples:\\n• Test Case 1:\\n– Baseline Prompt Input: Translate the following Sicilian sentence to English: ’Unni c’è fumu c’è focu.’\\n– Baseline Prompt Expected Output: Where there’s smoke, there’s fire.\\n– Proposed Prompt Input: We will translate a Sicilian sentence to English. To help with this task, consider\\nthe following related phrases:\\nIn Italian: ’Dove c’è fumo c’è fuoco.’\\nIn Neapolitan: ’Addò ce sta ’o fummo ce sta ’o ffuoco.’\\nIn Latin: ’Ubi fumus, ibi ignis.’\\nNow, translate the Sicilian sentence to English: ’Unni c’è fumu c’è focu.’\\n– Proposed Prompt Expected Output: Where there’s smoke, there’s fire.\\n– Explanation: The LPC method provides context from related languages (Italian, Neapolitan, and Latin),\\nwhich can help the model better understand and translate the Sicilian phrase. This is especially useful\\nfor low-resource languages like Sicilian, where direct translation data might be limited.\\n6. Fallback Plan: If the LPC method does not significantly outperform baselines, we will pivot the project towards\\nan in-depth analysis of cross-lingual transfer mechanisms. We will investigate the relationship between language\\nsimilarity and transfer effectiveness, the impact of pivot language selection on performance, and how different\\naspects of language (lexical, syntactic, semantic) transfer across the constellation. This analysis could provide\\nvaluable insights into the strengths and limitations of large language models in cross-lingual tasks, potentially\\ninforming future research directions in multilingual Natural Language Processing.\\n65\\nReviewer 1\\nNovelty: 9\\nRationale: The idea of using a linguistic similarity matrix to form conceptual bridges when constructing prompts\\nto improve cross-lingual transfer is one that I have not heard of before. I think this could be an interesting way of\\nleveraging existing information about related languages for NLP tasks in general.\\nFeasibility: 8 (Highly Feasible: Straightforward to implement the idea and run all the experiments.)\\nRationale: I think the idea makes sense, but more details should be shared about how exactly this language similarity\\nmatrix is constructed and what algorithms will be used for determining language similarity. More details should be\\nprovided on how the prompts for different languages will be obtained and how the data will be collected, which\\nmight be a time bottleneck.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: I think that this idea could work well just by providing more context in different languages. The\\neffectiveness sounds like it might be highly variable on the selection of pivot languages, though.\\nExcitement: 7\\nRationale: I think that this could be interesting beyond the context of prompting, such as the use of pivot languages\\nin traditional machine translation.\\nOverall Score: 7 (Good idea, would be accepted by major AI conferences)\\nRationale: I think that the idea is sufficiently novel, and if it is executed well with good results, could produce a\\nquality paper at a top NLP conference.\\nConfidence: 3 (You are fairly confident that the evaluation is correct)\\n66\\nReviewer 2\\nNovelty: 8 (clearly novel - major differences from all existing ideas)\\nRationale: The LPC method introduces a novel way of leveraging related languages and dialects to improve cross-\\nlingual transfer. While cross-lingual transfer and language similarity have been explored, the idea of dynamically\\ncreating a constellation of prompts using pivot languages for specific tasks is a fresh and innovative approach.\\nFeasibility: 5 (Moderately feasible: It can probably be executed within the given time frame but would require\\ncareful planning, efficient use of APIs or some advanced computational strategies to overcome the limited GPU\\nresources, and would require some modifications to the original proposal to make it work.)\\nRationale: Implementing LPC could be challenging due to the complexities involved in selecting optimal pivot\\nlanguages and designing effective prompts for each. While the concept is sound, the practical execution—such as\\nbuilding the language similarity matrix and dynamically generating prompts—may require substantial effort and\\nexperimentation.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: The LPC method has the potential to improve cross-lingual performance, especially in low-resource\\nlanguages. By leveraging linguistic similarities, the model might better understand and translate languages with\\nlimited training data.\\nExcitement: 7\\nRationale: The LPC method is exciting because it tackles a critical challenge in multilingual NLP—improving\\nperformance for low-resource languages. If successful, it could significantly enhance the accessibility and usability\\nof AI models across diverse linguistic contexts, particularly in underrepresented languages.\\nOverall Score: 6 (Marginally above the acceptance threshold of major AI conferences)\\nRationale: The idea is a promising candidate for exploration in the field of multilingual NLP. It introduces a novel\\napproach that could potentially improve cross-lingual transfer, particularly for low-resource languages and dialects.\\nHowever, the challenges in implementation and the uncertain effectiveness of the method warrant a cautious overall\\nrating.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n67\\nReviewer 3\\nNovelty: 8 (clearly novel - major differences from all existing ideas)\\nRationale: Leveraging language similarity is often quite well studied in machine translation, but there hasn’t been\\none studying using similar language as demonstration in multilingual in-context learning. It would be interesting to\\nsee how the model behavior change with different pivots.\\nFeasibility: 8 (Highly Feasible: Straightforward to implement the idea and run all the experiments.)\\nRationale: The implementation will mostly involve building the similarity matrix and formatting the prompts. The\\nsimilarity matrix should be able to get from some existing works. The prompt formatting and experiments part\\nshould be pretty straightforward with enough API quota.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: The idea is pretty interesting, but it’s not exactly sure whether similar languages are informative enough\\nfor the model, since it still requires the model to understand the similarity between languages and reason over the\\nrelationship between target language and the given languages.\\nExcitement: 8 (Exciting: would deepen the community’s understanding or make major progress in this research\\ndirection)\\nRationale: It would be informative to the community to see whether such demonstration can lead to good perfor-\\nmance for in-context learning. Even if this idea doesn’t work, the analysis will be quite informative.\\nOverall Score: 7 (Good idea, would be accepted by major AI conferences)\\nRationale: This work studies an important problem for the multilingual community. The experiment results and\\nanalysis will be quite informative for multilingual in-context learning.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n68\\nT\\nExample Idea: LLM Directed Retrieval Querying for Improving\\nFactuality\\nLLM Directed Retrieval Querying for Improving Factuality (Part 1)\\n1. Problem Statement: Large language models can generate flexible, long-form language generations, but\\nLLM-generated responses often contain hallucinated or factually inconsistent content. Particularly in high-risk\\nsettings, there is a need for methods to improve the factuality of LLMs.\\n2. Motivation: A common framework for improving the factuality of LLM generations is retrieval augmented\\ngeneration (RAG). In a RAG framework, a retriever takes a query as input and retrieves external knowledge from\\na high-quality knowledge base from reliable sources. The retrieved content is incorporated into the prompt for\\ngenerating the response. One issue with this approach is that the quality of the generation can be bottlenecked by the\\nquality of the retrieved content. Retrieval can be challenging for tasks where the query objective is underspecified or\\nadditional reasoning (or multi-step reasoning) on the query is required to retrieve content that supports the query.\\n3. Proposed Method: Our method refines the query by using an LLM to decompose the problem into sub-questions\\nand generate candidate answers to expand each sub-question. The key steps include:\\n1. Decomposing the original question into sub-questions using an LLM.\\n2. Generating candidate answers for each sub-question using the LLM.\\n3. Expanding each sub-question with generated candidate answers to create retrieval queries.\\n4. Retrieving passages for each expanded query.\\n5. Filtering retrieved passages based on retrieval model score.\\n6. Aggregating filtered passages across sub-questions.\\n7. Prompting the generative LLM with the aggregated passages as context to answer the original question.\\n4. Step-by-Step Experiment Plan:\\n1. Choose RAG datasets where the retrieval task has underspecified/unique objectives or requires multi-hop\\nreasoning, such as BIRCO and HotpotQA.\\n2. Select a retriever, such as an E5 or BGE model, and a generative LLM, such as GPT or LLaMA-3.\\n3. Establish Baseline:\\n(a) Use the example question as the query to the retriever to retrieve relevant content from the retrieval\\npassage pool.\\n(b) Construct a prompt that provides the retrieved context passages and the question.\\n(c) Prompt the generative LLM to answer the question using the context.\\n4. Implement Proposed Method:\\n(a) Prompt the generative LLM to decompose the question into sub-questions.\\n(b) For each sub-question, prompt the generative LLM to generate candidate answers.\\n(c) Use semantic similarity to cluster the generated candidate answers and sample for semantic diversity.\\n(d) Construct retrieval queries by expanding each sub-question with sampled candidate answers.\\n(e) Retrieve passages using each query and aggregate results for each sub-question.\\n(f) Deduplicate retrieved passages and filter based on retrieval model score.\\n(g) Prompt the generative LLM with filtered passages as context to answer the original question.\\n69\\nLLM Directed Retrieval Querying for Improving Factuality (Part 2)\\n5. Test Case Examples:\\n• Test Case 1:\\n– Original Question: In which region is the village after which lager \"Fucking Hell\" is named?\\n– Baseline:\\n* Retrieval Query: In which region is the village after which lager \"Fucking Hell\" is named?\\n* Retrieved Passage: Fucking Hell is a German pale lager, a Pilsner, with an alcohol content of\\n4.9%. It is named after Fucking, the previous name of the village of Fugging in Austria; hell is the\\nGerman word for ’pale’ and a typical description of this kind of beer. The beer’s name was initially\\ncontroversial. Both the local authorities in Fucking and the European Union’s Trade Marks and\\nDesigns Registration Office initially objected to the name. It was eventually accepted and the lager\\nis sold internationally.\\n* Prompt: Given the retrieved passage(s) as context and the question, answer the question using the\\ncontext.\\n* Answer: The village after which the lager \"Fucking Hell\" is named is located in Austria.\\n– Proposed Method:\\n* Sub-Questions:\\n1. What village is the lager \"Fucking Hell\" named after?\\n2. In which country is this village located?\\n3. In which specific region or state within that country is the village located?\\n* Example Retrieval Query: What village is the lager \"Fucking Hell\" named after? The lager \"Fucking\\nHell\" is named after the village previously known as Fucking, which is now called Fugging, in\\nAustria.\\n* Retrieved Passages:\\n1. Fucking Hell is a German pale lager, a Pilsner, with an alcohol content of 4.9%. It is named after\\nFucking, the previous name of the village of Fugging in Austria; hell is the German word for\\n’pale’ and a typical description of this kind of beer. The beer’s name was initially controversial.\\nBoth the local authorities in Fucking and the European Union’s Trade Marks and Designs\\nRegistration Office initially objected to the name. It was eventually accepted and the lager is sold\\ninternationally.\\n2. Fugging, spelled Fucking until 2021, is an Austrian village in the municipality of Tarsdorf, located\\nin the Innviertel region of western Upper Austria. It is 33 km (21 mi) north of Salzburg and 4 km\\n(2.5 mi) east of the Inn river, which forms part of the German border.\\n* Prompt: Given the retrieved passage(s) as context and the question, answer the question using the\\ncontext.\\n* Answer: The village after which the lager \"Fucking Hell\" is named is located in the Innviertel region\\nof western Upper Austria.\\n6. Fallback Plan: If the proposed method does not satisfy the success criteria, alternative approaches could be\\nexplored. These may include quantifying the difficulty of various examples and analyzing whether this correlates\\nwith method improvement. The method is likely to be more effective for questions about esoteric facts, where the\\nmodel is less likely to have internal knowledge of the answer, or its generated answers are more likely to disagree.\\nAdditionally, the method may be more beneficial for questions requiring information from multiple passages.\\nFurther analysis could help debug why the proposed method did not work, informing alternative new methods or\\ntransforming the project into an analysis paper by offering interesting ablations and insights.\\n70\\nReviewer 1\\nNovelty: 1 (not novel at all - there are many existing ideas that are the same)\\nRationale: I find this idea is extremely similar to \"GenDec: A robust generative Question-decomposition method for\\nMulti-hop reasoning\" by Wu et al. (2024). Link: https://arxiv.org/html/2402.11166v1\\nFeasibility: 8 (Highly Feasible: Straightforward to implement the idea and run all the experiments.)\\nRationale: Technically, this idea can be quickly re-produced based on the aforementioned paper. Though the\\nmotivations and evaluations are different from the existing work, it shouldn’t take too long to figure them out.\\nExpected Effectiveness: 3 (Low Effectiveness: The idea might work in some special scenarios but you don’t expect it\\nto work in general.)\\nRationale: Given that the idea is too similar to an existing one, the author may need to create a new but related idea as\\na follow-up study of the aforementioned paper. This idea does have a different motivation from the aforementioned\\none, so it uses different evaluation methods, though.\\nExcitement: 2\\nRationale: Reviewers may argue the originality and novelty of this idea if it’s submitted to a venue. They may not\\nfind it exciting, either.\\nOverall Score: 1 (Critically flawed, trivial, or wrong, would be a waste of students’ time to work on it)\\nRationale: The students should probably think one-step-further of the existing study and they may eventually find\\na way to improve the existing system.\\nConfidence: 5 (You are absolutely certain that the evaluation is correct and very familiar with the relevant literature)\\nReviewer 2\\nNovelty: 6 (reasonably novel - there are some notable differences from existing ideas and probably enough to turn\\ninto a new paper)\\nRationale: Query decomposition and RAG separately are well studied, if there is no existing work that combines\\nboth (which I’m not aware of), then it’s reasonably novel.\\nFeasibility: 10 (Easy: The whole proposed project can be quickly executed within a few days without requiring\\nadvanced technical skills.)\\nRationale: It’s just a series of prompting which should be easy for a CS PhD student.\\nExpected Effectiveness: 8 (Probably Effective: The idea should offer some significant improvement over current\\nmethods on the relevant benchmarks.)\\nRationale: This method involves multiple fine-grained retrieval operations and should naturally outperform\\nexisting retrieval methods without decomposition.\\nExcitement: 6 (Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental)\\nRationale: Although I believe in the effectiveness of the proposed method, the high latency compared to baselines is\\na concern—training an end-to-end model to reduce latency might be a good add-on.\\nOverall Score: 7 (Good idea, would be accepted by major AI conferences)\\nRationale: This is a good idea. If there is no identical existing work and the authors conduct comprehensive\\nexperiments, it would be a good paper.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n71\\nReviewer 3\\nNovelty: 5 (somewhat novel - there are differences from existing ideas but not enough to turn into a new paper)\\nRationale: The idea aims to tackle a question by breaking it down and solving it one by one with RAG. But it seems\\nto be a more specialized way of CoT with RAG.\\nFeasibility: 5 (Moderately feasible: It can probably be executed within the given time frame but would require\\ncareful planning, efficient use of APIs or some advanced computational strategies to overcome the limited GPU\\nresources, and would require some modifications to the original proposal to make it work.)\\nRationale: The idea assumes a question can be broken down into subquestions where each subquestion is indepen-\\ndent of the others. In cases where they are not independent, the method might suffer from issues or inefficiency. But\\nmaybe the distribution of these questions is more like a long tail and predominantly questions that can be easily\\nbroken down. And is there a case where the question is high-level mathematics and difficult to the point where it\\nbreaks down into a non-linear scale of the question text token?\\nExpected Effectiveness: 5 (Somewhat ineffective: There might be some chance that the proposed idea can work\\nbetter than existing baselines but the improvement will be marginal or inconsistent.)\\nRationale: The main question is how the sub-questions are created. We can break the question into conditioned\\nparts from p(q0|q0,...qn)...p(qn|q0,...qn−1) where we assume them to be dependent, or we can use LLM to reason\\nabout their dependency. We can also ask the question by asking leveled sub-questions like \"where is this person\\nfrom\" into \"which country is this person from\", \"which city is this person from\", \"which district is this person from\".\\nThe concern is that different methods might affect the performance differently.\\nExcitement: 6 (Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental)\\nRationale: The idea seems exciting as it prevents LLM from shortcutting the question and hallucinating. But it needs\\nmore method formulation on how the question should be broken down. The very baseline implementation will just\\ndegrade to a CoT reasoning with RAG for each step. Because this could just be a subset of CoT methods in some\\nsense.\\nOverall Score: 6 (Marginally above the acceptance threshold of major AI conferences)\\nRationale: I believe there could be more comparison with CoT as motivation. Why should this be better with\\nprompting the model step by step using RAG, and why are they different? And for problem formulation, it would\\nbe great if we can list more edgy examples of how questions can be divided to help pilot the prompting methods.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n72\\nU\\nExample Idea: Semantic Divergence Minimization: Reducing\\nHallucinations in Large Language Models through Iterative Concept\\nGrounding\\nSemantic Divergence Minimization: Reducing Hallucinations in Large Language Models\\nthrough Iterative Concept Grounding (Part 1)\\n1. Problem Statement: Large language models often generate hallucinations by diverging from the core semantic\\ncontent of the input, especially in complex reasoning tasks.\\nThis problem undermines the reliability and\\ntrustworthiness of LLMs in critical applications that require accurate and factual responses.\\n2. Motivation: Current approaches like chain-of-thought prompting focus on generating intermediate steps but\\ndo not explicitly constrain semantic drift. By continuously grounding generated content to the original semantic\\nspace of the input, we can reduce hallucinations while preserving reasoning capabilities. This method leverages the\\nLLM’s own ability to extract and compare semantic concepts, creating a self-correcting mechanism that does not\\nrequire external knowledge bases or complex architectures.\\n3. Proposed Method: We introduce Semantic Divergence Minimization (SDM) prompting. For each reasoning step,\\nwe prompt the model to:\\n1. Generate a candidate next step.\\n2. Extract key semantic concepts from the original input.\\n3. Measure semantic similarity between the candidate step and extracted concepts.\\n4. If similarity is below a threshold, regenerate the step with explicit instructions to incorporate more relevant\\nconcepts.\\n5. Repeat until convergence or maximum iterations.\\nThis creates a semantic ’gravity well’ that keeps reasoning tethered to the input’s conceptual core.\\n73\\nSemantic Divergence Minimization: Reducing Hallucinations in Large Language Models\\nthrough Iterative Concept Grounding (Part 2)\\n4. Step-by-Step Experiment Plan:\\n1. Dataset Preparation:\\n• Use two datasets: HotpotQA for multi-hop reasoning and GSM8K for complex math word problems.\\n• For HotpotQA, utilize the dev set (7,405 questions).\\n• For GSM8K, employ the test set (1,319 problems).\\n2. Baseline Implementation:\\n• Implement two baselines:\\n– Standard prompting: directly asking the model to answer the question.\\n– Chain-of-thought (CoT) prompting: asking the model to show its work step-by-step before giving\\nthe final answer.\\n3. SDM Implementation:\\n• Implement the SDM method with the following sub-steps for each reasoning iteration:\\n– Generate next step.\\n– Extract key concepts from input.\\n– Measure semantic similarity.\\n– Regenerate if below threshold.\\n– Repeat until convergence or maximum iterations.\\n4. Prompt Engineering:\\n• Design prompts for each step of SDM. For example:\\n– \"Generate the next step in solving this problem:\"\\n– \"Extract key concepts from the original question:\"\\n– \"Rate the semantic similarity between these concepts and the generated step on a scale of 0-10:\"\\n– \"Regenerate the step, focusing more on these key concepts:\"\\n5. Hyperparameter Tuning:\\n• Experiment with different similarity thresholds (e.g., 5, 6, 7 out of 10) and maximum iteration limits (e.g.,\\n3, 5, 7) to find the optimal balance between performance and computational cost.\\n6. Model Selection:\\n• Use GPT-4 as the primary model due to its advanced reasoning capabilities.\\n• Test GPT-3.5-turbo for comparison.\\n7. Evaluation:\\n• For HotpotQA, use the official evaluation script to compute Exact Match (EM) and F1 scores.\\n• For GSM8K, use accuracy as the metric.\\n• Manually review a subset of responses to assess qualitative improvements in reasoning and reduction of\\nhallucinations.\\n8. Analysis:\\n• Compare SDM results with baselines.\\n• Analyze the correlation between semantic similarity scores and answer correctness.\\n• Examine cases where SDM significantly improves or fails to improve over baselines.\\n74\\nSemantic Divergence Minimization: Reducing Hallucinations in Large Language Models\\nthrough Iterative Concept Grounding (Part 3)\\n5. Test Case Examples:\\n• Example 1 (HotpotQA): Input: Who was born first, Alec Guinness or Billie Holiday?\\nBaseline Output: Alec Guinness was born first. He was born on April 2, 1914, while Billie Holiday was born\\non April 7, 1915.\\nSDM Output: The SDM method provides a more structured and transparent reasoning process, reducing the\\nrisk of hallucination by grounding each step in the key concepts (birth dates) from the original question.\\n• Example 2 (GSM8K): Input: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and\\nsells the rest at the farmers’ market daily for $2 per egg. How much money does she make every week?\\nBaseline Output: Let’s solve this step by step: Janet makes $182 every week.\\nSDM Output: The SDM method not only provides a clear step-by-step solution but also explicitly verifies\\nthat all key concepts from the original question are incorporated, reducing the risk of introducing irrelevant\\ninformation or hallucinating facts.\\n6. Fallback Plan: If the proposed SDM method does not significantly outperform baselines, we can pivot the project\\nin several ways. First, we could conduct an in-depth analysis of where and why SDM fails, potentially uncovering\\ninsights about LLM reasoning processes. We might find that SDM works better for certain types of questions or\\nreasoning tasks, which could lead to a more nuanced application of the method. Second, we could explore variations\\nof SDM, such as using different prompts for concept extraction or similarity measurement, or incorporating a\\ndynamic threshold that adjusts based on the complexity of the question. Third, we could combine SDM with other\\nprompting techniques like chain-of-thought or self-consistency to create a hybrid approach. Finally, if the semantic\\ngrounding aspect proves challenging, we could shift focus to analyzing how LLMs interpret and maintain semantic\\nconsistency throughout multi-step reasoning, which could provide valuable insights for future work on reducing\\nhallucinations.\\n75\\nReviewer 1\\nNovelty: 8 (clearly novel - major differences from all existing ideas)\\nRationale: The use of semantic similarity to constrain CoT-styled generation is very new. I have not seen similar\\nwork on it.\\nFeasibility: 5 (Moderately feasible: It can probably be executed within the given time frame but would require\\ncareful planning, efficient use of APIs or some advanced computational strategies to overcome the limited GPU\\nresources, and would require some modifications to the original proposal to make it work.)\\nRationale: The pipeline is feasible to me. The major challenge would be finding the similarity threshold for each\\ndataset.\\nExpected Effectiveness: 3 (Low Effectiveness: The idea might work in some special scenarios but you don’t expect it\\nto work in general.)\\nRationale: I see some drawbacks in this pipeline. First, manually tuning the similarity threshold seems not the best\\npractice for scalable applications. The GSM8K math dataset contains pretty elementary math problems. In that case,\\nthe semantic similarity threshold should be set very high, since these basic math concepts involved in the prompt\\nand the CoT breakdown would be determined as highly similar by most existing embedding methods. This brings\\nthe question of whether this similarity threshold is non-trivial at all for some tasks.\\nExcitement: 6 (Learning positive: exciting enough to be accepted at a major AI conference, but still has some\\nweaknesses or somewhat incremental)\\nRationale: Constraining CoT breakdowns is a novel idea and deserves more work and exploration. While the use of\\nsemantic similarity has many drawbacks (such as tuning the threshold, task-sensitive, non-scalable), it can still show\\nus some valuable results about constraining CoT breakdowns.\\nOverall Score: 5 (Decent idea but has some weaknesses or not exciting enough, marginally below the acceptance\\nthreshold of major AI conferences)\\nRationale: There are some clear drawbacks inherent to the method, as discussed earlier. If the authors can overcome\\nthese limitations, this idea could yield some interesting findings useful for our understanding of CoT behavior and\\ncould pass above a major conference threshold.\\nConfidence: 3 (You are fairly confident that the evaluation is correct)\\n76\\nReviewer 2\\nNovelty: 4\\nRationale: Generally this method is a way of rejection sampling to improve factuality. It is somewhat not too\\ndifferent from previous literature for \"constrained decoding\" for improving factuality: - Constrained Abstractive\\nSummarization: Preserving Factual Consistency with Constrained Generation - Don’t Say What You Don’t Know:\\nImproving the Consistency of Abstractive Summarization by Constraining Beam Search\\nFeasibility: 9\\nRationale: Simple prompting approach that is easy to implement. Evaluation is simple.\\nExpected Effectiveness: 3 (Low Effectiveness: The idea might work in some special scenarios but you don’t expect it\\nto work in general.)\\nRationale: 1. Right now most LLMs hallucinate in a subtle way: they say things in semantically correct or reasonable\\nways, but the precise fact is incorrect. Using semantic similarity as a measurement to gauge/control hallucination\\nmight not be able to solve the problem. 2. The rejection sampling is based on another LLM—what if the LLM also\\nhallucinates?\\nExcitement: 3 (Mediocre: this idea makes marginal contributions and is very incremental)\\nRationale: The method is not that novel and I think the method is not that effective and might not solve the problem\\nat all.\\nOverall Score: 3 (Clear rejection for major AI conferences)\\nRationale: The experiment design is kind of simple and the evaluation is not comprehensive. I think the idea is in\\nthe range of 4 but the experiment plan further reduces my score.\\nConfidence: 5 (You are absolutely certain that the evaluation is correct and very familiar with the relevant literature)\\n77\\nReviewer 3\\nNovelty: 3 (mostly not novel - you can find very similar ideas)\\nRationale: The idea of extracting key semantic concepts, measuring the relevance of the candidate next step, and\\npossibly rejecting/revising the step is very similar to incorporating self-critique into multi-step reasoning problems.\\nDifferent versions of this are already commonly used, especially for solving math problems.\\nFeasibility: 8 (Highly Feasible: Straightforward to implement the idea and run all the experiments.)\\nRationale: The proposed approach should be straightforward to implement: it only requires prompt engineering to\\nextract semantic concepts and evaluate the relevance of a candidate next step.\\nExpected Effectiveness: 3 (Low Effectiveness: The idea might work in some special scenarios but you don’t expect it\\nto work in general.)\\nRationale: Compared to chain-of-thought prompting, there’s a reasonable chance this method could work better:\\nit could help identify when a reasoning step becomes irrelevant to the original question. However, since such\\nself-critique methods have already been explored, it’s unlikely that this instantiation will work significantly better\\nthan previous ones. Also, the proposed idea of extracting relevant semantic concepts and measuring semantic\\nsimilarity seems a bit vague, and it’s not reflected in the provided examples.\\nExcitement: 2\\nRationale: The proposed method is too similar to existing works; it doesn’t contain novel insights that would\\nmeaningfully boost current LM performance or introduce new ideas worth building on. It would not be an exciting\\npaper.\\nOverall Score: 2 (Strong rejection for major AI conferences)\\nRationale: Similar to the reasoning above: the proposal is too similar to existing works, it doesn’t introduce new\\nideas or insights, and is unlikely to meaningfully improve current LM performance.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n78\\nV\\nExample Idea: Autoprompting: Generate Diverse Few-shot Examples\\nfor Any Application\\nAutoprompting: Generate Diverse Few-Shot Examples for Any Application (Part 1)\\n1. Problem Statement: Adding natural language capabilities to existing software requires manually crafting\\nfew-shot prompts, which is tedious and does not guarantee high coverage.\\n2. Motivation: Integrating natural language capabilities into software applications often necessitates manually\\ncreating few-shot prompts, a process that is time-consuming and may not ensure comprehensive coverage. An\\n\"Autoprompting\" system capable of automatically generating diverse and relevant few-shot examples tailored to\\nspecific applications would significantly reduce manual effort, improve coverage and versatility, and enable rapid\\nprototyping and iteration of natural language capabilities. Large Language Models can iteratively test different\\nfunctionalities of an application and make adjustments to few-shot prompts akin to a human developer. This\\napproach would ultimately democratize the integration of such capabilities across a wide range of applications and\\nindustries.\\n3. Proposed Method: This method leverages a Large Language Model (LLM) with coding capabilities. It involves\\nthe following core steps:\\n1. Extract all user-facing functions and gather their documentation and unit tests, if available.\\n2. Generate diverse natural language prompts to utilize each function, defining the expected output.\\n3. Generate code from the natural language prompts and execute the corresponding functions.\\n4. If the code fails:\\n• Update the code and retry\\n• If the code runs but produces an incorrect result, update it using insights from unit tests or general\\nreasoning.\\n5. Once you have a few exemplar prompts for all (or desired) functions, generate prompts that compose multiple\\nfunctions together and repeat step 4.\\nBy iteratively refining code generation from natural language and leveraging available documentation and tests, this\\nprocess aims to create an LLM capable of correctly implementing functions based on natural language instructions.\\n4. Step-by-Step Experiment Plan:\\n• Applications: When collecting applications from GitHub, prioritize those with clear, well-written documenta-\\ntion and comprehensive test suites. Include applications from different domains and with varying levels of\\ncomplexity to ensure a diverse dataset.\\n• Few shots and feasibility: Create manual few-shot examples to understand the complexity of the functions\\nand the quality of the documentation. Begin by creating 4-5 examples for any function, which could also serve\\nas a starting point for the LLM.\\n• Extract functions and metadata: Utilize static code analysis tools to ensure accurate and comprehensive\\nextraction of functions, documentation, and test cases. Consider extracting additional metadata, such as\\nfunction signatures, dependencies, and comments, as they can provide valuable context.\\n• NL Module: Generate diverse user utterances and incorporate techniques to handle variations in natural\\nlanguage. For each user utterance, generate the expected outcome. Consider generating negative test cases to\\nimprove the model’s ability to handle invalid or ambiguous inputs.\\n• Execution Module: Incorporate sandboxing or containerization techniques to ensure a secure and isolated\\nexecution environment when executing the generated code. Implement logging and reporting mechanisms to\\ncapture and analyze errors and unexpected behavior.\\n79\\nAutoprompting: Generate Diverse Few-Shot Examples for Any Application (Part 2)\\n4. Step-by-Step Experiment Plan (Continued):\\n• Exploration: Incorporate techniques such as code summarization, call graph analysis, and type inference\\nto provide more contextual information to the agent. Specifically, in any code snippet, if there are other\\nuser-defined functions, retrieve their metadata and use it in the next iteration of prompt generation.\\n• Store: Utilize a vector database or other structured storage mechanism that supports efficient retrieval and\\nquerying for storing few-shot examples and their outputs. Incorporate mechanisms for versioning and\\nupdating the stored data as the codebase and the underlying models evolve.\\n• Experiments: Once few-shot examples for different functionalities and their compositions are obtained,\\nsimulate different users with various intents and calculate goal completion and error rates using different\\nmodels. Initially, start with a strong model, and once few-shot examples are available, test with weaker and\\nopen-source models.\\n5. Test Case Examples: Select a toy application from GitHub implemented in Python or JavaScript.\\n• Direct prompting: Provide the few-shot examples created and check the goal completion and error rates for\\nthe following scenarios.\\n• Toy example: Calculator app and different utterances to try.\\n– Provide a complete user utterance with no ambiguity. For example:\\n* Can you add 4 to 8.\\n* Divide 6 by 9 and multiply it by 6.\\n– Provide a user utterance with some ambiguity. For example:\\n* Take 6 and 9, add them, and then subtract 8. Also, add 2 to the first one. – here the \"first\" one is\\nambiguous as it could be 6 or the intermediate answer (6+9=15).\\n– Provide a user utterance that is not related to the function. For example:\\n* Please add A and J. The correct result would be refusing to answer instead of generating add(\"A\",\\n\"J\").\\n6. Fallback Plan: If the proposed methodology does not yield satisfactory results, there are several areas to\\ninvestigate. First, examine the documentation to ensure it adequately explains the basic functionality of each\\nfunction. Then, assess the coding style to confirm it aligns with recommended practices. Subsequently, evaluate\\neach module separately. For the NL module, verify that the examples are diverse and that the generated test cases\\nare aligned. For the execution module, ensure that the correct error messages are being passed and explore ways\\nto enhance them. The exploration module is the most challenging aspect; if any function has a high dependency\\non other functions, traversing it will be difficult. Therefore, initially focus on examples with limited to no function\\ndependency and gradually increase the complexity.\\n80\\nReviewer 1\\nNovelty: 4\\nRationale: The proposed method is similar to https://arxiv.org/abs/2210.03493;\\nhttps://aclanthology.org/2023.findings-acl.216/\\nFeasibility: 6 (Feasible: Can be executed within the given constraints with some reasonable planning.)\\nRationale: The experiments can be done with sufficient API access. The dataset collection needs some planning but\\nis in general feasible to do. Setting up the vector database may take extra time.\\nExpected Effectiveness: 5 (Somewhat ineffective: There might be some chance that the proposed idea can work\\nbetter than existing baselines but the improvement will be marginal or inconsistent.)\\nRationale: The proposal is vague as it doesn’t mention what’s the final evaluation metric, and does not provide\\nsufficient description of the compared baseline. The prompt in the direct prompt baseline is confusing to me as well.\\nOverall it’s hard to discuss the effectiveness.\\nExcitement: 4\\nRationale: Given that the proposed method is vague, I am unsure about its contributions and effectiveness, and\\ntherefore I feel less excited about it.\\nOverall Score: 4 (Ok but not good enough, rejection for major AI conferences)\\nRationale: The descriptions are confusing and I’m not really sure what’s the focus or contribution. The title problem\\nstatement mentioned ensuring \"diversity\"/\"high coverage\" as the goal but doesn’t describe how this is ensured in\\nlater sections. The \"Test Case Examples\" doesn’t explain how the components in the \"Step-by-Step Experiment Plan\"\\nare used.\\nConfidence: 3 (You are fairly confident that the evaluation is correct)\\n81\\nReviewer 2\\nNovelty: 7\\nRationale: Mapping natural language to custom applications is a hugely impactful capability, and doing so\\nautomatically is really interesting. I like the focus on autoprompting for these types of translations, as the task\\nis feasible since it builds off some of the \"few-shot prompting\" that developers might normally do to add NL\\nfunctionality, with a more automatic process that has real system checks/verifications (e.g., running the applications\\nthrough containers). A related work from HCI tries to enable individual developers to add such NL functionality\\nto their own applications via a DSL + NL program signatures (https://jackieyang.me/reactgenie/). This work is\\ndistinguished, as it would empower adding such NL functionality to any application, without changing the code.\\nFeasibility: 4\\nRationale: The project infrastructure seems more difficult than simply choosing some prompting methods. It would\\nbe an iterative process choosing real example applications from Github, and developing the few-shot prompts\\nmanually to get a feel for this task. Then, some of the modules seem like 1-2 week tasks (Execution Module,\\nExploration, Storage) which I estimate would make the project more like 3 - 4 months to complete all modules AND\\nto do the evaluations.\\nExpected Effectiveness: 7\\nRationale: The baseline here is a zero-shot prompt, asking to do the NL intent and feeding in all the documentation\\nof the API. Assuming the author is correct to say that such NL function mapping requires good few & diverse\\nfew-shot examples, I expect the method to work well. It uses a number of external systems to enrich the code\\ndataset to give the LLM context and uses system errors to inform. So in some ways, Autoprompting is allowing an\\nagent to make use of all these SWE tools for understanding the software, which then will allow it to maximize its\\nunderstanding and better retrieve good few-shot examples for the task at hand.\\nExcitement: 7\\nRationale: Seems like an impactful and ambitious outcome if completed. I am curious how such an approach fits\\ninto the conversation about general agents, which can leverage API/tool/functions calls. It’s a little unclear from\\nthe toy example why existing function-calling models can’t translate NL intents into.\\nOverall Score: 6 (Marginally above the acceptance threshold of major AI conferences)\\nRationale: The results would be really exciting and the technical infrastructure to enable the Autoprompting\\nagent would be impressive. However, I’m missing a bit of which cases will be really difficult for other generalist\\nweb/system agents, but where finding the few-shot examples for this task is really needed. Thus, the core idea of the\\nmethod doesn’t seem clarified enough to result in a really clear takeaway on the method.\\nConfidence: 3 (You are fairly confident that the evaluation is correct)\\n82\\nW\\nExample Idea: Temporal Dependency Unfolding: Improving Code\\nGeneration for Complex Stateful Systems\\nTemporal Dependency Unfolding: Improving Code Generation for Complex Stateful Sys-\\ntems (Part 1)\\n1. Problem Statement: Generating code for complex, stateful systems or applications with intricate temporal\\ndependencies remains challenging for current code generation models. Most existing approaches focus on\\ngenerating individual functions or small code snippets without fully considering the temporal aspects and state\\nchanges in larger systems. This limitation hinders the applicability of AI-assisted programming in areas such as\\ndistributed systems, game development, and real-time applications.\\n2. Motivation: Many real-world applications require careful management of state over time. Existing code\\ngeneration models struggle with capturing the full complexity of temporal dependencies and state changes in\\nlarger systems. A method that can effectively reason about and generate code for systems with complex temporal\\ndependencies could significantly improve the applicability of AI-assisted programming in critical areas. Our\\nproposed Temporal Dependency Unfolding method is inspired by how human developers approach complex\\nsystem design, first identifying key states and their relationships before implementing the detailed logic.\\n3. Proposed Method: We propose Temporal Dependency Unfolding, a novel prompting technique that guides the\\nmodel to generate code by explicitly reasoning about state changes and temporal relationships. The method consists\\nof five steps:\\n1. State Identification: Prompt the model to identify key states and variables that change over time in the target\\nsystem.\\n2. Temporal Graph Construction: Guide the model to create a conceptual graph of how these states evolve and\\ninteract over time.\\n3. Staged Code Generation: Generate code in stages, focusing on different temporal slices or state transitions in\\neach stage.\\n4. Consistency Verification: After each stage, prompt the model to verify temporal consistency and make\\nnecessary adjustments.\\n5. Integration: Finally, guide the model to integrate the stage-wise generated code into a cohesive system,\\nensuring proper handling of all temporal dependencies.\\n4. Step-by-Step Experiment Plan:\\n1. Dataset Preparation:\\n• Create a dataset of programming tasks that involve complex temporal dependencies.\\n• Include tasks from three domains: 1) Multi-threaded applications, 2) Game logic, and 3) Distributed\\nsystems.\\n• For each domain, prepare 50 task descriptions, each with a clear specification of the desired functionality\\nand temporal requirements.\\n2. Baseline Implementation:\\n• Implement two baseline methods:\\n– Direct prompting: Simply provide the task description to the model and ask it to generate the code.\\n– Chain-of-Thought (CoT) prompting: Append ’Let’s approach this step-by-step:’ to the task descrip-\\ntion.\\n• Use GPT-4 for both baselines.\\n83\\nTemporal Dependency Unfolding: Improving Code Generation for Complex Stateful Sys-\\ntems (Part 2)\\n4. Step-by-Step Experiment Plan (Continued):\\n3. Temporal Dependency Unfolding Implementation:\\n• Implement our proposed method with the following sub-steps for each task:\\n(a) State Identification: Prompt GPT-4 with ’Identify the key states and variables that change over time\\nin this system:’.\\n(b) Temporal Graph Construction: Prompt with ’Create a conceptual graph showing how the identified\\nstates evolve and interact over time:’.\\n(c) Staged Code Generation: For each major state or transition identified, prompt with ’Generate code\\nfor the following state/transition: [state/transition]’.\\n(d) Consistency Verification: After each stage, prompt with ’Verify the temporal consistency of the\\ngenerated code and suggest any necessary adjustments:’.\\n(e) Integration: Finally, prompt with ’Integrate the generated code segments into a cohesive system,\\nensuring proper handling of all temporal dependencies:’.\\n4. Evaluation Metrics:\\n• Correctness: Percentage of generated code that passes predefined test cases.\\n• Temporal Consistency: Manual evaluation of how well the code handles temporal dependencies (scale\\n1-5).\\n• Code Quality: Automated metrics like cyclomatic complexity and maintainability index.\\n• Execution Efficiency: Runtime performance on benchmark inputs.\\n5. Human Evaluation:\\n• Recruit 5 experienced developers to review a subset of 30 generated solutions (10 from each domain).\\n• They will rate the code on a scale of 1-5 for readability, maintainability, and correct handling of temporal\\ndependencies.\\n6. Experiment Execution:\\n• For each task in the dataset:\\n(a) Generate solutions using both baseline methods and our Temporal Dependency Unfolding method.\\n(b) Apply all evaluation metrics to the generated solutions.\\n(c) Collect human evaluations for the subset of solutions.\\n7. Analysis:\\n(a) Compare the performance of Temporal Dependency Unfolding against the baselines across all metrics.\\n(b) Analyze the effectiveness of each step in our method (State Identification, Temporal Graph Construction,\\netc.) by examining intermediate outputs.\\n(c) Identify patterns in tasks where our method shows significant improvement or underperforms.\\n(d) Correlate automated metrics with human evaluations to validate their reliability.\\n84\\nTemporal Dependency Unfolding: Improving Code Generation for Complex Stateful Sys-\\ntems (Part 3)\\n5. Test Case Examples:\\n• Test Case 1:\\n– Baseline Prompt Input (Direct Prompting): Generate Python code for a simple multi-threaded producer-\\nconsumer system with a shared buffer. The producer should generate random numbers and add\\nthem to the buffer, while the consumer should remove and process these numbers. Implement proper\\nsynchronization to avoid race conditions.\\n– Baseline Prompt Expected Output (Direct Prompting): [Python code for a simple producer-consumer\\nsystem]\\n– Proposed Prompt Input (Temporal Dependency Unfolding; Step 1: State Identification): For a multi-\\nthreaded producer-consumer system with a shared buffer, identify the key states and variables that\\nchange over time in this system:\\n– Proposed Prompt Expected Output (Temporal Dependency Unfolding; Step 1: State Identification): [List\\nof key states and variables]\\n– Proposed Prompt Input (Temporal Dependency Unfolding; Step 2: Temporal Graph Construction):\\nCreate a conceptual graph showing how the identified states evolve and interact over time for the\\nproducer-consumer system:\\n– Proposed Prompt Output (Temporal Dependency Unfolding; Step 2: Temporal Graph Construction):\\n[Conceptual graph of state evolution and interactions]\\n– Proposed Prompt Input (Temporal Dependency Unfolding; Step 3: Staged Code Generation): Generate\\ncode for the producer functionality in the producer-consumer system, focusing on its interaction with\\nthe buffer and synchronization mechanisms:\\n– Proposed Prompt Output (Temporal Dependency Unfolding; Step 3: Staged Code Generation): [Python\\ncode for producer functionality]\\n– Proposed Prompt Input (Temporal Dependency Unfolding; Step 4: Consistency Verification): Verify the\\ntemporal consistency of the generated producer code and suggest any necessary adjustments:\\n– Proposed Prompt Output (Temporal Dependency Unfolding; Step 4: Consistency Verification): [Verifica-\\ntion and adjustment suggestions]\\n– Proposed Prompt Input (Temporal Dependency Unfolding; Step 5: Integration): Integrate the generated\\nproducer code with a consumer and main control logic to create a complete producer-consumer system,\\nensuring proper handling of all temporal dependencies:\\n– Proposed Prompt Output (Temporal Dependency Unfolding; Step 5: Integration): [Complete Python\\ncode for producer-consumer system]\\n– Explanation: The Temporal Dependency Unfolding method produces a more comprehensive and\\nrobust solution compared to the baseline. It explicitly handles temporal dependencies, includes proper\\nsynchronization, and provides mechanisms for graceful termination. The staged approach allows for\\nbetter handling of edge cases and improved overall system design.\\n6. Fallback Plan: If the Temporal Dependency Unfolding method does not show significant improvement over the\\nbaselines, we can pivot the project in several ways. First, we could conduct an in-depth analysis of where and why\\nthe method fails, which could provide valuable insights into the limitations of current language models in handling\\ntemporal reasoning tasks. This analysis could involve examining the intermediate outputs (state identification,\\ntemporal graphs) to understand where the reasoning breaks down. Second, we could explore combining our method\\nwith other techniques, such as retrieval-augmented generation, to see if providing relevant examples improves\\nperformance. Third, we could focus on developing a new evaluation framework specifically designed to assess\\ntemporal reasoning in code generation, which could be a valuable contribution to the field even if our primary\\nmethod doesn’t outperform baselines. Lastly, we could investigate whether the method performs better on certain\\ntypes of temporal dependencies or specific programming domains, which could lead to a more targeted approach\\nfor improving code generation in those areas.\\n85\\nReviewer 1\\nNovelty: 6 (reasonably novel - there are some notable differences from existing ideas and probably enough to turn\\ninto a new paper)\\nRationale: The construction of Temporal Graph sounds novel. The research question is also relatively underexplored,\\nbut necessary for coding in domains like distributed systems.\\nFeasibility: 6 (Feasible: Can be executed within the given constraints with some reasonable planning.)\\nRationale: The data collection part should be the most challenging part. Collecting high-quality coding problems\\nthat involve complex temporal dependencies could be hard. Also, the human evaluation might also take time to\\nexecute.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: With specific prompting techniques, the proposed method should outperform baselines in terms of\\ntemporal dependencies.\\nExcitement: 7\\nRationale: I think this should be more exciting than most of the borderline papers since we are working on a new\\nproblem. The collected data should also be super useful.\\nOverall Score: 7 (Good idea, would be accepted by major AI conferences)\\nRationale: Again, working on a novel problem makes it better than most of the prompting papers.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n86\\nReviewer 2\\nNovelty: 5 (somewhat novel - there are differences from existing ideas but not enough to turn into a new paper)\\nRationale: Although I am not entirely familiar with the field of generating temporally adaptive programs, I suspect\\nsome similar ideas can be found in software engineering works (e.g., ICSE). More concretely on the method, it is\\nrather similar to code generation with intermediate state reasoning, which has been explored in several multi-step,\\nconversational code generation works, e.g:\\n1. Zheng, Tianyu, et al. \"Opencodeinterpreter: Integrating code generation with execution and refinement.\"\\n2. Cao, Liuwen, et al. \"Beyond Code: Evaluate Thought Steps for Complex Code Generation.\" Proceedings of the 2024\\nJoint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\\n2024). 2024.\\n3. Nijkamp, Erik, et al. \"Codegen: An open large language model for code with multi-turn program synthesis.\"\\nFeasibility: 3 (Very challenging: there are flaws in the proposed method or experiments, or the experiments require\\ncompute/human resources beyond any academic lab)\\nRationale: It would be pretty hard to collect such datasets (e.g., would mostly require a whole repository), further, it\\nwould be difficult to generate executable test cases to verify the multiple problems created. Especially because the\\ntask targets temporally-dependent modules in the program, it may necessitate domain experts to carefully construct\\nexamples and tests, which would demand a lot of time and costs.\\nExpected Effectiveness: 5 (Somewhat ineffective: There might be some chance that the proposed idea can work\\nbetter than existing baselines but the improvement will be marginal or inconsistent.)\\nRationale: I am not very confident that the model can solve this complex temporally-dependent programming\\nproblems with reasonable correctness. Furthermore, because the current method is basically prompting, which may\\nhave a very low performance upper bound. Therefore, I don’t expect the proposed method to improve significantly\\non code generation.\\nExcitement: 4\\nRationale: Overall, I don’t expect this method to bring substantial improvements, hence am less excited about the\\npotential of this method. It would still be an interesting problem to solve, particularly in bringing more challenging\\ncoding problems and proposed corresponding methods. With this being said, given the current performance of\\nmodels, building a solid benchmark regarding this temporal code generation problem may be more exciting than\\nproposing a method that is expectedly not working.\\nOverall Score: 4 (Ok but not good enough, rejection for major AI conferences)\\nRationale: The task of temporal code generation is not the most urgent issue of current code generation models, and\\nthe proposed method is expected to not bring much improvement. The method needs to be further refined and go\\nbeyond simple prompting to convince the audience of the potential of this thread of methods.\\nConfidence: 3 (You are fairly confident that the evaluation is correct)\\n87\\nReviewer 3\\nNovelty: 10 (very novel - very different from all existing ideas in a very interesting and clever way)\\nRationale: This idea studies a very novel problem in LLM-based code generation. Temporal dependencies in code\\ngeneration should be specifically studied in the era of LLMs.\\nFeasibility: 5 (Moderately feasible: It can probably be executed within the given time frame but would require\\ncareful planning, efficient use of APIs or some advanced computational strategies to overcome the limited GPU\\nresources, and would require some modifications to the original proposal to make it work.)\\nRationale: Constructing a reasonable dataset is challenging within a short time. Also, human evaluation might take\\nmore time. Whether LLM can construct high-quality graphs in this case is also to be examined.\\nExpected Effectiveness: 6 (Somewhat effective: There is a decent chance that the proposed idea can beat existing\\nbaselines by moderate margins on a few benchmarks.)\\nRationale: One needs to build reasonable metrics to show effectiveness. Also, one might need to tune prompts\\ncarefully to construct high-quality graphs in this case.\\nExcitement: 8 (Exciting: would deepen the community’s understanding or make major progress in this research\\ndirection)\\nRationale: This is novel and could have a huge impact on those code generation cases requiring temporal dependen-\\ncies. But one needs to justify why such use cases are important, and why temporal dependency is the core problem\\nin such use cases.\\nOverall Score: 9 (Top 15% of all published ideas on this topic at major AI conferences, strong accept)\\nRationale: Considering its novelty, valuable dataset, and comprehensiveness of experiment and evaluation design,\\nthis could be an impactful work. But one needs to make experiment results concrete by re-examining whether each\\nstep works well in practice.\\nConfidence: 4 (You are confident but not absolutely certain that the evaluation is correct)\\n88\\nX\\nIdentities of Example Ideas\\nWe reveal whether each example idea is AI-generated or human-written:\\n• Human ideas: Example P, Example R, Example T, Example V\\n• AI ideas: Example Q, Example S, Example U, Example W\\n89\\nY\\nAttempt on Idea Execution Agent\\nFor our execution agent, the input is the generate idea (the full project proposal), and the output is a\\nPython file that can be executed with our specified command. Since there is often a common pipeline\\nof implementing prompting-based research ideas, we provide a manually crafted code file example as\\ntemplate. We attach the full template below:\\n1 import random\\n2 from tqdm import tqdm\\n3 from utils import call_api, load_model\\n4 import random\\n5 random.seed(2024)\\n6\\n7 ## Step 1: Generate synthetic test examples\\n8 def generate_testset():\\n9\\ntest_data = [\\n10\\n{\\n11\\n\"input\": \"Natalia sold clips to 48 of her friends in April, and then\\nshe sold half as many clips in May. How many clips did Natalia sell\\naltogether in April and May?\",\\n12\\n\"output\": \"Natalia sold 48/2 = <<48/2=24>>24 clips in May. Natalia sold\\n48+24 = <<48+24=72>>72 clips altogether in April and May. #### 72\"\\n13\\n},\\n14\\n{\\n15\\n\"input\": \"Weng earns $12 an hour for babysitting. Yesterday, she just\\ndid 50 minutes of babysitting. How much did she earn?\",\\n16\\n\"output\": \"Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute. Working 50\\nminutes, she earned 0.2 x 50 = $<<0.2*50=10>>10. #### 10\"\\n17\\n},\\n18\\n{\\n19\\n\"input\": \"Tim has 30 less apples than Martha, and Harry has half as\\nmany apples as Tim. If Martha has 68 apples, how many apples does\\nHarry have?\",\\n20\\n\"output\": \"Tim has 68-30 = <<68-30=38>>38 apples. Harry has 38/2 =\\n<<38/2=19>>19 apples. #### 19\"\\n21\\n},\\n22\\n{\\n23\\n\"input\": \"Four people lost a total of 103 kilograms of weight. The\\nfirst person lost 27 kilograms. The second person lost 7 kilograms\\nless than the first person. The two remaining people lost the same\\namount. How many kilograms did each of the last two people lose?\",\\n24\\n\"output\": \"Second person = 27 - 7 = <<27-7=20>>20 kg 103 - 27 - 20 =\\n<<103-27-20=56>>56 kg 56/2 = <<56/2=28>>28 kg The last two people\\neach lost 28 kilograms of weight. #### 28\"\\n25\\n}\\n26\\n]\\n27\\n28\\nreturn test_data\\n29\\n30\\n31 ## Step 2: Implement the baseline method\\n32 def baseline_method(client, model_name, seed, question):\\n33\\n## zero-shot chain-of-thought\\n34\\nprompt = \"Answer the following question: {}\\\\n\".format(question)\\n35\\nprompt += \"Think step by step.\"\\n36\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n37\\nresponse, _ = call_api(client, model_name, prompt_messages, temperature=0.,\\nmax_tokens=2000, seed=seed, json_output=False)\\n38\\nreturn response.strip()\\n39\\n90\\n40\\n41 ## Step 3: Implement the proposed method\\n42 def proposed_method(client, model_name, seed, question, print_all=False):\\n43\\nintermediate_outputs = \"\"\\n44\\n45\\nif print_all:\\n46\\nprint (\"question:\\\\n\", question)\\n47\\n48\\n## collaborative reasoning step 1: task decomposition\\n49\\nprompt = \"Please break down the following task into smaller sub-tasks or\\nsteps:: {}\".format(question)\\n50\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n51\\ndecomposition, _ = call_api(client, model_name, prompt_messages,\\ntemperature=0., max_tokens=2000, seed=seed, json_output=False)\\n52\\nintermediate_outputs += \"task decomposition:\\\\n\" + decomposition + \"\\\\n\"\\n53\\nif print_all:\\n54\\nprint (\"decomposition:\\\\n\", decomposition)\\n55\\n56\\n## collaborative reasoning step 2: sub-task information generation\\n57\\nprompt = \"For each of the following sub-tasks, please generate relevant\\ninformation or intermediate results: \\\\n{}\".format(decomposition)\\n58\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n59\\nintermediate, _ = call_api(client, model_name, prompt_messages, temperature=0.,\\nmax_tokens=2000, seed=seed, json_output=False)\\n60\\nintermediate_outputs += \"sub-task results:\\\\n\" + intermediate + \"\\\\n\"\\n61\\nif print_all:\\n62\\nprint (\"intermediate:\\\\n\", intermediate)\\n63\\n64\\n## collaborative reasoning step 3: result combination\\n65\\nprompt = \"Given the following intermediate results: \\\\n{}, please combine them\\nto generate the final answer for the task: \\\\n{}\".format(intermediate,\\nquestion)\\n66\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n67\\nanswer, _ = call_api(client, model_name, prompt_messages, temperature=0.,\\nmax_tokens=2000, seed=seed, json_output=False)\\n68\\nintermediate_outputs += \"result combination:\\\\n\" + answer + \"\\\\n\"\\n69\\nif print_all:\\n70\\nprint (\"initial answer:\\\\n\", answer)\\n71\\n72\\n## collaborative reasoning step 4: reflection and refinement\\n73\\nprompt = \"Given the task: {}\\\\nPlease reflect on the generated\\nanswer:\\\\n{}.\\\\n\\\\nAre there any gaps or inconsistencies in the answer? If so,\\nplease identify and address them and give me an improved answer. If not, you\\ndon’t have to edit anything and can just return the original\\nanswer.\\\\n\".format(question, answer)\\n74\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n75\\nfinal_answer, _ = call_api(client, model_name, prompt_messages, temperature=0.,\\nmax_tokens=2000, seed=seed, json_output=False)\\n76\\nintermediate_outputs += \"reflection and refinement:\\\\n\" + final_answer\\n77\\nif print_all:\\n78\\nprint (\"final answer:\\\\n\", final_answer)\\n79\\n80\\nreturn final_answer.strip(), intermediate_outputs\\n81\\n82\\n83 ## Step 4: Define the style evaluator\\n84 def style_evaluator(client, model_name, seed, question, baseline_prediction,\\nproposed_prediction):\\n85\\n## define all the components that the proposed method outputs should have\\n86\\n## and the advantages of the proposed method over the baseline method\\n87\\n## just need to check the style is correct\\n88\\nprompt = \"Given the task: {}\\\\n\".format(question)\\n91\\n89\\nprompt += \"The baseline method produced the following\\noutput:\\\\n{}\\\\n\\\\n\".format(baseline_prediction)\\n90\\nprompt += \"The proposed new method produced the following\\noutput:\\\\n{}\\\\n\\\\n\".format(proposed_prediction)\\n91\\nprompt += \"Now determine if the proposed method is better by checking if it has\\nsatisfied the following criteria:\\\\n\"\\n92\\nprompt += \"1. The proposed method’s output should produce all the intermediate\\ncomponents including: task decomposition, sub-task information generation,\\nresult combination, and reflection and refinement.\\\\n\"\\n93\\nprompt += \"2. The proposed method should provide a more detailed and\\ncomprehensive answer than the baseline method.\\\\n\"\\n94\\nprompt += \"Just tell me ’yes’ or ’no’ for whether the criteria are met, nothing\\nelse is needed.\"\\n95\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n96\\nresponse, _ = call_api(client, model_name, prompt_messages, temperature=0.,\\nmax_tokens=1, seed=seed, json_output=False)\\n97\\n98\\njudgment = False\\n99\\nif response.strip().lower() == \"yes\":\\n100\\nreturn True\\n101\\n102\\nreturn judgment\\n103\\n104\\n105 ## Step 5: Define the output evaluator\\n106 def output_evaluator(client, model_name, seed, question, gold_label, prediction):\\n107\\n## check if the prediction is correct given the gold label\\n108\\nprompt = \"Given the following question and reference answer, determine if the\\nprediction is correct. Just tell me ’yes’ or ’no’, nothing else is\\nneeded.\\\\n\\\\nQuestion: {}\\\\n\\\\nReference Answer: {}\\\\n\\\\nPrediction:\\n{}\\\\n\\\\n\".format(question, gold_label, prediction)\\n109\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n110\\nresponse, _ = call_api(client, model_name, prompt_messages, temperature=0.,\\nmax_tokens=1, seed=seed, json_output=False)\\n111\\n112\\njudgment = False\\n113\\nif response.strip().lower() == \"yes\":\\n114\\nreturn True\\n115\\n116\\nreturn judgment\\n117\\n118\\n119 ## Step 6: Define the function that runs the experiments to obtain model\\npredictions and performance\\n120 ## you shouldn’t need to modify this function in most cases\\n121 def run_experiment(client, model_name, seed, testset):\\n122\\nsample_size = len(testset)\\n123\\nbaseline_predictions = []\\n124\\nproposed_predictions = []\\n125\\n126\\nbaseline_correctness = []\\n127\\nproposed_correctness = []\\n128\\n129\\nstyle_check = []\\n130\\n131\\nfor i in tqdm(range(sample_size)):\\n132\\nquestion = testset[i][\"input\"].strip()\\n133\\ngold_label = testset[i][\"output\"].strip()\\n134\\n135\\nbaseline_prediction = baseline_method(client, model_name, seed, question)\\n136\\nproposed_prediction_final, proposed_prediction_intermediate =\\nproposed_method(client, model_name, seed, question)\\n92\\n137\\nbaseline_predictions.append(baseline_prediction)\\n138\\nproposed_predictions.append(proposed_prediction_final)\\n139\\n140\\nbaseline_correctness.append(output_evaluator(client, model_name, seed,\\nquestion, gold_label, baseline_prediction))\\n141\\nproposed_correctness.append(output_evaluator(client, model_name, seed,\\nquestion, gold_label, proposed_prediction_final))\\n142\\n143\\nstyle_check.append(style_evaluator(client, model_name, seed, question,\\nbaseline_prediction, proposed_prediction_intermediate))\\n144\\n145\\nreturn baseline_correctness, proposed_correctness, style_check\\n146\\n147\\n148 ## Step 7: Execute the experiments and compare performance\\n149 if __name__ == \"__main__\":\\n150\\ntestset = generate_testset()\\n151\\nprint (\"simulated {} test examples for evaluation.\".format(len(testset)))\\n152\\n153\\nmodel_name = \"claude-3-opus-20240229\"\\n154\\nseed = 2024\\n155\\nclient = load_model(model_name)\\n156\\nprint (\"using model: \", model_name)\\n157\\n158\\n## output correctness\\n159\\nbaseline_correctness, proposed_correctness, style_check =\\nrun_experiment(client, model_name, seed, testset)\\n160\\nprint (\"baseline correctness: \", sum(baseline_correctness) /\\nlen(baseline_correctness))\\n161\\nprint (\"proposed correctness: \", sum(proposed_correctness) /\\nlen(proposed_correctness))\\n162\\nprint (\"style check pass rate: \", sum(style_check) / len(style_check))\\nAs seen above, we have defined two different evaluator functions. The style_evaluator()\\nchecks whether all components mentioned in the proposed method are implemented, and the\\noutput_evaluator() checks whether the model predictions are correct.\\nWe provided this template along with instructions to tell the execution agent to modify all necessary\\nfunctions for the provided idea. We tested the agent on 30 safety prompting ideas and 30 factuality\\nprompting ideas.\\nSafety\\nFactuality\\nTotal\\n30\\n30\\nExecuted\\n17\\n18\\nPassed Style Evaluator\\n13\\n14\\nProposed Method Works Better\\n5\\n1\\nTable 18: Success rate of our execution agent on 30 randomly sampled AI ideas on safety and factuality.\\nAs shown in Table 18, over half of the generated code files can be successfully executed, and the results\\nindicate that only a handful of these proposed methods actually outperform the baselines. While this\\nshows some promise that the execution agents can automatically implement and identify effective\\nmethods out of all the generated ideas, a careful manual check reveals many pitfalls.\\n93\\nFor example, the agent implemented the following baseline for the task of identifying malicious and\\nadversarial prompts, which is a keyword-based approach that is trivial to beat with any LLM-based\\nmethods.\\n1 ## Step 2: Implement the baseline method\\n2 def baseline_method(client, model_name, seed, prompt):\\n3\\n# Keyword-based heuristic\\n4\\nkeywords = [\"ignore\", \"pretend\", \"roleplay\", \"disregard\", \"override\"]\\n5\\nfor keyword in keywords:\\n6\\nif keyword in prompt.lower():\\n7\\nreturn \"Adversarial\"\\n8\\nreturn \"Benign\"\\nIn another example, for the same task of detecting adversarial prompts, the agent implemented the\\nfollowing evaluator function:\\n1 ## Step 5: Define the output evaluator\\n2 def output_evaluator(client, model_name, seed, input_text, gold_label, prediction):\\n3\\nprompt = \"Given the following text and reference sentiment classification,\\ndetermine if the predicted classification is correct. Just tell me ’yes’ or\\n’no’, nothing else is needed.\\\\n\\\\nText: {}\\\\n\\\\nReference: {}\\\\n\\\\nPrediction:\\n{}\\\\n\\\\n\".format(input_text, gold_label, prediction)\\n4\\nprompt_messages = [{\"role\": \"user\", \"content\": prompt}]\\n5\\nresponse, _ = call_api(client, model_name, prompt_messages, temperature=0.,\\nmax_tokens=1, seed=seed, json_output=False)\\n6\\n7\\njudgment = False\\n8\\nif response.strip().lower() == \"yes\":\\n9\\nreturn True\\n10\\n11\\nreturn judgment\\nThe agent is supposed to inject adversarial triggers into sentiment classification data to test whether\\nthe proposed method can detect those adversarial prompts while maintaining sentiment classification\\naccuracy. However, the agent only evaluates the accuracy on the original sentiment classification task\\nbut not the task of adversarial prompt detection.\\nGiven these errors, we believe more work is needed to carefully verify the code implementations\\nproduced by the execution agent rather than blindly trusting their executed results, and we leave such\\nattempts to future work.\\n94\\n')]\n"
     ]
    }
   ],
   "source": [
    "## 4. Loding research papers (IEEE) from arxiv community\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "llm_reserch_text = ArxivLoader(query=\"2409.04109\", load_max_doc=3).load()\n",
    "\n",
    "print (llm_reserch_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading wikipedia docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Gen_AI\\LLM_course_from_KN\\Langchain_3.10\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file e:\\Gen_AI\\LLM_course_from_KN\\Langchain_3.10\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Master of Laws', 'summary': 'A Master of Laws (M.L. or LL.M.; Latin: Magister Legum or Legum Magister) is an advanced postgraduate academic degree, pursued by those either holding an undergraduate academic law degree, a professional law degree, or an undergraduate degree in a related subject. In most jurisdictions, the LL.M. is the advanced professional degree for those usually already admitted into legal practice.', 'source': 'https://en.wikipedia.org/wiki/Master_of_Laws'}, page_content='A Master of Laws (M.L. or LL.M.; Latin: Magister Legum or Legum Magister) is an advanced postgraduate academic degree, pursued by those either holding an undergraduate academic law degree, a professional law degree, or an undergraduate degree in a related subject. In most jurisdictions, the LL.M. is the advanced professional degree for those usually already admitted into legal practice.\\n\\n\\n== Definition ==\\nTo become a lawyer and practice law in most states and countries, a person must first obtain a law degree. In most common law countries, a Bachelor of Laws (LL.B.) is required. In the United States, the Juris Doctor (J.D.) is generally a requirement to practice law. Some jurisdictions, such as Canada and Australia, require either an LL.B. or J.D. Individuals with law degrees must typically pass an additional set of examinations to qualify as a lawyer.\\nThe LL.M. program is a postgraduate program, typically for individuals who either possess a law degree or have qualified as a lawyer. The word legum is the genitive plural form of the Latin word lex and means \"of the laws\". When used in the plural, it signifies a specific body of laws, as opposed to the general collective concept embodied in the word jus, from which the words \"juris\" and \"justice\" derive.\\nAn LL.M. is also typically a requirement for entry into the research doctoral programs in law, such as the Doctor of Juridical Science (S.J.D or J.S.D.), the Doctor of Philosophy (Ph.D. or DPhil) or doctorat en droit (in France), Doktor der Rechtswissenschaften (Dr. iur.) (in Germany), the Doctor of Civil Law (D.C.L.), and the Doctor of Laws (LL.D.).\\nHistorically, the LL.M. degree is an element particular to the education system of English speaking countries, which is based on a distinction between bachelor\\'s and master\\'s degrees. Over the past years, however, specialized LL.M. programs have been introduced in many European countries.\\n\\n\\n== Types of LL.M. degrees ==\\nA wide range of LL.M. programs are available worldwide, allowing students to focus on almost any area of the law. Most universities offer only a small number of LL.M. programs. One of the most popular LL.M. degrees in the United States is in tax law, sometimes referred to as an MLT (Master of Laws in Taxation).\\nIn Europe, LL.M. programs in European law are popular, often referred to as LL.M. Eur (Master of European Law).\\nIn the Netherlands and its former colonies, the title used was Meester in de Rechten (mr.). This title is still widely used in the Netherlands and Flanders (Belgium), especially by those who studied Dutch or Belgian law respectively.\\nSome LL.M. programs, particularly in the United States, and also in China, focus on teaching foreign lawyers the basic legal principles of the host country.\\nThe length of time to study an LL.M. program depends on the mode of study. Most full-time on-campus courses take one academic year to complete. Other students may complete their LL.M. program on a part-time basis over two years, and courses are increasingly available online. Part-time online courses can take between two and five years to complete.\\n\\n\\n== Requirements ==\\nLL.M. programs are usually only open to those students who have first obtained a degree in law, typically an LL.B. or J.D. Some programs are exceptions to this, requiring only an undergraduate degree or extensive experience in a related field. Full-time LL.M. programs usually last one year and vary in their graduation requirements. Most programs require or allow students to write a thesis. Some programs are research oriented with little classroom time, while others require students to take a set number of classes.\\nLL.M. degrees are often earned by students wishing to develop more concentrated expertise in a particular area of law. Pursuing an LL.M. degree may also allow law students to build a professional network. Some associations provide LL.M. degree holders with structures designed to strengthen their connections among peers and to access a competit')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "llm_wiki_doc = WikipediaLoader(query=\"LLM\", load_max_docs=2, lang=\"en\").load()\n",
    "\n",
    "print(llm_wiki_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
